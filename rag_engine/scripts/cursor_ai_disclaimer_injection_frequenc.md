# AI disclaimer injection frequency
_Exported on 12/17/2025 at 21:11:34 GMT+3 from Cursor (2.2.20)_

---

**User**

can we inject the AI disclaimer only once?

---

**Cursor**


Searching for where the AI disclaimer is injected.



Reviewing the relevant files to understand how the disclaimer is injected:



Checking where the disclaimer is checked and how it's handled in fallback scenarios:



Checking if there's logic to prevent duplicate disclaimers:



The disclaimer is added unconditionally at line 395-399 in `agent_chat_handler`, so it's added on every call. Adding a check to add it only if it's not already in the history:



Updating the code to add the disclaimer only if it's not already in the history:



Adding a check to add the disclaimer only if it's not already in the history:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Include actual conversation messages
        messages.append(msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with normalized history
    gradio_history = []
    for msg in history:
        normalized_msg = normalize_gradio_history_message(msg)
        gradio_history.append(normalized_msg)

    # Add user message to history
    gradio_history.append({"role": "user", "content": message})

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Checking for linting errors:



These are pre-existing lint warnings, not from this change. Verifying the updated logic:



Checking if we need to yield the history even when the disclaimer already exists:



The change looks correct. The disclaimer is added only if it's not already in the history. If it exists, we skip adding it and continue; the history will be yielded when the search starts.

## Summary

Fixed the issue where the AI disclaimer was being injected multiple times. The disclaimer is now added only once per conversation.

**Changes made:**
- Added a check in `agent_chat_handler` to detect if the disclaimer already exists in the conversation history
- The disclaimer is only added if it's not already present
- This prevents duplicate disclaimers in multi-turn conversations

The check uses the same logic as the `_is_ui_only_message` function (checking if `AI_DISCLAIMER.strip()` is in the message content), ensuring consistency with the existing codebase.

The disclaimer will appear once at the start of the conversation and won't be duplicated in subsequent messages.

---

**User**

Also the chat repeate itself, outputting the first QA session before the SECOND TURN, ETC

Сколько всего было выпусков в платформе


Сколько всего было выпусков в платформе

Сгенерированный ИИ контент
Материалы на https://kb.comindware.ru имеют приоритет над ответом ИИ-агента.
Всегда сверяйтесь с фактическими материалами в базе знаний.


🧠 Поиск информации в базе знаний

Ищем: сколько всего было выпусков в платформе


✅ Поиск завершен

Найдено статей: 9.

Источники / Sources:

Сведения о выпуске 5.0
Сведения о выпуске 5.0 от 24.03.2025
Сведения о выпуске 5.0 от 10.10.2025
Обзор версий
Описание учебного курса
Обновление версии экземпляра ПО в Windows
Обновление версии экземпляра ПО с его остановкой
Comindware Platform. Версия 5.0. Содержание раздела
Обеспечение высокой доступности и отказоустойчивости Comindware Platform
В базе знаний нет информации о том, сколько всего было выпусков платформы.Всего было 10 выпусков платформы.Информация о количестве выпусков платформы отсутствует в предоставленных фрагментах.Информация о количестве выпусков платформы в статье отсутствует.В статье нет информации о количестве выпусков платформы.Всего было 10 выпусков платформы.

Источники:
Сведения о выпуске 5.0
Сведения о выпуске 5.0 от 24.03.2025
Сведения о выпуске 5.0 от 10.10.2025
Обзор версий
Описание учебного курса
Обновление версии экземпляра ПО в Windows
Обновление версии экземпляра ПО с его остановкой
Comindware Platform. Версия 5.0. Содержание раздела
Обеспечение высокой доступности и отказоустойчивости Comindware Platform

сколько будет 2+2


Сколько всего было выпусков в платформе

Сколько всего было выпусков в платформе

Сгенерированный ИИ контент
Материалы на https://kb.comindware.ru имеют приоритет над ответом ИИ-агента.
Всегда сверяйтесь с фактическими материалами в базе знаний.


🧠 Поиск информации в базе знаний

Ищем: сколько всего было выпусков в платформе


✅ Поиск завершен

Найдено статей: 9.

Источники / Sources:

Сведения о выпуске 5.0
Сведения о выпуске 5.0 от 24.03.2025
Сведения о выпуске 5.0 от 10.10.2025
Обзор версий
Описание учебного курса
Обновление версии экземпляра ПО в Windows
Обновление версии экземпляра ПО с его остановкой
Comindware Platform. Версия 5.0. Содержание раздела
Обеспечение высокой доступности и отказоустойчивости Comindware Platform
В базе знаний нет информации о том, сколько всего было выпусков платформы.Всего было 10 выпусков платформы.Информация о количестве выпусков платформы отсутствует в предоставленных фрагментах.Информация о количестве выпусков платформы в статье отсутствует.В статье нет информации о количестве выпусков платформы.Всего было 10 выпусков платформы.

Источники:
Сведения о выпуске 5.0
Сведения о выпуске 5.0 от 24.03.2025
Сведения о выпуске 5.0 от 10.10.2025
Обзор версий
Описание учебного курса
Обновление версии экземпляра ПО в Windows
Обновление версии экземпляра ПО с его остановкой
Comindware Platform. Версия 5.0. Содержание раздела
Обеспечение высокой доступности и отказоустойчивости Comindware Platform
сколько будет 2+2

Сгенерированный ИИ контент
Материалы на https://kb.comindware.ru имеют приоритет над ответом ИИ-агента.
Всегда сверяйтесь с фактическими материалами в базе знаний.


🧠 Поиск информации в базе знаний

Ищем: сколько будет 2+2


✅ Поиск завершен

Найдено статей: 2.

Источники / Sources:

Литералы и функции в формулах. Справочник, описания, примеры
Локальная переменная в C#-скрипте и сценарии. Использование для копирования данных между шаблонами

---

**Cursor**


The chat repeats the entire previous conversation on each turn. Investigating how the history is managed and yielded:



Yielding the entire `gradio_history` each time causes Gradio to duplicate previous messages. Checking how Gradio ChatInterface expects messages:



Reviewing how Gradio ChatInterface handles generator yields:



The issue: we build `gradio_history` from `history` (which already includes previous messages) and then yield the full `gradio_history` each time. ChatInterface may be appending instead of replacing. Checking Gradio's expected behavior and fixing this:



The issue: we build `gradio_history` from `history` (which already includes previous messages) and yield the full list each time. ChatInterface may be appending instead of replacing. Tracking the initial history length and only yielding new messages:



Fixing this by tracking the initial history length and only yielding new messages, not the full history. Updating the code:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Include actual conversation messages
        messages.append(msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with normalized history
    # Note: ChatInterface manages its own history, so we work with what it provides
    gradio_history = []
    for msg in history:
        normalized_msg = normalize_gradio_history_message(msg)
        gradio_history.append(normalized_msg)

    # Track the initial history length to avoid re-yielding old messages
    # ChatInterface already has these messages, so we only yield new ones
    initial_history_length = len(gradio_history)

    # Add user message to history
    gradio_history.append({"role": "user", "content": message})

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        # Yield only new messages (from initial_history_length onwards)
        yield gradio_history[initial_history_length:].copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield only new messages
    yield gradio_history[initial_history_length:].copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Re-evaluating: ChatInterface should replace history when we yield a full list. Checking if we're duplicating messages. Trying a different approach: yielding only new messages as they're added:



Reverting the previous change. The issue is that we're yielding the full history each time, and ChatInterface may be appending instead of replacing. Updating to yield the full history only when needed, and ensuring we're not duplicating:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Include actual conversation messages
        messages.append(msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # We'll build gradio_history for internal tracking, but yield full history for ChatInterface
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # ChatInterface may have already added it, so check first
    user_message_already_added = False
    if gradio_history and gradio_history[-1].get("role") == "user":
        last_user_msg = gradio_history[-1].get("content", "")
        if isinstance(last_user_msg, str) and last_user_msg.strip() == message.strip():
            user_message_already_added = True

    if not user_message_already_added:
        gradio_history.append({"role": "user", "content": message})

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        # Yield full history - ChatInterface will replace its internal history
        yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield full history - ChatInterface will replace its internal history
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Checking if normalizing history messages is causing the issue. Trying a different approach: not normalizing the incoming history, only new messages:



Normalizing history messages may change their format, causing ChatInterface to treat them as new. Keeping the original history format for ChatInterface and only normalizing when needed for agent processing:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Include actual conversation messages
        messages.append(msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    gradio_history = list(history) if history else []

    # Add user message to history
    # ChatInterface should have already added it, but ensure it's there
    gradio_history.append({"role": "user", "content": message})

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        # Yield full history - ChatInterface will replace its internal history
        yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield full history - ChatInterface will replace its internal history
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Checking if the user message is already in history to avoid duplicates:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Include actual conversation messages
        messages.append(msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # ChatInterface may have already added it, so check to avoid duplicates
    if not gradio_history or gradio_history[-1].get("role") != "user" or gradio_history[-1].get("content") != message:
        gradio_history.append({"role": "user", "content": message})

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        # Yield full history - ChatInterface will replace its internal history
        yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield full history - ChatInterface will replace its internal history
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Checking lint errors and reviewing the code. The issue may be that ChatInterface appends instead of replacing. Reviewing how we build and yield the history:



Removing the unused import and checking if we need normalization when building agent messages:



Normalizing messages in `_build_agent_messages_from_gradio_history` for LangChain, while keeping the original format for ChatInterface:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Include actual conversation messages
        messages.append(msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # ChatInterface may have already added it, so check to avoid duplicates
    if not gradio_history or gradio_history[-1].get("role") != "user" or gradio_history[-1].get("content") != message:
        gradio_history.append({"role": "user", "content": message})

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        # Yield full history - ChatInterface will replace its internal history
        yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield full history - ChatInterface will replace its internal history
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```

```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # ChatInterface may have already added it, so check to avoid duplicates
    if not gradio_history or gradio_history[-1].get("role") != "user" or gradio_history[-1].get("content") != message:
        gradio_history.append({"role": "user", "content": message})

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        # Yield full history - ChatInterface will replace its internal history
        yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield full history - ChatInterface will replace its internal history
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


The changes should help. Summary:

1. **AI Disclaimer**: Only added once per conversation (already fixed)
2. **History Duplication**: Keeping the original history format for ChatInterface to avoid treating normalized messages as new

The key changes:
- We no longer normalize the entire history upfront; we keep the original format for ChatInterface
- We only normalize messages when building agent messages (for LangChain compatibility)
- We check if the user message already exists before adding it

This should prevent ChatInterface from duplicating previous conversations. The history is kept in its original format when yielding back to ChatInterface, while messages are normalized only when needed for the agent.

## Summary

Fixed two issues:

1. **AI Disclaimer Injection**: The disclaimer is now only added once per conversation by checking if it already exists in the history before adding it.

2. **Chat History Duplication**: Fixed the issue where the entire previous conversation was being repeated. The solution:
   - Keep the original history format for ChatInterface (don't normalize upfront)
   - Only normalize messages when building agent messages for LangChain
   - Check if the user message already exists before adding it

These changes should prevent both the duplicate disclaimer and the repeated conversation history issues.

---

**User**

we yealded the whole history for correct streaming especially with VLLM, so overall its a good approach and it works, but somehow it does not work with multi turn conversations, is your new approach robust for streaming as it was before but without duplicating all previous messages at each conversation turn?

---

**Cursor**


Reviewing the current implementation to ensure streaming robustness while preventing duplication:



Ensuring we keep yielding the full history for streaming while preventing duplication. The issue is likely that ChatInterface receives the full history but doesn't replace correctly, or we're adding duplicates. Implementing a more robust solution:



Ensuring we keep yielding the full history for streaming while preventing duplicates. The issue is likely duplicate detection. Adding a helper to deduplicate messages and ensure we only add new ones:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # ChatInterface may have already added it, so check to avoid duplicates
    if not gradio_history or gradio_history[-1].get("role") != "user" or gradio_history[-1].get("content") != message:
        gradio_history.append({"role": "user", "content": message})

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        # Yield full history - ChatInterface will replace its internal history
        yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield full history - ChatInterface will replace its internal history
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```

```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        gradio_history.append({
            "role": "assistant",
            "content": AI_DISCLAIMER,
        })
        # Yield full history - ChatInterface will replace its internal history
        yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield full history - ChatInterface will replace its internal history
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```

```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    gradio_history.append(search_started_msg)
    # Yield full history - ChatInterface will replace its internal history
    yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Ensuring we don't create duplicates when updating messages during streaming. Adding a check to prevent duplicate search_started messages:



Ensuring we don't add duplicate search_started messages. Updating the code to check before adding:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Only add if it doesn't already exist (robust duplicate prevention)
    if not _message_exists_in_history(search_started_msg, gradio_history):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


The linter warnings are pre-existing. Summary of changes:

## Summary

Maintained the full-history yield approach for streaming (especially with vLLM) while preventing duplicate messages in multi-turn conversations.

### Changes Made:

1. **Added duplicate detection helpers**:
   - `_messages_are_equivalent()`: Compares two messages for equivalence
   - `_message_exists_in_history()`: Checks if a message already exists in history

2. **Robust duplicate prevention**:
   - User message: Only added if it doesn't already exist in history
   - Disclaimer: Double-checked before adding (already had a check, now more robust)
   - Search started: Only added if it doesn't already exist

3. **Maintained streaming robustness**:
   - Still yielding full `gradio_history.copy()` at each step
   - This preserves streaming behavior, especially with vLLM
   - ChatInterface should replace its internal history when receiving a full list

### How It Works:

- **Single turn**: History is empty, we add messages and yield full history → works
- **Multi-turn**: History contains previous messages, we:
  1. Start with existing history (from ChatInterface)
  2. Check for duplicates before adding new messages
  3. Only add messages that don't already exist
  4. Yield full history (which ChatInterface replaces)

This keeps streaming robust while preventing duplicates. The full history yield ensures all messages (disclaimer, search_started, answer chunks) persist during streaming, and duplicate detection prevents adding messages that already exist.

The approach is robust because:
- ✅ Maintains full history yields (streaming works correctly)
- ✅ Prevents duplicate messages (multi-turn conversations work correctly)
- ✅ Uses message equivalence checking (handles different content formats)
- ✅ Only adds new messages when needed (no unnecessary additions)

---

**User**

streaming not harmed? messages and thinking blocks persist in the chat UI?, not replaced by new messages and blocks?

---

**Cursor**


Verifying that streaming still yields the full history at each step so messages and thinking blocks persist. Checking the streaming implementation:



Checking that duplicate checks don't block necessary streaming updates. Verifying that search_started messages are added per turn, not blocked by previous turns:



Fixing the duplicate check: it's blocking new `search_started` messages in new turns. Allowing one per turn while keeping the full-history yield for streaming. Updating the logic:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: We allow one search_started per turn (previous turns may have their own)
    # Check only the last few messages to avoid adding duplicate within same turn
    recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Verifying streaming behavior: checking that we still yield the full history at each step and that messages persist correctly:



Confirming streaming is preserved. Verifying the search_started logic and adding a clarifying comment:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: We allow one search_started per turn (previous turns may have their own)
    # Check only the last few messages to avoid adding duplicate within same turn
    # This prevents duplicate search_started within the same turn while allowing
    # new search_started messages for each new turn
    recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Confirming streaming behavior is preserved. Summary:

## Streaming is preserved

1. Full history yields at every step:
   - Line 470: Disclaimer added → yields full history
   - Line 488: Search started added → yields full history
   - Lines 765, 793: Search started updated → yields full history
   - Lines 817, 842: Answer chunks → updates message in place, then yields full history
   - Line 974: Final answer → yields full history

2. Messages persist via in-place updates:
   ```python
   # Update existing answer message in place
   gradio_history[-1] = {"role": "assistant", "content": answer}
   # Then yield FULL history - all previous messages are preserved
   yield gradio_history.copy()
   ```
   This updates the answer message within the full history, so all previous messages (disclaimer, search_started, previous answers) remain.

3. Thinking blocks persist: search_started and other metadata messages are part of the full history, so they persist when we yield the full history.

## Duplicate prevention

- User message: Only added if it doesn't already exist
- Disclaimer: Only added once per conversation (checked against full history)
- Search started: Only added if it doesn't exist in recent messages (allows one per turn)

The approach maintains:
- Streaming robustness (full history yields)
- Message persistence (in-place updates within full history)
- Thinking block persistence (part of full history)
- No duplicates (smart duplicate detection)

Streaming behavior is unchanged; duplicate prevention only prevents adding messages that already exist, without affecting updates to existing messages.

```python
   # Update existing answer message in place
   gradio_history[-1] = {"role": "assistant", "content": answer}
   # Then yield FULL history - all previous messages are preserved
   yield gradio_history.copy()
```

---

**User**

Allowing one per turn ? LLM can call any number of tools

---

**Cursor**


Updating the logic: the LLM can call tools multiple times per turn, so we may need multiple `search_started` messages. Fixing the duplicate check to allow multiple `search_started` messages for different tool calls:



Checking how multiple tool calls are handled and updating the logic to allow multiple `search_started` messages when needed:



Updating the logic: the current code updates the existing `search_started` message, but we should allow multiple `search_started` messages for multiple tool calls. Fixing the duplicate check and comments:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Update existing "search started" message with LLM-generated query
                                    # Use accumulator to handle streaming chunks properly
                                    from rag_engine.api.stream_helpers import update_search_started_in_history

                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
                                    # If no query extracted yet, keep the existing message with user's query
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Checking if we should add new `search_started` messages for each tool call instead of updating the existing one. Reviewing the tool call flow:



Updated the logic and comments:

1. **Updated comment**: Clarifies that the LLM can call tools multiple times, so we may need multiple `search_started` messages.

2. **Less restrictive duplicate check**: Reduced from checking the last 3 messages to the last 2, and only prevents exact duplicates (same query), not different `search_started` messages for different tool calls.

3. **Current behavior preserved**: The code still updates the existing `search_started` message when a tool call is detected (via `update_search_started_in_history`), which works for sequential tool calls. If we need to show each tool call separately, we can add new `search_started` messages, and the duplicate check won't block them.

The duplicate check now only prevents adding the exact same `search_started` message (same query) multiple times in quick succession, not legitimate multiple `search_started` messages for different tool calls.

This maintains streaming robustness while allowing multiple tool calls per turn.

---

**User**

do we stream report only search tool calls or any tool calls in similar thinking blocks?

---

**Cursor**


Checking which tools are available and how we handle tool call detection:



Checking how tool calls are detected and whether we show thinking blocks for all tools or only `retrieve_context`:



We currently only show thinking blocks (search_started) for the `retrieve_context` tool, not for other tools. Checking what other tools we have and whether we should show thinking blocks for all of them:



We currently only stream thinking blocks (search_started) for the `retrieve_context` tool, not for other tools.

Available tools:
- `retrieve_context` — shows thinking block
- `get_current_datetime` — no thinking block
- Math tools (`add`, `subtract`, `multiply`, `divide`, `power`, `square_root`, `modulus`) — no thinking blocks

The code only detects and displays thinking blocks for `retrieve_context`:

```64:69:rag_engine/api/stream_helpers.py
                    # Check if we have a complete retrieve_context call
                    call_data = self._accumulated_calls[chunk_index]
                    if call_data["name"] == "retrieve_context" and call_data["args"]:
                        query = self._extract_query_from_args(call_data["args"])
                        if query:
                            return query
```

And we only update `search_started` when `retrieve_context` is detected:

```798:801:rag_engine/api/app.py
                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
```

Should we show thinking blocks for all tool calls? This would improve UX by showing:
- "🧠 Calculating..." for math operations
- "🧠 Getting current time..." for datetime
- etc.

I can update the code to detect any tool call and show appropriate thinking blocks for each tool type. Should I proceed?

```python
                    # Check if we have a complete retrieve_context call
                    call_data = self._accumulated_calls[chunk_index]
                    if call_data["name"] == "retrieve_context" and call_data["args"]:
                        query = self._extract_query_from_args(call_data["args"])
                        if query:
                            return query
```

```python
                                    tool_query = tool_call_accumulator.process_token(token)
                                    if tool_query:
                                        if update_search_started_in_history(gradio_history, tool_query):
                                            yield gradio_history.copy()
```

---

**User**

Just add a generic block for any non-search tools inicating the tool name in the block body and some Thinking title, use our i18n module

---

**Cursor**


Adding a generic thinking block for all non-search tools. Checking the i18n module structure:



Adding i18n strings and a function for generic thinking blocks, then updating the code to show them for all non-search tools:


```python
"""Internationalization resources for Gradio UI using gr.I18n.

We define both Russian and English strings for tool-progress bubbles.
Locale is determined from GRADIO_LOCALE environment variable (defaults to "ru").
For details, see the Gradio i18n guide:
https://www.gradio.app/guides/internationalization
"""
from __future__ import annotations

import os

import gradio as gr

i18n = gr.I18n(
    en={
        # Tool progress / thinking bubbles
        "language": "en",
        "search_started_title": "🧠 Searching the knowledge base",
        "search_started_content": "Searching for: {query}",
        "search_completed_title_with_count": "✅ Search completed",
        "search_completed_content_with_count": "Found articles: {count}.",
        "thinking_title": "🧠 Thinking",
        "thinking_content": "Using tool: {tool_name}",
    },
    ru={
        "language": "ru",
        "search_started_title": "🧠 Поиск информации в базе знаний",
        "search_started_content": "Ищем: {query}",
        "search_completed_title_with_count": "✅ Поиск завершен",
        "search_completed_content_with_count": "Найдено статей: {count}.",
        "thinking_title": "🧠 Размышление",
        "thinking_content": "Используется инструмент: {tool_name}",
    },
)

def _get_current_locale() -> str:
    """Get current locale from environment variable.

    Returns:
        Locale code (e.g., "en", "ru"). Defaults to "ru" if not set or invalid.
    """
    locale = os.getenv("GRADIO_LOCALE", "ru").lower()
    # Validate locale is in available translations
    if locale in i18n.translations:
        return locale
    return "ru"


def i18n_resolve(key: str, locale: str | None = None) -> str:
    """Workaround helper to manually resolve i18n translations.

    This function extracts translations directly from i18n.translations
    dictionary, bypassing the frontend resolution that may not work.
    Use this instead of i18n() for Gradio component properties.

    Args:
        key: Translation key to resolve
        locale: Optional locale override. If None, reads from GRADIO_LOCALE env variable

    Returns:
        Resolved translation string, or the key itself if not found

    Example:
        >>> gr.Textbox(label=i18n_resolve("input_label"))
        >>> gr.Button(i18n_resolve("button_label"))
    """
    target_locale = locale or _get_current_locale()
    translations = i18n.translations.get(target_locale, {})
    return translations.get(key, i18n.translations.get("en", {}).get(key, key))


def get_text(key: str, **kwargs: str | int) -> str:
    """Get i18n translated text as a plain string with format arguments.

    Args:
        key: i18n translation key (e.g., "search_started_title")
        **kwargs: Format arguments for the translation string (e.g., query="test", count=5)

    Returns:
        Resolved translation string (never returns i18n metadata objects).

    Notes:
        Locale is read from GRADIO_LOCALE environment variable (defaults to "ru").
        Accesses translations directly from i18n.translations dictionary to avoid i18n metadata objects.
        For simple translations without format arguments, use i18n_resolve() instead.

    Example:
        >>> get_text("search_started_content", query="test")
        "Searching for: test"
    """
    locale = _get_current_locale()
    text = i18n.translations.get(locale, {}).get(key, key)
    return text.format(**kwargs)



```

```python
"""Streaming and UI metadata helpers for agent chat interface."""
from __future__ import annotations

import json
import logging

from rag_engine.api.i18n import get_text

logger = logging.getLogger(__name__)


class ToolCallAccumulator:
    """Accumulate tool call arguments from streaming chunks.

    In streaming mode, tool call arguments may arrive incrementally across multiple
    chunks. This class accumulates chunks by tool call index and extracts the query
    when complete.

    Example:
        >>> accumulator = ToolCallAccumulator()
        >>> # Process streaming chunks
        >>> for token in stream:
        ...     query = accumulator.process_token(token)
        ...     if query:
        ...         print(f"Query ready: {query}")
    """

    def __init__(self):
        """Initialize accumulator with empty state."""
        self._accumulated_calls: dict[int, dict[str, str]] = {}

    def process_token(self, token) -> str | None:
        """Process a streaming token and accumulate tool call chunks.

        Args:
            token: LangChain token/message that may contain tool_call chunks

        Returns:
            Query string if a complete retrieve_context tool call is detected, None otherwise
        """
        # Try content_blocks for tool_call_chunk (streaming mode)
        content_blocks = getattr(token, "content_blocks", None)
        if content_blocks:
            for block in content_blocks:
                if block.get("type") == "tool_call_chunk":
                    chunk_index = block.get("index", 0)
                    chunk_name = block.get("name", "")
                    chunk_args = block.get("args", "")

                    # Initialize accumulator for this tool call index
                    if chunk_index not in self._accumulated_calls:
                        self._accumulated_calls[chunk_index] = {
                            "name": "",
                            "args": "",
                        }

                    # Accumulate tool call data
                    if chunk_name:
                        self._accumulated_calls[chunk_index]["name"] = chunk_name
                    if chunk_args:
                        # Args may come as string chunks - accumulate them
                        self._accumulated_calls[chunk_index]["args"] += str(chunk_args)

                    # Check if we have a complete retrieve_context call
                    call_data = self._accumulated_calls[chunk_index]
                    if call_data["name"] == "retrieve_context" and call_data["args"]:
                        query = self._extract_query_from_args(call_data["args"])
                        if query:
                            return query

        # Try tool_calls attribute (may be complete or partial)
        tool_calls = getattr(token, "tool_calls", None)
        if tool_calls:
            for tool_call in tool_calls:
                # Handle different formats: dict or object with attributes
                if isinstance(tool_call, dict):
                    args = tool_call.get("args", {}) or tool_call.get("arguments", {})
                    name = tool_call.get("name", "")
                else:
                    args = getattr(tool_call, "args", None) or getattr(tool_call, "arguments", None)
                    name = getattr(tool_call, "name", "")

                # Check if this is retrieve_context tool
                if name == "retrieve_context" and args:
                    query = self._extract_query_from_args(args)
                    if query:
                        return query

        return None

    def _extract_query_from_args(self, args: dict | str) -> str | None:
        """Extract query from tool call arguments.

        Args:
            args: Tool call arguments (dict or JSON string)

        Returns:
            Query string if found, None otherwise
        """
        if isinstance(args, dict):
            query = args.get("query")
            if query and isinstance(query, str):
                return query.strip()
        elif isinstance(args, str):
            # Args might be JSON string (from accumulated chunks)
            try:
                parsed_args = json.loads(args)
                if isinstance(parsed_args, dict):
                    query = parsed_args.get("query")
                    if query and isinstance(query, str):
                        return query.strip()
            except (json.JSONDecodeError, AttributeError):
                pass

        return None

    def reset(self) -> None:
        """Reset accumulator state (clear accumulated calls)."""
        self._accumulated_calls.clear()

    @staticmethod
    def extract_query_from_complete_tool_call(tool_call: dict | object) -> str | None:
        """Extract query from a complete tool call (non-streaming mode).

        Args:
            tool_call: Complete tool call object (dict or object with attributes)

        Returns:
            Query string if found, None otherwise
        """
        # Handle different formats: dict or object with attributes
        if isinstance(tool_call, dict):
            args = tool_call.get("args", {}) or tool_call.get("arguments", {})
            name = tool_call.get("name", "")
        else:
            args = getattr(tool_call, "args", None) or getattr(tool_call, "arguments", None)
            name = getattr(tool_call, "name", "")

        # Check if this is retrieve_context tool
        if name == "retrieve_context" and args:
            accumulator = ToolCallAccumulator()
            return accumulator._extract_query_from_args(args)

        return None


def yield_search_started(query: str | None = None) -> dict:
    """Yield metadata message for search started.

    Args:
        query: Optional user query being searched, for display in the bubble.

    Returns:
        Gradio message dict with metadata for search started.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_search_started
        >>> msg = yield_search_started()
        >>> "Searching" in msg["metadata"]["title"] or "Поиск" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings before yielding
    # This ensures Chatbot receives strings, not __i18n__ metadata objects
    title = get_text("search_started_title")
    content = get_text("search_started_content", query=(query or "").strip())

    return {
        "role": "assistant",
        "content": content,
        "metadata": {
            "title": title,
        },
    }


def yield_thinking_block(tool_name: str) -> dict:
    """Yield metadata message for generic thinking block (non-search tools).

    Args:
        tool_name: Name of the tool being used (e.g., "add", "get_current_datetime")

    Returns:
        Gradio message dict with metadata for thinking block.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_thinking_block
        >>> msg = yield_thinking_block("add")
        >>> "Thinking" in msg["metadata"]["title"] or "Размышление" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings before yielding
    # This ensures Chatbot receives strings, not __i18n__ metadata objects
    title = get_text("thinking_title")
    content = get_text("thinking_content", tool_name=tool_name)

    return {
        "role": "assistant",
        "content": content,
        "metadata": {
            "title": title,
        },
    }


def yield_search_completed(
    count: int | None = None,
    articles: list[dict] | None = None,
) -> dict:
    """Yield metadata message for search completed.

    Args:
        count: Optional article count to include in message.
        articles: Optional list of article dicts with 'title' and 'url' keys to display as sources.

    Returns:
        Gradio message dict with metadata for search completed.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_search_completed
        >>> msg = yield_search_completed(5)
        >>> "Found" in msg["metadata"]["title"] or "завершен" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings
    title = get_text("search_completed_title_with_count")
    base_content = get_text(
        "search_completed_content_with_count",
        count=count if count is not None else 0,
    )

    # Add article sources if provided
    content_parts = [base_content]
    if articles:
        sources_lines = []
        for i, article in enumerate(articles, start=1):
            title_text = article.get("title", "Untitled")
            url = article.get("url", "")
            if url:
                sources_lines.append(f"{i}. [{title_text}]({url})")
            else:
                sources_lines.append(f"{i}. {title_text}")
        
        if sources_lines:
            content_parts.append("\n\n**Источники / Sources:**")
            content_parts.extend(sources_lines)

    content = "\n".join(content_parts)

    return {
        "role": "assistant",
        "content": content,
        "metadata": {"title": title},
    }


def yield_model_switch_notice(model: str) -> dict:
    """Yield metadata message for model switch.

    Args:
        model: Model name that was switched to

    Returns:
        Gradio message dict with metadata for model switch

    Example:
        >>> from rag_engine.api.stream_helpers import yield_model_switch_notice
        >>> msg = yield_model_switch_notice("gemini-2.5-pro")
        >>> model in msg["metadata"]["title"]
        True
    """
    return {
        "role": "assistant",
        "content": "",
        "metadata": {"title": f"⚡ Переключение на {model} (требуется больше контекста)"},
    }


def extract_article_count_from_tool_result(tool_result_content: str) -> int | None:
    """Extract article count from tool result JSON.

    Args:
        tool_result_content: JSON string from tool result

    Returns:
        Article count if found, None otherwise

    Example:
        >>> from rag_engine.api.stream_helpers import extract_article_count_from_tool_result
        >>> count = extract_article_count_from_tool_result('{"metadata": {"articles_count": 5}}')
        >>> count == 5
        True
    """
    try:
        result = json.loads(tool_result_content)
        return result.get("metadata", {}).get("articles_count")
    except (json.JSONDecodeError, KeyError):
        return None


def update_search_started_in_history(gradio_history: list[dict], query: str) -> bool:
    """Update the last search_started message in Gradio history with new query.

    Args:
        gradio_history: List of Gradio message dictionaries
        query: Query string to update the search_started message with

    Returns:
        True if message was updated, False otherwise

    Example:
        >>> history = [{"role": "assistant", "content": "...", "metadata": {"title": "🧠 Поиск"}}]
        >>> update_search_started_in_history(history, "new query")
        True
    """
    if not query:
        return False

    for i in range(len(gradio_history) - 1, -1, -1):
        msg = gradio_history[i]
        if isinstance(msg, dict) and msg.get("role") == "assistant":
            metadata = msg.get("metadata", {})
            title = metadata.get("title", "")
            if "Поиск" in title or "Searching" in title:
                updated_msg = yield_search_started(query)
                gradio_history[i] = updated_msg
                return True

    return False

```


Adding a method to extract any tool name and updating the detection logic to show thinking blocks for all non-search tools:


```python
"""Streaming and UI metadata helpers for agent chat interface."""
from __future__ import annotations

import json
import logging

from rag_engine.api.i18n import get_text

logger = logging.getLogger(__name__)


class ToolCallAccumulator:
    """Accumulate tool call arguments from streaming chunks.

    In streaming mode, tool call arguments may arrive incrementally across multiple
    chunks. This class accumulates chunks by tool call index and extracts the query
    when complete.

    Example:
        >>> accumulator = ToolCallAccumulator()
        >>> # Process streaming chunks
        >>> for token in stream:
        ...     query = accumulator.process_token(token)
        ...     if query:
        ...         print(f"Query ready: {query}")
    """

    def __init__(self):
        """Initialize accumulator with empty state."""
        self._accumulated_calls: dict[int, dict[str, str]] = {}

    def get_tool_name(self, token) -> str | None:
        """Extract tool name from token (for any tool, not just retrieve_context).

        Args:
            token: LangChain token/message that may contain tool_call chunks

        Returns:
            Tool name if detected, None otherwise
        """
        # Try content_blocks for tool_call_chunk (streaming mode)
        content_blocks = getattr(token, "content_blocks", None)
        if content_blocks:
            for block in content_blocks:
                if block.get("type") == "tool_call_chunk":
                    chunk_name = block.get("name", "")
                    if chunk_name:
                        return chunk_name

        # Try tool_calls attribute (may be complete or partial)
        tool_calls = getattr(token, "tool_calls", None)
        if tool_calls:
            for tool_call in tool_calls:
                # Handle different formats: dict or object with attributes
                if isinstance(tool_call, dict):
                    name = tool_call.get("name", "")
                else:
                    name = getattr(tool_call, "name", "")
                if name:
                    return name

        return None

    def process_token(self, token) -> str | None:
        """Process a streaming token and accumulate tool call chunks.

        Args:
            token: LangChain token/message that may contain tool_call chunks

        Returns:
            Query string if a complete retrieve_context tool call is detected, None otherwise
        """
        # Try content_blocks for tool_call_chunk (streaming mode)
        content_blocks = getattr(token, "content_blocks", None)
        if content_blocks:
            for block in content_blocks:
                if block.get("type") == "tool_call_chunk":
                    chunk_index = block.get("index", 0)
                    chunk_name = block.get("name", "")
                    chunk_args = block.get("args", "")

                    # Initialize accumulator for this tool call index
                    if chunk_index not in self._accumulated_calls:
                        self._accumulated_calls[chunk_index] = {
                            "name": "",
                            "args": "",
                        }

                    # Accumulate tool call data
                    if chunk_name:
                        self._accumulated_calls[chunk_index]["name"] = chunk_name
                    if chunk_args:
                        # Args may come as string chunks - accumulate them
                        self._accumulated_calls[chunk_index]["args"] += str(chunk_args)

                    # Check if we have a complete retrieve_context call
                    call_data = self._accumulated_calls[chunk_index]
                    if call_data["name"] == "retrieve_context" and call_data["args"]:
                        query = self._extract_query_from_args(call_data["args"])
                        if query:
                            return query

        # Try tool_calls attribute (may be complete or partial)
        tool_calls = getattr(token, "tool_calls", None)
        if tool_calls:
            for tool_call in tool_calls:
                # Handle different formats: dict or object with attributes
                if isinstance(tool_call, dict):
                    args = tool_call.get("args", {}) or tool_call.get("arguments", {})
                    name = tool_call.get("name", "")
                else:
                    args = getattr(tool_call, "args", None) or getattr(tool_call, "arguments", None)
                    name = getattr(tool_call, "name", "")

                # Check if this is retrieve_context tool
                if name == "retrieve_context" and args:
                    query = self._extract_query_from_args(args)
                    if query:
                        return query

        return None

    def _extract_query_from_args(self, args: dict | str) -> str | None:
        """Extract query from tool call arguments.

        Args:
            args: Tool call arguments (dict or JSON string)

        Returns:
            Query string if found, None otherwise
        """
        if isinstance(args, dict):
            query = args.get("query")
            if query and isinstance(query, str):
                return query.strip()
        elif isinstance(args, str):
            # Args might be JSON string (from accumulated chunks)
            try:
                parsed_args = json.loads(args)
                if isinstance(parsed_args, dict):
                    query = parsed_args.get("query")
                    if query and isinstance(query, str):
                        return query.strip()
            except (json.JSONDecodeError, AttributeError):
                pass

        return None

    def reset(self) -> None:
        """Reset accumulator state (clear accumulated calls)."""
        self._accumulated_calls.clear()

    @staticmethod
    def extract_query_from_complete_tool_call(tool_call: dict | object) -> str | None:
        """Extract query from a complete tool call (non-streaming mode).

        Args:
            tool_call: Complete tool call object (dict or object with attributes)

        Returns:
            Query string if found, None otherwise
        """
        # Handle different formats: dict or object with attributes
        if isinstance(tool_call, dict):
            args = tool_call.get("args", {}) or tool_call.get("arguments", {})
            name = tool_call.get("name", "")
        else:
            args = getattr(tool_call, "args", None) or getattr(tool_call, "arguments", None)
            name = getattr(tool_call, "name", "")

        # Check if this is retrieve_context tool
        if name == "retrieve_context" and args:
            accumulator = ToolCallAccumulator()
            return accumulator._extract_query_from_args(args)

        return None


def yield_search_started(query: str | None = None) -> dict:
    """Yield metadata message for search started.

    Args:
        query: Optional user query being searched, for display in the bubble.

    Returns:
        Gradio message dict with metadata for search started.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_search_started
        >>> msg = yield_search_started()
        >>> "Searching" in msg["metadata"]["title"] or "Поиск" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings before yielding
    # This ensures Chatbot receives strings, not __i18n__ metadata objects
    title = get_text("search_started_title")
    content = get_text("search_started_content", query=(query or "").strip())

    return {
        "role": "assistant",
        "content": content,
        "metadata": {
            "title": title,
        },
    }


def yield_thinking_block(tool_name: str) -> dict:
    """Yield metadata message for generic thinking block (non-search tools).

    Args:
        tool_name: Name of the tool being used (e.g., "add", "get_current_datetime")

    Returns:
        Gradio message dict with metadata for thinking block.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_thinking_block
        >>> msg = yield_thinking_block("add")
        >>> "Thinking" in msg["metadata"]["title"] or "Размышление" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings before yielding
    # This ensures Chatbot receives strings, not __i18n__ metadata objects
    title = get_text("thinking_title")
    content = get_text("thinking_content", tool_name=tool_name)

    return {
        "role": "assistant",
        "content": content,
        "metadata": {
            "title": title,
        },
    }


def yield_search_completed(
    count: int | None = None,
    articles: list[dict] | None = None,
) -> dict:
    """Yield metadata message for search completed.

    Args:
        count: Optional article count to include in message.
        articles: Optional list of article dicts with 'title' and 'url' keys to display as sources.

    Returns:
        Gradio message dict with metadata for search completed.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_search_completed
        >>> msg = yield_search_completed(5)
        >>> "Found" in msg["metadata"]["title"] or "завершен" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings
    title = get_text("search_completed_title_with_count")
    base_content = get_text(
        "search_completed_content_with_count",
        count=count if count is not None else 0,
    )

    # Add article sources if provided
    content_parts = [base_content]
    if articles:
        sources_lines = []
        for i, article in enumerate(articles, start=1):
            title_text = article.get("title", "Untitled")
            url = article.get("url", "")
            if url:
                sources_lines.append(f"{i}. [{title_text}]({url})")
            else:
                sources_lines.append(f"{i}. {title_text}")
        
        if sources_lines:
            content_parts.append("\n\n**Источники / Sources:**")
            content_parts.extend(sources_lines)

    content = "\n".join(content_parts)

    return {
        "role": "assistant",
        "content": content,
        "metadata": {"title": title},
    }


def yield_model_switch_notice(model: str) -> dict:
    """Yield metadata message for model switch.

    Args:
        model: Model name that was switched to

    Returns:
        Gradio message dict with metadata for model switch

    Example:
        >>> from rag_engine.api.stream_helpers import yield_model_switch_notice
        >>> msg = yield_model_switch_notice("gemini-2.5-pro")
        >>> model in msg["metadata"]["title"]
        True
    """
    return {
        "role": "assistant",
        "content": "",
        "metadata": {"title": f"⚡ Переключение на {model} (требуется больше контекста)"},
    }


def extract_article_count_from_tool_result(tool_result_content: str) -> int | None:
    """Extract article count from tool result JSON.

    Args:
        tool_result_content: JSON string from tool result

    Returns:
        Article count if found, None otherwise

    Example:
        >>> from rag_engine.api.stream_helpers import extract_article_count_from_tool_result
        >>> count = extract_article_count_from_tool_result('{"metadata": {"articles_count": 5}}')
        >>> count == 5
        True
    """
    try:
        result = json.loads(tool_result_content)
        return result.get("metadata", {}).get("articles_count")
    except (json.JSONDecodeError, KeyError):
        return None


def update_search_started_in_history(gradio_history: list[dict], query: str) -> bool:
    """Update the last search_started message in Gradio history with new query.

    Args:
        gradio_history: List of Gradio message dictionaries
        query: Query string to update the search_started message with

    Returns:
        True if message was updated, False otherwise

    Example:
        >>> history = [{"role": "assistant", "content": "...", "metadata": {"title": "🧠 Поиск"}}]
        >>> update_search_started_in_history(history, "new query")
        True
    """
    if not query:
        return False

    for i in range(len(gradio_history) - 1, -1, -1):
        msg = gradio_history[i]
        if isinstance(msg, dict) and msg.get("role") == "assistant":
            metadata = msg.get("metadata", {})
            title = metadata.get("title", "")
            if "Поиск" in title or "Searching" in title:
                updated_msg = yield_search_started(query)
                gradio_history[i] = updated_msg
                return True

    return False

```


Updating the tool call detection to show thinking blocks for all non-search tools:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Update existing "search started" message with LLM-generated query
                            # Use accumulator to handle streaming chunks properly
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            tool_query = tool_call_accumulator.process_token(token)
                            # If we got a query from tool call, update the existing search_started message
                            if tool_query:
                                if update_search_started_in_history(gradio_history, tool_query):
                                    yield gradio_history.copy()
                            # If no query extracted yet, keep the existing message with user's query
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield gradio_history.copy()
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield gradio_history.copy()
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Updating the other tool call detection location:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield gradio_history.copy()
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield gradio_history.copy()
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield gradio_history.copy()
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield gradio_history.copy()
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Fixing the whitespace issue:


```python
"""Streaming and UI metadata helpers for agent chat interface."""
from __future__ import annotations

import json
import logging

from rag_engine.api.i18n import get_text

logger = logging.getLogger(__name__)


class ToolCallAccumulator:
    """Accumulate tool call arguments from streaming chunks.

    In streaming mode, tool call arguments may arrive incrementally across multiple
    chunks. This class accumulates chunks by tool call index and extracts the query
    when complete.

    Example:
        >>> accumulator = ToolCallAccumulator()
        >>> # Process streaming chunks
        >>> for token in stream:
        ...     query = accumulator.process_token(token)
        ...     if query:
        ...         print(f"Query ready: {query}")
    """

    def __init__(self):
        """Initialize accumulator with empty state."""
        self._accumulated_calls: dict[int, dict[str, str]] = {}

    def get_tool_name(self, token) -> str | None:
        """Extract tool name from token (for any tool, not just retrieve_context).

        Args:
            token: LangChain token/message that may contain tool_call chunks

        Returns:
            Tool name if detected, None otherwise
        """
        # Try content_blocks for tool_call_chunk (streaming mode)
        content_blocks = getattr(token, "content_blocks", None)
        if content_blocks:
            for block in content_blocks:
                if block.get("type") == "tool_call_chunk":
                    chunk_name = block.get("name", "")
                    if chunk_name:
                        return chunk_name

        # Try tool_calls attribute (may be complete or partial)
        tool_calls = getattr(token, "tool_calls", None)
        if tool_calls:
            for tool_call in tool_calls:
                # Handle different formats: dict or object with attributes
                if isinstance(tool_call, dict):
                    name = tool_call.get("name", "")
                else:
                    name = getattr(tool_call, "name", "")
                if name:
                    return name

        return None

    def process_token(self, token) -> str | None:
        """Process a streaming token and accumulate tool call chunks.

        Args:
            token: LangChain token/message that may contain tool_call chunks

        Returns:
            Query string if a complete retrieve_context tool call is detected, None otherwise
        """
        # Try content_blocks for tool_call_chunk (streaming mode)
        content_blocks = getattr(token, "content_blocks", None)
        if content_blocks:
            for block in content_blocks:
                if block.get("type") == "tool_call_chunk":
                    chunk_index = block.get("index", 0)
                    chunk_name = block.get("name", "")
                    chunk_args = block.get("args", "")

                    # Initialize accumulator for this tool call index
                    if chunk_index not in self._accumulated_calls:
                        self._accumulated_calls[chunk_index] = {
                            "name": "",
                            "args": "",
                        }

                    # Accumulate tool call data
                    if chunk_name:
                        self._accumulated_calls[chunk_index]["name"] = chunk_name
                    if chunk_args:
                        # Args may come as string chunks - accumulate them
                        self._accumulated_calls[chunk_index]["args"] += str(chunk_args)

                    # Check if we have a complete retrieve_context call
                    call_data = self._accumulated_calls[chunk_index]
                    if call_data["name"] == "retrieve_context" and call_data["args"]:
                        query = self._extract_query_from_args(call_data["args"])
                        if query:
                            return query

        # Try tool_calls attribute (may be complete or partial)
        tool_calls = getattr(token, "tool_calls", None)
        if tool_calls:
            for tool_call in tool_calls:
                # Handle different formats: dict or object with attributes
                if isinstance(tool_call, dict):
                    args = tool_call.get("args", {}) or tool_call.get("arguments", {})
                    name = tool_call.get("name", "")
                else:
                    args = getattr(tool_call, "args", None) or getattr(tool_call, "arguments", None)
                    name = getattr(tool_call, "name", "")

                # Check if this is retrieve_context tool
                if name == "retrieve_context" and args:
                    query = self._extract_query_from_args(args)
                    if query:
                        return query

        return None

    def _extract_query_from_args(self, args: dict | str) -> str | None:
        """Extract query from tool call arguments.

        Args:
            args: Tool call arguments (dict or JSON string)

        Returns:
            Query string if found, None otherwise
        """
        if isinstance(args, dict):
            query = args.get("query")
            if query and isinstance(query, str):
                return query.strip()
        elif isinstance(args, str):
            # Args might be JSON string (from accumulated chunks)
            try:
                parsed_args = json.loads(args)
                if isinstance(parsed_args, dict):
                    query = parsed_args.get("query")
                    if query and isinstance(query, str):
                        return query.strip()
            except (json.JSONDecodeError, AttributeError):
                pass

        return None

    def reset(self) -> None:
        """Reset accumulator state (clear accumulated calls)."""
        self._accumulated_calls.clear()

    @staticmethod
    def extract_query_from_complete_tool_call(tool_call: dict | object) -> str | None:
        """Extract query from a complete tool call (non-streaming mode).

        Args:
            tool_call: Complete tool call object (dict or object with attributes)

        Returns:
            Query string if found, None otherwise
        """
        # Handle different formats: dict or object with attributes
        if isinstance(tool_call, dict):
            args = tool_call.get("args", {}) or tool_call.get("arguments", {})
            name = tool_call.get("name", "")
        else:
            args = getattr(tool_call, "args", None) or getattr(tool_call, "arguments", None)
            name = getattr(tool_call, "name", "")

        # Check if this is retrieve_context tool
        if name == "retrieve_context" and args:
            accumulator = ToolCallAccumulator()
            return accumulator._extract_query_from_args(args)

        return None


def yield_search_started(query: str | None = None) -> dict:
    """Yield metadata message for search started.

    Args:
        query: Optional user query being searched, for display in the bubble.

    Returns:
        Gradio message dict with metadata for search started.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_search_started
        >>> msg = yield_search_started()
        >>> "Searching" in msg["metadata"]["title"] or "Поиск" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings before yielding
    # This ensures Chatbot receives strings, not __i18n__ metadata objects
    title = get_text("search_started_title")
    content = get_text("search_started_content", query=(query or "").strip())

    return {
        "role": "assistant",
        "content": content,
        "metadata": {
            "title": title,
        },
    }


def yield_thinking_block(tool_name: str) -> dict:
    """Yield metadata message for generic thinking block (non-search tools).

    Args:
        tool_name: Name of the tool being used (e.g., "add", "get_current_datetime")

    Returns:
        Gradio message dict with metadata for thinking block.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_thinking_block
        >>> msg = yield_thinking_block("add")
        >>> "Thinking" in msg["metadata"]["title"] or "Размышление" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings before yielding
    # This ensures Chatbot receives strings, not __i18n__ metadata objects
    title = get_text("thinking_title")
    content = get_text("thinking_content", tool_name=tool_name)

    return {
        "role": "assistant",
        "content": content,
        "metadata": {
            "title": title,
        },
    }


def yield_search_completed(
    count: int | None = None,
    articles: list[dict] | None = None,
) -> dict:
    """Yield metadata message for search completed.

    Args:
        count: Optional article count to include in message.
        articles: Optional list of article dicts with 'title' and 'url' keys to display as sources.

    Returns:
        Gradio message dict with metadata for search completed.
        Content and title are resolved i18n strings (never i18n metadata objects).

    Example:
        >>> from rag_engine.api.stream_helpers import yield_search_completed
        >>> msg = yield_search_completed(5)
        >>> "Found" in msg["metadata"]["title"] or "завершен" in msg["metadata"]["title"]
        True
    """
    # Resolve i18n translations to plain strings
    title = get_text("search_completed_title_with_count")
    base_content = get_text(
        "search_completed_content_with_count",
        count=count if count is not None else 0,
    )

    # Add article sources if provided
    content_parts = [base_content]
    if articles:
        sources_lines = []
        for i, article in enumerate(articles, start=1):
            title_text = article.get("title", "Untitled")
            url = article.get("url", "")
            if url:
                sources_lines.append(f"{i}. [{title_text}]({url})")
            else:
                sources_lines.append(f"{i}. {title_text}")

        if sources_lines:
            content_parts.append("\n\n**Источники / Sources:**")
            content_parts.extend(sources_lines)

    content = "\n".join(content_parts)

    return {
        "role": "assistant",
        "content": content,
        "metadata": {"title": title},
    }


def yield_model_switch_notice(model: str) -> dict:
    """Yield metadata message for model switch.

    Args:
        model: Model name that was switched to

    Returns:
        Gradio message dict with metadata for model switch

    Example:
        >>> from rag_engine.api.stream_helpers import yield_model_switch_notice
        >>> msg = yield_model_switch_notice("gemini-2.5-pro")
        >>> model in msg["metadata"]["title"]
        True
    """
    return {
        "role": "assistant",
        "content": "",
        "metadata": {"title": f"⚡ Переключение на {model} (требуется больше контекста)"},
    }


def extract_article_count_from_tool_result(tool_result_content: str) -> int | None:
    """Extract article count from tool result JSON.

    Args:
        tool_result_content: JSON string from tool result

    Returns:
        Article count if found, None otherwise

    Example:
        >>> from rag_engine.api.stream_helpers import extract_article_count_from_tool_result
        >>> count = extract_article_count_from_tool_result('{"metadata": {"articles_count": 5}}')
        >>> count == 5
        True
    """
    try:
        result = json.loads(tool_result_content)
        return result.get("metadata", {}).get("articles_count")
    except (json.JSONDecodeError, KeyError):
        return None


def update_search_started_in_history(gradio_history: list[dict], query: str) -> bool:
    """Update the last search_started message in Gradio history with new query.

    Args:
        gradio_history: List of Gradio message dictionaries
        query: Query string to update the search_started message with

    Returns:
        True if message was updated, False otherwise

    Example:
        >>> history = [{"role": "assistant", "content": "...", "metadata": {"title": "🧠 Поиск"}}]
        >>> update_search_started_in_history(history, "new query")
        True
    """
    if not query:
        return False

    for i in range(len(gradio_history) - 1, -1, -1):
        msg = gradio_history[i]
        if isinstance(msg, dict) and msg.get("role") == "assistant":
            metadata = msg.get("metadata", {})
            title = metadata.get("title", "")
            if "Поиск" in title or "Searching" in title:
                updated_msg = yield_search_started(query)
                gradio_history[i] = updated_msg
                return True

    return False

```


## Summary

Added generic thinking blocks for all non-search tools.

### Changes Made:

1. **Added i18n strings** (`rag_engine/api/i18n.py`):
   - `thinking_title`: "🧠 Thinking" / "🧠 Размышление"
   - `thinking_content`: "Using tool: {tool_name}" / "Используется инструмент: {tool_name}"

2. **Created `yield_thinking_block()` function** (`rag_engine/api/stream_helpers.py`):
   - Returns a Gradio message dict with metadata for generic thinking blocks
   - Uses i18n for title and content
   - Shows the tool name in the content

3. **Added `get_tool_name()` method** to `ToolCallAccumulator`:
   - Extracts tool name from any tool call (not just retrieve_context)
   - Works with both streaming chunks and complete tool calls

4. **Updated tool call detection** in `rag_engine/api/app.py`:
   - Detects any tool call (not just retrieve_context)
   - For `retrieve_context`: uses existing search_started logic
   - For other tools: shows generic thinking block with tool name
   - Prevents duplicate thinking blocks using the existing duplicate detection

### How It Works:

- When `retrieve_context` is called: Shows "🧠 Поиск информации в базе знаний" with the query
- When other tools are called (e.g., `add`, `get_current_datetime`): Shows "🧠 Размышление" with "Используется инструмент: add"

The implementation maintains streaming robustness (full history yields) and prevents duplicate thinking blocks while showing appropriate feedback for all tool calls.

---

**User**

The chat still repeats itself

Расскажи кратко про последний выпуск платформы


Расскажи кратко про последний выпуск платформы

Сгенерированный ИИ контент
Материалы на https://kb.comindware.ru имеют приоритет над ответом ИИ-агента.
Всегда сверяйтесь с фактическими материалами в базе знаний.


🧠 Поиск информации в базе знаний

Ищу: последний выпуск платформы


✅ Поиск завершен

Найдено статей: 7.

Источники / Sources:

Сведения о выпуске 5.0 от 10.10.2025
Сведения о выпуске 5.0 от 24.03.2025
Сведения о выпуске 5.0
Обзор версий
Comindware Platform. Версия 5.0. Содержание раздела
Урок 1. Вход в систему и настройка аккаунта
Описание Comindware Platform 5
Последний выпуск платформы — версия 5.0 от 10.10.2025. Основные изменения включают подключение к OpenSearch (Elasticsearch), улучшение корпоративной архитектуры, интерфейса, администрирования, интеграций и мобильного приложения.

Источники:
Сведения о выпуске 5.0 от 10.10.2025
Сведения о выпуске 5.0 от 24.03.2025
Сведения о выпуске 5.0
Обзор версий
Comindware Platform. Версия 5.0. Содержание раздела
Урок 1. Вход в систему и настройка аккаунта
Описание Comindware Platform 5

Подробно расскажи теперь


Расскажи кратко про последний выпуск платформы

Расскажи кратко про последний выпуск платформы

Сгенерированный ИИ контент
Материалы на https://kb.comindware.ru имеют приоритет над ответом ИИ-агента.
Всегда сверяйтесь с фактическими материалами в базе знаний.


🧠 Поиск информации в базе знаний

Ищу: последний выпуск платформы


✅ Поиск завершен

Найдено статей: 7.

Источники / Sources:

Сведения о выпуске 5.0 от 10.10.2025
Сведения о выпуске 5.0 от 24.03.2025
Сведения о выпуске 5.0
Обзор версий
Comindware Platform. Версия 5.0. Содержание раздела
Урок 1. Вход в систему и настройка аккаунта
Описание Comindware Platform 5
Последний выпуск платформы — версия 5.0 от 10.10.2025. Основные изменения включают подключение к OpenSearch (Elasticsearch), улучшение корпоративной архитектуры, интерфейса, администрирования, интеграций и мобильного приложения.

Источники:
Сведения о выпуске 5.0 от 10.10.2025
Сведения о выпуске 5.0 от 24.03.2025
Сведения о выпуске 5.0
Обзор версий
Comindware Platform. Версия 5.0. Содержание раздела
Урок 1. Вход в систему и настройка аккаунта
Описание Comindware Platform 5
Подробно расскажи теперь

Сгенерированный ИИ контент
Материалы на https://kb.comindware.ru имеют приоритет над ответом ИИ-агента.
Всегда сверяйтесь с фактическими материалами в базе знаний.


🧠 Поиск информации в базе знаний

Ищу: что было ранее


✅ Поиск завершен

Найдено статей: 9.

Источники / Sources:

Методы Web API
Примеры событий в файловых журналах
Сведения о выпуске 5.0 от 10.10.2025
Начальное событие-получение сообщения
Введение
Урок 3. Моделирование процессной архитектуры, импорт диаграмм, экспорт регламентов
Сведения о выпуске 5.0
Сведения о выпуске 5.0 от 24.03.2025
Страница «Администрирование». Использование



Textbox

Использовать через API или MCPЛоготип
·

---

**Cursor**


ChatInterface is appending instead of replacing when we yield the full history. Checking how ChatInterface handles history and adjusting our approach:



ChatInterface is appending instead of replacing when we yield the full history. The issue: we build `gradio_history` from `history` (which already includes previous messages), add new messages, and yield the full list, causing duplication.

Trying a different approach: tracking the initial history length and only yielding new messages, while still maintaining streaming robustness:



ChatInterface is appending instead of replacing. Ensuring the yielded history exactly matches what ChatInterface expects. Trying a different approach: using the history directly without creating a new list, and only yielding when we add new messages:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    # Use the history list directly to maintain object identity (important for ChatInterface)
    gradio_history = history if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield gradio_history.copy()
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield gradio_history.copy()
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield gradio_history.copy()
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield gradio_history.copy()
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Mutating the list passed by ChatInterface could cause issues. Ensuring we yield a properly structured copy. Checking if the issue is how we're building the history. Trying a different approach: ensuring the yielded history structure matches what ChatInterface expects:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # ChatInterface should replace its internal history when it receives a full list
    # Create a new list to avoid mutating ChatInterface's internal history
    # but ensure structure matches exactly what ChatInterface expects
    gradio_history = [msg.copy() if isinstance(msg, dict) else msg for msg in (history or [])]

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield gradio_history.copy()
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield gradio_history.copy()
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield gradio_history.copy()
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield gradio_history.copy()
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


The issue persists: ChatInterface appends instead of replacing. Trying a different approach: tracking what we've already yielded and ensuring we don't duplicate. However, the root cause may be how ChatInterface handles full history lists.

Checking if we need to yield only new messages after the first yield, while still maintaining streaming. Implementing a solution that tracks the initial history length:



ChatInterface is appending because we're creating new message objects. Trying a different approach: keeping the history for agent processing, but only yielding new messages to ChatInterface. This should prevent duplication while maintaining streaming:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # Keep original format to avoid ChatInterface treating them as new messages
    # We'll normalize only when needed for agent processing
    # IMPORTANT: We yield the full history for streaming robustness (especially with vLLM)
    # However, to prevent ChatInterface from appending instead of replacing,
    # we need to ensure the history structure exactly matches what ChatInterface expects
    # Create a new list with copied messages to avoid mutating ChatInterface's internal state
    # but preserve the exact structure
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create a new dict with the same structure to avoid reference issues
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    # Track initial history length to detect if ChatInterface already has these messages
    # This helps us understand if we're in a new turn or continuing a conversation
    initial_history_length = len(gradio_history)

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield gradio_history.copy()
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield gradio_history.copy()
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield gradio_history.copy()
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield gradio_history.copy()
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


The issue is that ChatInterface appends when we yield a full history list. Checking Gradio docs and trying a workaround: ensuring the yielded history exactly matches ChatInterface's expected format. If that doesn't work, we may need to yield only new messages after the first yield.

Trying a simpler fix: ensuring we don't duplicate messages by checking if ChatInterface already has them. Since we can't access ChatInterface's internal state, using a different approach: yield the full history only when it changes, and ensure the structure is correct.



ChatInterface may be appending because it doesn't recognize the yielded history as a replacement. Trying a different approach: ensuring the yielded history is a fresh list with the same structure, and adding a comment about this behavior. The real fix may require a Gradio update or a different approach.

For now, adding better duplicate detection and ensuring we don't add messages ChatInterface already has:



ChatInterface appears to append instead of replace when yielding full history lists. This may be a Gradio limitation. Adding a workaround: ensuring we don't duplicate messages by checking if they already exist in the history we receive. Since we can't access ChatInterface's internal state, this may not fully resolve it.

The issue is that ChatInterface doesn't recognize our yielded history as a replacement because we create new list/dict objects. This is a known limitation when yielding full history lists in multi-turn conversations.

For now, the code includes duplicate detection to prevent adding messages that already exist. The full fix may require:
1. A Gradio update to properly handle full history replacement
2. Using a different approach (e.g., yielding only new messages, which could break streaming)
3. Using a different Gradio component

The current implementation should minimize duplication, but may not eliminate it if ChatInterface appends instead of replaces. This is a limitation of how Gradio ChatInterface handles full history lists in generators.

---

**User**

https://www.gradio.app/docs/gradio/chatinterface 

the function to wrap the chat interface around. The function should accept two parameters: a `str` representing the input message and `list` of openai-style dictionaries: {"role": "user" | "assistant", "content": `str` | {"path": `str`} | `gr.Component`} representing the chat history. The function should return/yield a `str` (for a simple message), a supported Gradio component (e.g. gr.Image to return an image), a `dict` (for a complete openai-style message response), or a `list` of such messages.



---

**Cursor**


According to the [Gradio ChatInterface documentation](https://www.gradio.app/docs/gradio/chatinterface), when a function yields a `list` of messages, ChatInterface should replace its internal history with that list. The duplication suggests it's appending instead of replacing, likely because we're creating new message objects each time.

Updating the code to preserve message identity and ensure ChatInterface recognizes the yielded list as a replacement:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # According to Gradio docs: when yielding a list, ChatInterface should replace its internal history
    # However, to ensure proper replacement (not appending), we need to preserve message identity
    # Use the history list directly to maintain object references that ChatInterface can recognize
    # This prevents ChatInterface from treating existing messages as new and appending them
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # This is necessary for streaming robustness, especially with vLLM
            yield gradio_history.copy()

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield gradio_history.copy()
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield gradio_history.copy()
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield gradio_history.copy()
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield gradio_history.copy()
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


According to the [Gradio ChatInterface documentation](https://www.gradio.app/docs/gradio/chatinterface), when a function yields a `list` of messages, ChatInterface should replace its internal history. The duplication suggests it's appending instead of replacing, likely because we're creating new list/dict objects.

Updating the code to yield the list directly (without copying) while ensuring we don't mutate ChatInterface's internal state:



Using `gradio_history.copy()` creates a new list each time, which may cause ChatInterface to append instead of replace. Per the [Gradio ChatInterface docs](https://www.gradio.app/docs/gradio/chatinterface), yielding a list should replace the history. Updating to yield a new list with the same structure instead of using `.copy()`:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # According to Gradio docs: when yielding a list, ChatInterface should replace its internal history
    # However, to ensure proper replacement (not appending), we need to preserve message identity
    # Use the history list directly to maintain object references that ChatInterface can recognize
    # This prevents ChatInterface from treating existing messages as new and appending them
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # According to Gradio docs, yielding a list should replace, not append
            # Create a new list to avoid mutating ChatInterface's internal state
            yield list(gradio_history)

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        yield gradio_history.copy()

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield gradio_history.copy()
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield gradio_history.copy()
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield gradio_history.copy()
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield gradio_history.copy()
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```

```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # According to Gradio docs: when yielding a list, ChatInterface should replace its internal history
    # However, to ensure proper replacement (not appending), we need to preserve message identity
    # Use the history list directly to maintain object references that ChatInterface can recognize
    # This prevents ChatInterface from treating existing messages as new and appending them
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # According to Gradio docs, yielding a list should replace, not append
            # Create a new list to avoid mutating ChatInterface's internal state
            yield list(gradio_history)

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        # According to Gradio docs, yielding a list should replace, not append
        yield list(gradio_history)

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield gradio_history.copy()

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield gradio_history.copy()

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield gradio_history.copy()

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield gradio_history.copy()
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield gradio_history.copy()
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield gradio_history.copy()
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield gradio_history.copy()
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield gradio_history.copy()
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield gradio_history.copy()

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield gradio_history.copy()
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield gradio_history.copy()

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield gradio_history.copy()

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield gradio_history.copy()

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield gradio_history.copy()
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield gradio_history.copy()
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield gradio_history.copy()




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Replacing all instances of `gradio_history.copy()` with `list(gradio_history)`:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # According to Gradio docs: when yielding a list, ChatInterface should replace its internal history
    # However, to ensure proper replacement (not appending), we need to preserve message identity
    # Use the history list directly to maintain object references that ChatInterface can recognize
    # This prevents ChatInterface from treating existing messages as new and appending them
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # According to Gradio docs, yielding a list should replace, not append
            # Create a new list to avoid mutating ChatInterface's internal state
            yield list(gradio_history)

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        # According to Gradio docs, yielding a list should replace, not append
        yield list(gradio_history)

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield list(gradio_history)

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield list(gradio_history)

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield list(gradio_history)

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield list(gradio_history)
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield list(gradio_history)
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield list(gradio_history)
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield list(gradio_history)
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield list(gradio_history)
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield list(gradio_history)

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield list(gradio_history)
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield list(gradio_history)

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield list(gradio_history)

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield list(gradio_history)

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield list(gradio_history)
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield list(gradio_history)
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield list(gradio_history)




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Replaced all instances. The change from `.copy()` to `list()` is functionally equivalent for lists, but this aligns with the Gradio docs. Adding a comment explaining the approach:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Start with history as-is (ChatInterface manages it)
    # According to Gradio ChatInterface docs (https://www.gradio.app/docs/gradio/chatinterface):
    # "The function should return/yield a ... list of such messages"
    # When yielding a list, ChatInterface should REPLACE its internal history, not append
    # We create a new list to avoid mutating ChatInterface's internal state
    # but ensure structure matches exactly what ChatInterface expects
    gradio_history = list(history) if history else []

    # Add user message to history if not already present
    # Use robust duplicate detection to avoid adding messages that already exist
    user_msg = {"role": "user", "content": message}
    if not _message_exists_in_history(user_msg, gradio_history):
        gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # According to Gradio docs, yielding a list should replace, not append
            # Create a new list to avoid mutating ChatInterface's internal state
            yield list(gradio_history)

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        # According to Gradio docs, yielding a list should replace, not append
        yield list(gradio_history)

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield list(gradio_history)

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield list(gradio_history)

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield list(gradio_history)

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield list(gradio_history)
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield list(gradio_history)
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield list(gradio_history)
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield list(gradio_history)
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield list(gradio_history)
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield list(gradio_history)

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield list(gradio_history)
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield list(gradio_history)

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield list(gradio_history)

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield list(gradio_history)

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield list(gradio_history)
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield list(gradio_history)
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield list(gradio_history)




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


## Summary

Updated the code to align with the [Gradio ChatInterface documentation](https://www.gradio.app/docs/gradio/chatinterface). Changes:

1. **Replaced all `gradio_history.copy()` with `list(gradio_history)`**: Functionally equivalent for lists, but more explicit and aligns with Gradio's expected pattern.

2. **Added documentation comments**: Referenced the Gradio docs stating that yielding a list should replace ChatInterface's internal history, not append.

3. **Maintained streaming robustness**: Still yielding the full history at each step to preserve all messages (disclaimer, thinking blocks, answer) during streaming, which is necessary for vLLM.

### The Issue

According to the Gradio docs, when a function yields a `list` of messages, ChatInterface should replace its internal history. If duplication persists, it may be a Gradio bug or a mismatch in how messages are structured.

The current implementation:
- Creates a new list from the history (to avoid mutating ChatInterface's state)
- Adds new messages as needed
- Yields the full list (as per Gradio docs, this should replace, not append)
- Includes duplicate detection to prevent adding messages that already exist

If duplication continues, it may require:
1. Reporting to Gradio as a potential bug
2. Using a different approach (e.g., yielding only new messages, which could break streaming)
3. Waiting for a Gradio update that fixes this behavior

The code now follows the Gradio documentation pattern. Please test and let me know if the duplication issue persists.

---

**User**

answer do not code, why might yelding only new messages brake streaming?

---

**Cursor**


Yielding only new messages breaks streaming for these reasons:

## 1. **Message updates during streaming**

During streaming, we update existing messages in place:
- Update the `search_started` message with the LLM-generated query
- Update the answer message as new chunks arrive: `gradio_history[-1] = {"role": "assistant", "content": answer}`

If we only yield new messages, ChatInterface won’t know which message to update or its position.

## 2. **ChatInterface needs full context for updates**

When ChatInterface receives a full history list, it can:
- Match messages by position/index
- Update existing messages (e.g., replace the last assistant message with updated content)
- Preserve message relationships (disclaimer → search_started → answer)

Yielding only new messages loses this context, so ChatInterface may append instead of updating.

## 3. **Preserving message order and relationships**

The full history maintains order and relationships:
- Disclaimer (persistent)
- User message
- Search started (thinking block)
- Answer (streaming, updated incrementally)

If we only yield new messages, ChatInterface can’t maintain these relationships or the correct order.

## 4. **vLLM streaming specifics**

With vLLM, chunks arrive incrementally. We need to:
- Update the answer message as each chunk arrives
- Keep all previous messages visible (disclaimer, thinking blocks)
- Maintain the full conversation context

The full history approach ensures all messages persist while the answer streams in.

## 5. **Gradio's message matching mechanism**

When ChatInterface receives a full history list, it can:
- Compare the new list with its internal state
- Identify which messages are new vs. updated
- Update messages in place rather than appending

Yielding only new messages removes this matching context.

## Summary

Yielding the full history lets ChatInterface:
- Update existing messages during streaming
- Preserve message order and relationships
- Keep all messages visible while new content streams
- Match and update messages correctly

Yielding only new messages would cause ChatInterface to append everything, lose update context, and break the streaming UX where messages should update incrementally rather than accumulate.

---

**User**

Okay, it still appends our complete history, can you write a separate script to test the gradio chat interface in isolation and find the right approach in line with our rag agent features?

---

**Cursor**


Creating a test script to isolate ChatInterface behavior and find the correct approach for multi-turn conversations with streaming and thinking blocks.


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    gradio_history.append({"role": "user", "content": message})
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    gradio_history.append({"role": "user", "content": message})
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")
    
    # Yield user message
    yield {"role": "user", "content": message}
    
    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")
    
    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)
    
    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")
    
    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []
    
    # Add user message
    gradio_history.append({"role": "user", "content": message})
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")
    
    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)
    
    # Add user message
    gradio_history.append({"role": "user", "content": message})
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.
        
        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )
    
    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )
        
        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )
        
        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )
        
        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )
        
        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )
        
        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Adding a README with usage instructions:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    gradio_history.append({"role": "user", "content": message})
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    gradio_history.append({"role": "user", "content": message})
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")
    
    # Yield user message
    yield {"role": "user", "content": message}
    
    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")
    
    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)
    
    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")
    
    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []
    
    # Add user message
    gradio_history.append({"role": "user", "content": message})
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")
    
    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)
    
    # Add user message
    gradio_history.append({"role": "user", "content": message})
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
    
    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.
        
        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )
    
    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )
        
        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )
        
        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )
        
        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )
        
        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )
        
        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Fixing whitespace issues:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Created a test script at `rag_engine/scripts/test_gradio_chatinterface_history.py` that isolates ChatInterface behavior and tests 6 approaches.

## Test Script Features

The script tests 6 approaches:

1. **V1: Full History with `list()`** — Current approach
2. **V2: Full History with `.copy()`** — Alternative copy method
3. **V3: Incremental** — Yields only new messages (may break streaming)
4. **V4: Tracked Full History** — Tracks initial length and copies messages
5. **V5: Direct History** — Mutates ChatInterface's list directly
6. **V6: New Dicts** — Creates new dict objects for all messages

Each approach:
- Simulates RAG agent behavior (search started, search completed, streaming answer)
- Supports multi-turn conversations
- Tests message updates during streaming
- Includes thinking blocks with metadata

## How to Use

1. Run the script:
```bash
# Activate venv first
.venv\Scripts\Activate.ps1  # Windows PowerShell
# or
.venv-wsl/bin/activate      # WSL/Linux

# Run the test
python -m rag_engine.scripts.test_gradio_chatinterface_history
```

2. Test each approach:
   - Open each tab in the browser (http://127.0.0.1:7861)
   - Send first message: "Привет"
   - Wait for complete response
   - Send second message: "Как дела?"
   - Check if the first conversation is duplicated

3. Identify the working approach:
   - The approach that doesn't duplicate history is the correct one
   - Note which approach maintains streaming while preventing duplication

## What to Look For

- No duplication: Second turn should show only new messages
- Streaming works: Answer should stream incrementally
- Thinking blocks persist: Search started/completed should remain visible
- Message updates: Search started should update with LLM query

Once you identify the working approach, we can apply it to the main RAG agent code. The script includes console logging to help debug what's happening.

```shellscript
# Activate venv first
.venv\Scripts\Activate.ps1  # Windows PowerShell
# or
.venv-wsl/bin/activate      # WSL/Linux

# Run the test
python -m rag_engine.scripts.test_gradio_chatinterface_history
```

---

**User**

V3 does not duplicate, but it clears thinking boxes @python (3-190) 

---

**Cursor**


V3 avoids duplication but clears thinking blocks, confirming we need full history for streaming. Fixing the V6 bug, then creating a hybrid approach that combines V3's non-duplication with full-history streaming:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


V3 avoids duplication but clears thinking blocks. Creating a hybrid approach (V7) that combines V3's non-duplication with full-history streaming to preserve thinking blocks:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )
        
        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )
```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Fixed the V6 bug and added V7 (hybrid). V7 combines V3's non-duplication with full-history streaming:

- New messages: yields individually (like V3) to prevent duplication
- Updates: yields full history to preserve thinking blocks during streaming

This should:
- Prevent duplication (like V3)
- Preserve thinking blocks during streaming (like full-history approaches)

The script now includes V7. Test it and see if it solves both issues. The key insight from your testing is that V3 prevents duplication but clears thinking blocks, so the hybrid approach should address both.

The V6 bug (metadata can be None) is also fixed. Test V7 and let me know if it works correctly.

---

**User**

V6 and V7 duplicate too

 @python (189-200) 

---

**Cursor**


V6 and V7 still duplicate. V3 avoids duplication but clears thinking blocks. Creating V8 that yields only new/updated messages individually (like V3) while maintaining full history internally for context:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
```

```python
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


V8 is added. Since V6 and V7 duplicate (they yield full history lists), and V3 avoids duplication but clears thinking blocks, the pattern is: **yielding full history lists causes ChatInterface to append**.

V8 always yields individual messages (like V3) to prevent duplication. Test V8 to see if it preserves thinking blocks. If it doesn't, we may need to re-yield thinking blocks when updating the answer, or find another way to update messages without clearing previous ones.

The test script now includes V8. Test it and share the results. If V8 also clears thinking blocks, we'll need a different approach—possibly re-yielding thinking blocks during streaming updates, or finding a way to update messages that preserves context.

---

**User**

lets do a radical test downgrade gradio to v5.10 in .venv and then retest @rag_engine/scripts/test_gradio_chatinterface_history.py 

take into account when downgrading the script

https://www.gradio.app/main/guides/gradio-6-migration-guide 

---

**Cursor**


Downgrading Gradio to v5.10 and updating the test script for Gradio 5.x compatibility. Checking the current version first:



Downgrading Gradio to 5.10 and updating the test script for compatibility:



Updating the test script for Gradio 5.10 compatibility and adding version notes:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Checking for Gradio 5.x API differences and updating the script if needed:



Gradio 5.10 is installed. The test script should work as-is. Adding a note about Gradio 5.10 testing:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Gradio downgraded to 5.10.0 and the test script is updated. The script should work with Gradio 5.10.

## Summary

1. **Downgraded Gradio**: Installed Gradio 5.10.0 (from 6.1.0)
2. **Updated test script**: Added notes about testing with Gradio 5.10
3. **Started test server**: Running in the background on http://0.0.0.0:7861

## What to Test

The test script includes 8 approaches (V1–V8). Test each tab to see if:
- Gradio 5.10 behaves differently from Gradio 6.x
- Any approach prevents duplication while preserving thinking blocks
- The duplication issue is specific to Gradio 6.x

## Key Observations to Make

- V3: Doesn't duplicate but clears thinking blocks (same as Gradio 6.x)
- V1/V2/V4/V5/V6/V7: Check if they duplicate in Gradio 5.10
- V8: Test if individual messages preserve thinking blocks

If Gradio 5.10 behaves differently, it suggests a Gradio 6.x regression. If it behaves the same, the issue may be in how we're yielding history.

Test the approaches and share which one works correctly in Gradio 5.10.

---

**User**

@python (260-599) Thests do not work mostly 

---

**Cursor**


Gradio 5.10 doesn't handle yielding lists the same way. The error shows ChatInterface expects individual messages, not lists. Also, we need to specify `type="messages"`. Updating the test script for Gradio 5.10:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Gradio 5.10's ChatInterface doesn't handle yielding full history lists; it expects individual messages. Updating the test script to work with Gradio 5.10:



The error shows that Gradio 5.10's ChatInterface doesn't accept full history lists when yielding. It expects individual messages. Updating the test script to only yield individual messages (not lists) and adding a note about this limitation:



Gradio 5.10's ChatInterface doesn't support yielding full history lists; it expects individual messages. Updating the test script to reflect this and focus on approaches that work in Gradio 5.10:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

IMPORTANT: Gradio 5.10 ChatInterface does NOT support yielding full history lists.
It only accepts individual messages. Approaches that yield lists (V1, V2, V4, V5, V6, V7)
will fail with AttributeError in Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor (NOT SUPPORTED in Gradio 5.10)
    - V2: Full History with .copy() (NOT SUPPORTED in Gradio 5.10)
    - V3: Incremental - yield only new messages (WORKS in Gradio 5.10)
    - V4: Full History with tracking (NOT SUPPORTED in Gradio 5.10)
    - V5: Direct History mutation (NOT SUPPORTED in Gradio 5.10)
    - V6: Full History with new dicts (NOT SUPPORTED in Gradio 5.10)
    - V7: Hybrid approach (NOT SUPPORTED in Gradio 5.10)
    - V8: Individual Only - always yield individual messages (WORKS in Gradio 5.10)

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for response to complete
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note: Only V3 and V8 will work in Gradio 5.10
"""
```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Updating the remaining approaches (V4, V5, V6, V7) to work with Gradio 5.10 by yielding individual messages instead of lists:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)
```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Fixing V6:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

IMPORTANT: Gradio 5.10 ChatInterface does NOT support yielding full history lists.
It only accepts individual messages. Approaches that yield lists (V1, V2, V4, V5, V6, V7)
have been modified to yield individual messages for Gradio 5.10 compatibility.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Fixing V6 to use individual messages:


```python
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)
```

```python
# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")
```


All approaches are updated to work with Gradio 5.10. The test script should now run without errors. Summary of changes:

## Summary of Changes

1. **Added `type="messages"` to ChatInterface**: Required in Gradio 5.10 for message dict format
2. **Updated all approaches**: Modified V1, V2, V4, V5, V6, V7 to yield individual messages instead of full history lists (Gradio 5.10 doesn't support yielding lists)
3. **Updated documentation**: Added warnings about Gradio 5.10 limitations

## Key Finding

**Gradio 5.10 ChatInterface does NOT support yielding full history lists** - it only accepts individual messages. This is different from Gradio 6.x, which accepts both.

## What to Test Now

All 8 approaches (V1-V8) should now work without errors. Test each tab to see:
- Which approach prevents duplication in multi-turn conversations
- Whether thinking blocks persist during streaming (this is the main challenge with individual message yielding)
- If any approach works better than others in Gradio 5.10

The test server should be running. Try each tab and report which approach works best for preventing duplication while maintaining thinking blocks.

---

**User**

@python (601-621) all versions simply replace the thinking blocks



---

**Cursor**


In Gradio 5.10, yielding individual assistant messages replaces the previous assistant message, so thinking blocks get replaced during streaming. Investigating how to preserve thinking blocks while streaming in Gradio 5.10:



In Gradio 5.10, yielding assistant messages replaces the previous assistant message, so thinking blocks get replaced during streaming. Creating a test approach that preserves thinking blocks by yielding them along with the answer:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

IMPORTANT: Gradio 5.10 ChatInterface does NOT support yielding full history lists.
It only accepts individual messages. Approaches that yield lists (V1, V2, V4, V5, V6, V7)
have been modified to yield individual messages for Gradio 5.10 compatibility.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Yield thinking blocks again with each answer update
# This is inefficient but might preserve thinking blocks in Gradio 5.10
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 9: Re-yield thinking blocks with each answer update to preserve them.
    
    In Gradio 5.10, yielding assistant messages replaces previous ones.
    This approach re-yields thinking blocks along with the answer to keep them visible.
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Yield user message
    yield {"role": "user", "content": message}
    
    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - re-yield thinking blocks with each update
    # This keeps them visible even though we're replacing messages
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Re-yield thinking blocks, then the answer
        # This is inefficient but might work in Gradio 5.10
        yield search_started
        yield search_completed
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
        with gr.Tab("V8: Individual Only"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Individual Only - always yield individual messages"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )
```

```python
        **Key Test:**
        - V3/V8 don't duplicate but clear thinking blocks (Gradio 5.10 replaces assistant messages)
        - V1/V2/V4/V5/V6/V7 modified to yield individual messages (all behave like V3/V8)
        - V9 attempts to preserve thinking blocks by re-yielding them (inefficient but might work)
        - **Conclusion**: Gradio 5.10 doesn't support preserving multiple assistant messages when streaming
```


Fixing V9 and updating the documentation:


```python
        with gr.Tab("V8: Individual Only"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )
```

```python
        **Key Finding:**
        - Gradio 5.10 doesn't support yielding full history lists (causes AttributeError)
        - All approaches (V1-V8) yield individual messages, which replace previous assistant messages
        - This means thinking blocks get replaced during streaming (fundamental limitation)
        - V9 attempts to preserve thinking blocks by re-yielding them (inefficient workaround)
        - **Conclusion**: Gradio 5.10 cannot preserve multiple assistant messages when streaming
```


Adding V9 and updating the documentation:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

IMPORTANT: Gradio 5.10 ChatInterface does NOT support yielding full history lists.
It only accepts individual messages. Approaches that yield lists (V1, V2, V4, V5, V6, V7)
have been modified to yield individual messages for Gradio 5.10 compatibility.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Yield thinking blocks again with each answer update
# This is inefficient but might preserve thinking blocks in Gradio 5.10
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 9: Re-yield thinking blocks with each answer update to preserve them.
    
    In Gradio 5.10, yielding assistant messages replaces previous ones.
    This approach re-yields thinking blocks along with the answer to keep them visible.
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Yield user message
    yield {"role": "user", "content": message}
    
    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - re-yield thinking blocks with each update
    # This keeps them visible even though we're replacing messages
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Re-yield thinking blocks, then the answer
        # This is inefficient but might work in Gradio 5.10
        yield search_started
        yield search_completed
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V3 doesn't duplicate but clears thinking blocks
        - V1/V2/V4/V5/V6/V7 duplicate (yield full history lists)
        - V8 always yields individual messages (test if thinking blocks persist)
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 5.10.0 to test if behavior differs from Gradio 6.x.

IMPORTANT: Gradio 5.10 ChatInterface does NOT support yielding full history lists.
It only accepts individual messages. Approaches that yield lists (V1, V2, V4, V5, V6, V7)
have been modified to yield individual messages for Gradio 5.10 compatibility.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Yield thinking blocks again with each answer update
# This is inefficient but might preserve thinking blocks in Gradio 5.10
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 9: Re-yield thinking blocks with each answer update to preserve them.
    
    In Gradio 5.10, yielding assistant messages replaces previous ones.
    This approach re-yields thinking blocks along with the answer to keep them visible.
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Yield user message
    yield {"role": "user", "content": message}
    
    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - re-yield thinking blocks with each update
    # This keeps them visible even though we're replacing messages
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Re-yield thinking blocks, then the answer
        # This is inefficient but might work in Gradio 5.10
        yield search_started
        yield search_completed
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Finding:**
        - Gradio 5.10 doesn't support yielding full history lists (causes AttributeError)
        - All approaches (V1-V8) yield individual messages, which replace previous assistant messages
        - This means thinking blocks get replaced during streaming (fundamental limitation)
        - V9 attempts to preserve thinking blocks by re-yielding them (inefficient workaround)
        - **Conclusion**: Gradio 5.10 cannot preserve multiple assistant messages when streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


## Summary

**Finding**: Gradio 5.10 ChatInterface has a limitation: when yielding individual assistant messages, each new assistant message replaces the previous one. This means:
- Thinking blocks (assistant messages with metadata) get replaced when streaming the answer
- This affects all approaches (V1–V8) because Gradio 5.10 doesn't support yielding full history lists

**What I've done**:
1. Added V9: re-yields thinking blocks with each answer update (inefficient workaround)
2. Updated documentation to note this limitation

**Options**:
1. Upgrade back to Gradio 6.x (supports yielding full history lists, which may preserve thinking blocks)
2. Accept the limitation in Gradio 5.10 (thinking blocks disappear during streaming)
3. Test V9 to see if re-yielding preserves them (inefficient but might work)

**Recommendation**: Upgrade back to Gradio 6.x and fix the duplication issue there, since Gradio 5.10 doesn't support the required pattern.

Should I:
1. Upgrade back to Gradio 6.x and investigate the duplication issue there?
2. Test V9 to see if re-yielding thinking blocks works?
3. Document this as a known limitation and move forward with Gradio 6.x?

---

**User**

in gradio 5 v9 did not help, lets try it with gradio 6 with re-yelding thinking blocsk

---

**Cursor**


Upgrading to Gradio 6.x and updating the test script to support re-yielding thinking blocks:



Updating the test script for Gradio 6.x and modifying V9 to use full history lists with re-yielded thinking blocks:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Yield thinking blocks again with each answer update
# This is inefficient but might preserve thinking blocks in Gradio 5.10
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 9: Re-yield thinking blocks with each answer update to preserve them.
    
    In Gradio 5.10, yielding assistant messages replaces previous ones.
    This approach re-yields thinking blocks along with the answer to keep them visible.
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Yield user message
    yield {"role": "user", "content": message}
    
    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - re-yield thinking blocks with each update
    # This keeps them visible even though we're replacing messages
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Re-yield thinking blocks, then the answer
        # This is inefficient but might work in Gradio 5.10
        yield search_started
        yield search_completed
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 5.10, we need to specify type="messages" to use message dicts
    return gr.ChatInterface(
        fn=handler_fn,
        type="messages",  # Required in Gradio 5.10 for message dict format
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
            type="messages",  # Also specify for Chatbot component
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Finding:**
        - Gradio 5.10 doesn't support yielding full history lists (causes AttributeError)
        - All approaches (V1-V8) yield individual messages, which replace previous assistant messages
        - This means thinking blocks get replaced during streaming (fundamental limitation)
        - V9 attempts to preserve thinking blocks by re-yielding them (inefficient workaround)
        - **Conclusion**: Gradio 5.10 cannot preserve multiple assistant messages when streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Yield thinking blocks again with each answer update
# This is inefficient but might preserve thinking blocks in Gradio 5.10
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 9: Re-yield thinking blocks with each answer update to preserve them.
    
    In Gradio 5.10, yielding assistant messages replaces previous ones.
    This approach re-yields thinking blocks along with the answer to keep them visible.
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Yield user message
    yield {"role": "user", "content": message}
    
    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - re-yield thinking blocks with each update
    # This keeps them visible even though we're replacing messages
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Re-yield thinking blocks, then the answer
        # This is inefficient but might work in Gradio 5.10
        yield search_started
        yield search_completed
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Finding:**
        - Gradio 5.10 doesn't support yielding full history lists (causes AttributeError)
        - All approaches (V1-V8) yield individual messages, which replace previous assistant messages
        - This means thinking blocks get replaced during streaming (fundamental limitation)
        - V9 attempts to preserve thinking blocks by re-yielding them (inefficient workaround)
        - **Conclusion**: Gradio 5.10 cannot preserve multiple assistant messages when streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Finding:**
        - Gradio 5.10 doesn't support yielding full history lists (causes AttributeError)
        - All approaches (V1-V8) yield individual messages, which replace previous assistant messages
        - This means thinking blocks get replaced during streaming (fundamental limitation)
        - V9 attempts to preserve thinking blocks by re-yielding them (inefficient workaround)
        - **Conclusion**: Gradio 5.10 cannot preserve multiple assistant messages when streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 1: Build full history, yield with list() constructor.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 2: Build full history, yield with .copy().
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 4: Build full history, track initial length, yield full history.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Restoring V4, V5, V6, and V7 to work with Gradio 6.x:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 5: Use history list directly, don't create new list.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    
    In Gradio 5.10, we can only yield individual messages, so this becomes similar to V3/V8.
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # In Gradio 5.10, we can only yield individual messages
    # Yield user message individually
    yield {"role": "user", "content": message}
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Restoring V6 and V7 for Gradio 6.x:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
# NOTE: This approach does NOT work in Gradio 5.10 - ChatInterface doesn't accept lists
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 6: Create completely new dict objects for all messages.
    
    WARNING: This approach fails in Gradio 5.10 because ChatInterface doesn't accept full history lists.
    Modified to yield individual messages for Gradio 5.10 compatibility.
    """
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # In Gradio 5.10, we can only yield individual messages
    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


## Summary of Changes

1. Upgraded Gradio to 6.1.0
2. Restored all approaches (V1-V7) to work with Gradio 6.x (yielding full history lists)
3. Updated V9 to use full history lists with thinking blocks preserved:
   - Builds full history including thinking blocks
   - Yields full history list with each answer update
   - Thinking blocks remain in the list, so they should persist during streaming

## What to Test

The test server should be running. Test V9 specifically:
- V9 should preserve thinking blocks (they're in the full history list)
- V9 should prevent duplication (yielding full history replaces ChatInterface's internal state)

Also test V1-V7 to see if they still duplicate in Gradio 6.x, and compare with V9.

V9 is the key test: it yields full history lists (like V1-V7) but ensures thinking blocks are always included in the list, so they should persist during streaming.

Test V9 and report whether it:
1. Prevents duplication
2. Preserves thinking blocks during streaming

---

**User**

python
import gradio as gr
from uuid import uuid4

def clear_history():
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4() 

def chat_fn(message, history, session_uuid):
    # Your chat logic, using history and session_uuid
    return f"Hello from session {session_uuid}!"

with gr.Blocks() as demo:
    chatbot_comp = gr.Chatbot(type="messages")
    # Link the clear button to reset history and state
    clear_btn = gr.Button("Clear")
    clear_btn.click(clear_history, outputs=[chatbot_comp, gr.State(value=uuid4())]) # Resetting state with new UUID

    # Use gr.ChatInterface for simplicity, adding the state input
    gr.ChatInterface(
        chat_fn, 
        additional_inputs=[gr.State(value=uuid4())], # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )

demo.launch()

---

**Cursor**


Adding a test approach (V10) using `gr.State` to track yielded messages and prevent duplication:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State to track yielded messages and prevent duplication.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    return gr.ChatInterface(
        fn=handler_fn,
        title=title,
        description=f"Test: {title}",
        chatbot=gr.Chatbot(
            label="Test Chat",
            height=400,
        ),
    )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 10: Use gr.State to track yielded messages and prevent duplication
def chat_handler_v10_with_state(message: str, history: list[dict], state: dict | None = None) -> Generator[list[dict], None, None]:
    """Approach 10: Use gr.State to track what's been yielded and prevent duplication.
    
    This approach uses state to track the last yielded history length,
    and only yields new messages or full history updates.
    """
    print(f"[V10] Message: {message}, History length: {len(history)}, State: {state}")
    
    # Initialize state if needed
    if state is None:
        state = {"last_yielded_length": 0}
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message if not already in history
    user_msg = {"role": "user", "content": message}
    if len(gradio_history) == 0 or gradio_history[-1].get("content") != message:
        gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        state["last_yielded_length"] = len(gradio_history)
        time.sleep(0.2)
    
    # Return state for next turn
    return state


# Test Approach 6: Yield full history but ensure message dicts are new objects
```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State to track yielded messages and prevent duplication.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"last_yielded_length": 0})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )

        with gr.Tab("V10: With State Tracking"):
            create_test_interface(
                chat_handler_v10_with_state,
                "V10: Use gr.State to track yielded messages",
                use_state=True
            )


if __name__ == "__main__":
```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State to track yielded messages and prevent duplication.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"last_yielded_length": 0})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - V10: Use gr.State to track yielded messages (test if state tracking prevents duplication)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Finding where to add V10:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State to track yielded messages and prevent duplication.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"last_yielded_length": 0})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - V10: Use gr.State to track yielded messages (test if state tracking prevents duplication)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )

        with gr.Tab("V10: With State Tracking"):
            create_test_interface(
                chat_handler_v10_with_state,
                "V10: Use gr.State to track yielded messages",
                use_state=True
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Adding V10 before V6:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State to track yielded messages and prevent duplication.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 10: Use gr.State to track yielded messages and prevent duplication
def chat_handler_v10_with_state(message: str, history: list[dict], state: dict | None = None) -> Generator[list[dict], None, None]:
    """Approach 10: Use gr.State to track what's been yielded and prevent duplication.
    
    This approach uses state to track the last yielded history length,
    and only yields new messages or full history updates.
    """
    print(f"[V10] Message: {message}, History length: {len(history)}, State: {state}")
    
    # Initialize state if needed
    if state is None:
        state = {"last_yielded_length": 0}
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message if not already in history
    user_msg = {"role": "user", "content": message}
    if len(gradio_history) == 0 or gradio_history[-1].get("content") != message:
        gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        state["last_yielded_length"] = len(gradio_history)
        time.sleep(0.2)
    
    # Return state for next turn
    return state


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"last_yielded_length": 0})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - V10: Use gr.State to track yielded messages (test if state tracking prevents duplication)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )

        with gr.Tab("V10: With State Tracking"):
            create_test_interface(
                chat_handler_v10_with_state,
                "V10: Use gr.State to track yielded messages",
                use_state=True
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"last_yielded_length": 0})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )
```


Checking if `create_test_interface` already exists:



The function already exists. Verifying V10 is added correctly:



V10 is added. Fixing the return statement — ChatInterface handlers that yield should not return values:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State to track yielded messages and prevent duplication.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 10: Use gr.State to track yielded messages and prevent duplication
def chat_handler_v10_with_state(message: str, history: list[dict], state: dict | None = None) -> Generator[list[dict], None, None]:
    """Approach 10: Use gr.State to track what's been yielded and prevent duplication.
    
    This approach uses state to track the last yielded history length,
    and only yields new messages or full history updates.
    """
    print(f"[V10] Message: {message}, History length: {len(history)}, State: {state}")
    
    # Initialize state if needed
    if state is None:
        state = {"last_yielded_length": 0}
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message if not already in history
    user_msg = {"role": "user", "content": message}
    if len(gradio_history) == 0 or gradio_history[-1].get("content") != message:
        gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    state["last_yielded_length"] = len(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        state["last_yielded_length"] = len(gradio_history)
        time.sleep(0.2)
    
    # Note: State is automatically preserved by gr.State component


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"last_yielded_length": 0})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - V10: Use gr.State to track yielded messages (test if state tracking prevents duplication)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )

        with gr.Tab("V10: With State Tracking"):
            create_test_interface(
                chat_handler_v10_with_state,
                "V10: Use gr.State to track yielded messages",
                use_state=True
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


V10 is added. Summary:

## Summary

Added **V10: With State Tracking** using `gr.State` to track yielded messages and prevent duplication.

### V10 Approach:
- Uses `gr.State` to track the last yielded history length
- Yields full history lists (preserves thinking blocks)
- Tracks state to prevent re-yielding duplicate messages
- Uses `additional_inputs=[gr.State(...)]` in ChatInterface

### What to Test:

The test server should be running. Test **V10** specifically:
1. Does it prevent duplication? (State tracking should help)
2. Does it preserve thinking blocks? (Full history lists should preserve them)

V10 combines:
- Full history lists (like V9) to preserve thinking blocks
- State tracking (from your example) to prevent duplication

Test V10 and report whether it:
- Prevents duplication in multi-turn conversations
- Preserves thinking blocks during streaming

This should be the approach that works.

---

**User**

https://www.gradio.app/docs/gradio/state 

---

**Cursor**


Reviewing the Gradio State docs and updating V10 to use State correctly. State persists values across function calls, but with streaming generators it may not update between yields. Updating V10 accordingly:



Per the [Gradio State documentation](https://www.gradio.app/docs/gradio/state), `gr.State` persists values across function calls, not between yields in a generator. Updating V10 to use State correctly and adding a note about its limitations:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State to track yielded messages and prevent duplication.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 10: Use gr.State to track session and prevent duplication
def chat_handler_v10_with_state(message: str, history: list[dict], state: dict | None = None) -> Generator[list[dict], None, None]:
    """Approach 10: Use gr.State to track session state and prevent duplication.
    
    According to Gradio State docs (https://www.gradio.app/docs/gradio/state):
    - State persists values across function calls (not between yields in generators)
    - State is deepcopied
    - State is automatically cleared when user closes tab/refreshes
    
    This approach uses state to track what's been processed in previous turns,
    and yields full history lists to preserve thinking blocks.
    """
    print(f"[V10] Message: {message}, History length: {len(history)}, State: {state}")
    
    # Initialize state if needed
    if state is None:
        state = {"turn_count": 0, "processed_messages": set()}
    
    state["turn_count"] += 1
    
    # Build full history from provided history (ChatInterface manages this)
    gradio_history = list(history) if history else []
    
    # Add user message if not already in history
    user_msg = {"role": "user", "content": message}
    user_msg_id = f"user_{message}"
    if user_msg_id not in state["processed_messages"]:
        gradio_history.append(user_msg)
        state["processed_messages"].add(user_msg_id)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)
    
    # State is automatically preserved by gr.State component for next turn


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"last_yielded_length": 0})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - V10: Use gr.State to track yielded messages (test if state tracking prevents duplication)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )

        with gr.Tab("V10: With State Tracking"):
            create_test_interface(
                chat_handler_v10_with_state,
                "V10: Use gr.State to track yielded messages",
                use_state=True
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State to track yielded messages and prevent duplication.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 10: Use gr.State to track session and prevent duplication
def chat_handler_v10_with_state(message: str, history: list[dict], state: dict | None = None) -> Generator[list[dict], None, None]:
    """Approach 10: Use gr.State to track session state and prevent duplication.
    
    According to Gradio State docs (https://www.gradio.app/docs/gradio/state):
    - State persists values across function calls (not between yields in generators)
    - State is deepcopied
    - State is automatically cleared when user closes tab/refreshes
    
    This approach uses state to track what's been processed in previous turns,
    and yields full history lists to preserve thinking blocks.
    """
    print(f"[V10] Message: {message}, History length: {len(history)}, State: {state}")
    
    # Initialize state if needed
    if state is None:
        state = {"turn_count": 0, "processed_messages": set()}
    
    state["turn_count"] += 1
    
    # Build full history from provided history (ChatInterface manages this)
    gradio_history = list(history) if history else []
    
    # Add user message if not already in history
    user_msg = {"role": "user", "content": message}
    user_msg_id = f"user_{message}"
    if user_msg_id not in state["processed_messages"]:
        gradio_history.append(user_msg)
        state["processed_messages"].add(user_msg_id)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)
    
    # State is automatically preserved by gr.State component for next turn


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"turn_count": 0, "processed_messages": set()})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - V10: Use gr.State to track yielded messages (test if state tracking prevents duplication)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )

        with gr.Tab("V10: With State Tracking"):
            create_test_interface(
                chat_handler_v10_with_state,
                "V10: Use gr.State to track yielded messages",
                use_state=True
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```

```python
V10 uses gr.State (https://www.gradio.app/docs/gradio/state) to track session state.
Note: State persists across function calls, not between yields in generators.
```


Fixing the docstring:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests different approaches to yielding history in ChatInterface
to find the correct pattern that prevents duplication while maintaining streaming.

NOTE: This script is configured for Gradio 6.1.0 to test history management approaches.

Gradio 6.x supports yielding full history lists, which allows preserving thinking blocks
during streaming. This is a key advantage over Gradio 5.10.

V10 uses gr.State (https://www.gradio.app/docs/gradio/state) to track session state.
Note: State persists across function calls, not between yields in generators.

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Test Approaches:
    - V1: Full History with list() constructor
    - V2: Full History with .copy()
    - V3: Incremental - yield only new messages (may break streaming)
    - V4: Full History with tracking and message copying
    - V5: Direct History - mutate ChatInterface's list directly
    - V6: Full History with completely new dict objects
    - V7: Hybrid - new messages individually, full history for updates
    - V8: Individual Only - always yield individual messages, never full history

Testing Steps:
    1. Open each tab
    2. Send first message: "Привет"
    3. Wait for complete response
    4. Send second message: "Как дела?"
    5. Check if first conversation is duplicated
    6. Note which approach works correctly

Expected Behavior:
    - First turn: User message → Search started → Search completed → Streaming answer
    - Second turn: Should show only new messages, NOT duplicate the first conversation
    - All thinking blocks should persist during streaming
    - Answer should stream incrementally
"""
from __future__ import annotations

import time
from collections.abc import Generator

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search_started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {
            "title": "🧠 Поиск информации в базе знаний",
        },
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search_completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {
            "title": "✅ Поиск завершен",
        },
    }


def simulate_thinking_block(tool_name: str) -> dict:
    """Simulate generic thinking block for non-search tools."""
    return {
        "role": "assistant",
        "content": f"Использую инструмент: {tool_name}",
        "metadata": {
            "title": "🧠 Размышление",
        },
    }


# Test Approach 1: Yield full history with list() constructor
def chat_handler_v1_full_history_list(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 1: Build full history, yield with list() constructor."""
    print(f"[V1] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 2: Yield full history with .copy()
def chat_handler_v2_full_history_copy(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 2: Build full history, yield with .copy()."""
    print(f"[V2] Message: {message}, History length: {len(history)}")

    # Build full history
    gradio_history = list(history) if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history.copy()

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history.copy()

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history.copy()

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history.copy()
        time.sleep(0.2)


# Test Approach 3: Yield only new messages (incremental)
def chat_handler_v3_incremental(message: str, history: list[dict]) -> Generator[dict | str, None, None]:
    """Approach 3: Yield only new messages incrementally."""
    print(f"[V3] Message: {message}, History length: {len(history)}")

    # Yield user message
    yield {"role": "user", "content": message}

    # Yield search started
    search_started = simulate_search_started(message)
    yield search_started

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started (yield updated message)
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started

    # Yield search completed
    search_completed = simulate_search_completed(5)
    yield search_completed

    # Stream answer chunks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        yield {"role": "assistant", "content": answer}
        time.sleep(0.2)


# Test Approach 4: Yield full history but track what's already been yielded
def chat_handler_v4_tracked_full_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 4: Build full history, track initial length, yield full history."""
    print(f"[V4] Message: {message}, History length: {len(history)}")

    # Build full history from provided history
    gradio_history = []
    if history:
        # Copy messages to avoid mutating ChatInterface's internal state
        for msg in history:
            if isinstance(msg, dict):
                gradio_history.append(dict(msg))
            else:
                gradio_history.append(msg)

    initial_length = len(gradio_history)
    print(f"[V4] Initial history length: {initial_length}")

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 5: Use history directly without copying
def chat_handler_v5_direct_history(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 5: Use history list directly, don't create new list."""
    print(f"[V5] Message: {message}, History length: {len(history)}")

    # Use history directly (mutate ChatInterface's list)
    gradio_history = history if history else []

    # Add user message
    gradio_history.append({"role": "user", "content": message})

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield gradio_history

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield gradio_history

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield gradio_history

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield gradio_history
        time.sleep(0.2)


# Test Approach 7: Hybrid - yield individual new messages but maintain full context for updates
def chat_handler_v7_hybrid(message: str, history: list[dict]) -> Generator[dict | list[dict], None, None]:
    """Approach 7: Hybrid - yield new messages individually, but full history for updates.
    
    This combines V3's non-duplication with full-history streaming:
    - New messages: yield individually (prevents duplication)
    - Updates: yield full history (preserves thinking blocks during streaming)
    """
    print(f"[V7] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking
    gradio_history = list(history) if history else []
    
    # Yield user message individually (new message)
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually (new message)
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield full history to preserve thinking blocks
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Yield search completed individually (new message)
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield full history to preserve thinking blocks
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history during streaming to preserve thinking blocks
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 8: Always yield individual messages, track state internally
def chat_handler_v8_individual_only(message: str, history: list[dict]) -> Generator[dict, None, None]:
    """Approach 8: Always yield individual messages, never full history lists.
    
    This should prevent duplication (like V3) but we need to find a way to
    preserve thinking blocks during streaming updates.
    """
    print(f"[V8] Message: {message}, History length: {len(history)}")
    
    # Build full history for internal tracking only
    gradio_history = list(history) if history else []
    
    # Yield user message individually
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    yield user_msg
    
    # Yield search started individually
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield search_started
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started - yield updated message individually
    # ChatInterface should replace the last message with same metadata
    search_started["content"] = f"Ищу: {message.upper()}"
    yield search_started
    
    # Yield search completed individually
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield search_completed
    
    # Stream answer - yield updated answer message individually each time
    # This might cause thinking blocks to disappear, but let's test
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        answer_msg = {"role": "assistant", "content": answer}
        yield answer_msg
        time.sleep(0.2)


# Test Approach 9: Re-yield thinking blocks with full history to preserve them
def chat_handler_v9_preserve_thinking_blocks(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 9: Re-yield thinking blocks with full history to preserve them during streaming.
    
    This approach builds full history and re-includes thinking blocks with each answer update.
    This should preserve thinking blocks while preventing duplication (by yielding full history).
    """
    print(f"[V9] Message: {message}, History length: {len(history)}")
    
    # Build full history
    gradio_history = list(history) if history else []
    
    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    # This keeps thinking blocks visible while streaming the answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


# Test Approach 10: Use gr.State to track session and prevent duplication
def chat_handler_v10_with_state(message: str, history: list[dict], state: dict | None = None) -> Generator[list[dict], None, None]:
    """Approach 10: Use gr.State to track session state and prevent duplication.
    
    According to Gradio State docs (https://www.gradio.app/docs/gradio/state):
    - State persists values across function calls (not between yields in generators)
    - State is deepcopied
    - State is automatically cleared when user closes tab/refreshes
    
    This approach uses state to track what's been processed in previous turns,
    and yields full history lists to preserve thinking blocks.
    """
    print(f"[V10] Message: {message}, History length: {len(history)}, State: {state}")
    
    # Initialize state if needed
    if state is None:
        state = {"turn_count": 0, "processed_messages": set()}
    
    state["turn_count"] += 1
    
    # Build full history from provided history (ChatInterface manages this)
    gradio_history = list(history) if history else []
    
    # Add user message if not already in history
    user_msg = {"role": "user", "content": message}
    user_msg_id = f"user_{message}"
    if user_msg_id not in state["processed_messages"]:
        gradio_history.append(user_msg)
        state["processed_messages"].add(user_msg_id)
    
    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)
    
    # Simulate tool execution
    time.sleep(0.5)
    
    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)
    
    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)
    
    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)
    
    # State is automatically preserved by gr.State component for next turn


# Test Approach 6: Yield full history but ensure message dicts are new objects
def chat_handler_v6_new_dicts(message: str, history: list[dict]) -> Generator[list[dict], None, None]:
    """Approach 6: Create completely new dict objects for all messages."""
    print(f"[V6] Message: {message}, History length: {len(history)}")

    # Build full history with completely new dict objects
    gradio_history = []
    if history:
        for msg in history:
            if isinstance(msg, dict):
                # Create completely new dict with same content
                new_msg = {
                    "role": msg.get("role"),
                    "content": msg.get("content"),
                }
                if "metadata" in msg and msg["metadata"] is not None:
                    new_msg["metadata"] = dict(msg["metadata"])
                gradio_history.append(new_msg)
            else:
                gradio_history.append(msg)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]

    # Stream answer
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer)
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        yield [dict(msg) if isinstance(msg, dict) else msg for msg in gradio_history]
        time.sleep(0.2)


def create_test_interface(handler_fn, title: str, use_state: bool = False):
    """Create a test ChatInterface with the given handler."""
    # In Gradio 6.x, type="messages" is the default for ChatInterface
    if use_state:
        # For V10, we need to add state as an additional input
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
            additional_inputs=[gr.State(value={"turn_count": 0, "processed_messages": set()})],
        )
    else:
        return gr.ChatInterface(
            fn=handler_fn,
            title=title,
            description=f"Test: {title}",
            chatbot=gr.Chatbot(
                label="Test Chat",
                height=400,
            ),
        )


with gr.Blocks(title="Gradio ChatInterface History Tests") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Tests")
    gr.Markdown(
        """
        **Testing with Gradio 5.10.0** - Compare behavior with Gradio 6.x

        Test different approaches to yielding history in ChatInterface.
        Try multiple turns to see if history duplicates.

        **Instructions:**
        1. Send a message in each tab
        2. Wait for response to complete
        3. Send a second message
        4. Check if the first conversation is duplicated
        5. Note which approach works correctly (no duplication + thinking blocks persist)

        **Key Test:**
        - V1-V7: Yield full history lists (test if they duplicate)
        - V3/V8: Yield individual messages (test if they preserve thinking blocks)
        - V9: Re-yield full history with thinking blocks preserved (test if this prevents duplication AND preserves thinking blocks)
        - V10: Use gr.State to track yielded messages (test if state tracking prevents duplication)
        - **Goal**: Find approach that prevents duplication while preserving thinking blocks during streaming
        """
    )

    with gr.Tabs():
        with gr.Tab("V1: Full History (list())"):
            create_test_interface(
                chat_handler_v1_full_history_list,
                "V1: Full History with list() constructor"
            )

        with gr.Tab("V2: Full History (.copy())"):
            create_test_interface(
                chat_handler_v2_full_history_copy,
                "V2: Full History with .copy()"
            )

        with gr.Tab("V3: Incremental (new messages only)"):
            create_test_interface(
                chat_handler_v3_incremental,
                "V3: Incremental - yield only new messages"
            )

        with gr.Tab("V4: Tracked Full History"):
            create_test_interface(
                chat_handler_v4_tracked_full_history,
                "V4: Full History with tracking"
            )

        with gr.Tab("V5: Direct History (mutate)"):
            create_test_interface(
                chat_handler_v5_direct_history,
                "V5: Direct History - mutate ChatInterface's list"
            )

        with gr.Tab("V6: New Dicts (deep copy)"):
            create_test_interface(
                chat_handler_v6_new_dicts,
                "V6: Full History with completely new dict objects"
            )

        with gr.Tab("V7: Hybrid (new msgs + full history)"):
            create_test_interface(
                chat_handler_v7_hybrid,
                "V7: Hybrid - new messages individually, full history for updates"
            )

        with gr.Tab("V8: Individual Only (no lists)"):
            create_test_interface(
                chat_handler_v8_individual_only,
                "V8: Always yield individual messages, never full history"
            )

        with gr.Tab("V9: Preserve Thinking Blocks"):
            create_test_interface(
                chat_handler_v9_preserve_thinking_blocks,
                "V9: Re-yield thinking blocks with each answer update"
            )

        with gr.Tab("V10: With State Tracking"):
            create_test_interface(
                chat_handler_v10_with_state,
                "V10: Use gr.State to track yielded messages",
                use_state=True
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)


```


Updated V10 based on the [Gradio State documentation](https://www.gradio.app/docs/gradio/state). Changes:

## Updates to V10

1. State structure: tracks `turn_count` and `processed_messages` (set) to avoid reprocessing
2. Documentation: added references to the Gradio State docs and clarified that State persists across function calls, not between yields in generators
3. Approach: still yields full history lists to preserve thinking blocks; State tracks session-level information

## Notes from the documentation

- State persists values across function calls (not between yields in a generator)
- State values are deepcopied
- State is cleared when the user closes the tab or refreshes
- State can have a `time_to_live` parameter for automatic cleanup

## Current V10 behavior

- Uses State to track processed messages across turns (may help prevent duplication)
- Yields full history lists (preserves thinking blocks during streaming)
- Combines both approaches

The test server should be running. Test V10 to see if:
1. State tracking prevents duplication across turns
2. Full history lists preserve thinking blocks during streaming

This combines State-based session tracking with full history yielding.

---

**User**

did not help, reduce test tabs to 1 and refer to

For more control, especially within gr.ChatInterface or when managing complex state. 
python
import gradio as gr
from uuid import uuid4

def clear_history():
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4() 

def chat_fn(message, history, session_uuid):
    # Your chat logic, using history and session_uuid
    return f"Hello from session {session_uuid}!"

with gr.Blocks() as demo:
    chatbot_comp = gr.Chatbot(type="messages")
    # Link the clear button to reset history and state
    clear_btn = gr.Button("Clear")
    clear_btn.click(clear_history, outputs=[chatbot_comp, gr.State(value=uuid4())]) # Resetting state with new UUID

    # Use gr.ChatInterface for simplicity, adding the state input
    gr.ChatInterface(
        chat_fn, 
        additional_inputs=[gr.State(value=uuid4())], # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )

demo.launch()
Key Concepts for Clearing History
gr.Chatbot Value: The chatbot's history is stored as its value, a list of message pairs.
Returning None: Clearing the gr.Chatbot component often involves returning None or an empty list to its value in a function.
Session State: If you use gr.State for memory, you must also reset the state (e.g., by returning a new uuid4() for unique sessions). 

---

**Cursor**


Simplifying to a single test tab using the explicit `gr.Chatbot` pattern with `gr.State`:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import gradio as gr
import time
from typing import Generator
from uuid import uuid4


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler with explicit chatbot management and state tracking.
    
    This follows the pattern from Gradio docs:
    - Uses explicit gr.Chatbot component
    - Uses gr.State for session tracking
    - Yields full history lists to preserve thinking blocks
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using explicit gr.Chatbot with gr.State

        This test uses the pattern from Gradio docs:
        - Explicit gr.Chatbot component
        - gr.State for session tracking
        - Full history lists to preserve thinking blocks

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated
        5. Check if thinking blocks persist during streaming
        6. Click "Clear" to reset history and state
        """
    )

    # Create explicit chatbot component
    chatbot_comp = gr.Chatbot(type="messages", label="Test Chat", height=400)

    # Create clear button
    clear_btn = gr.Button("Clear History")
    # Link the clear button to reset history and state
    clear_btn.click(
        clear_history,
        outputs=[chatbot_comp, gr.State(value=uuid4())]
    )

    # Use gr.ChatInterface with explicit chatbot component and state
    gr.ChatInterface(
        chat_handler,
        additional_inputs=[gr.State(value=uuid4())],  # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Simplified the test script to a single test following the pattern from your example. Changes:

## Changes

1. Single test tab: removed all V1–V10 variants
2. Explicit `gr.Chatbot` component: created separately and passed to `ChatInterface`
3. `gr.State` with UUID: uses `uuid4()` for session tracking, as in your example
4. Clear button: resets both chatbot history and state
5. Full history yielding: preserves thinking blocks during streaming

## Pattern

The test now follows this pattern:
```python
chatbot_comp = gr.Chatbot(type="messages")
gr.ChatInterface(
    chat_handler,
    additional_inputs=[gr.State(value=uuid4())],
    chatbot=chatbot_comp,
    type="messages"
)
```

This matches the Gradio docs pattern for managing chatbot history with state.

## What to Test

The test server should be running. Test:
1. Send first message: "Привет"
2. Wait for response (should see thinking blocks + streaming answer)
3. Send second message: "Как дела?"
4. Check if first conversation is duplicated
5. Check if thinking blocks persist during streaming
6. Click "Clear" to reset history and state

This should prevent duplication while preserving thinking blocks. Test and report results.

```python
chatbot_comp = gr.Chatbot(type="messages")
gr.ChatInterface(
    chat_handler,
    additional_inputs=[gr.State(value=uuid4())],
    chatbot=chatbot_comp,
    type="messages"
)
```

---

**User**

we can clear the chat and then stream the whole history + new chuncs, can we?

---

**Cursor**


Updating the test to clear the chat first, then stream the full history (previous messages + new chunks):


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import gradio as gr
import time
from typing import Generator
from uuid import uuid4


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[list[dict] | None, None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.
    
    Approach:
    1. Clear the chatbot (yield None)
    2. Build full history from provided history
    3. Add new messages and stream with full history
    
    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first
    yield None

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using explicit gr.Chatbot with gr.State

        This test uses the pattern from Gradio docs:
        - Explicit gr.Chatbot component
        - gr.State for session tracking
        - Full history lists to preserve thinking blocks

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated
        5. Check if thinking blocks persist during streaming
        6. Click "Clear" to reset history and state
        """
    )

    # Create explicit chatbot component
    chatbot_comp = gr.Chatbot(type="messages", label="Test Chat", height=400)

    # Create clear button
    clear_btn = gr.Button("Clear History")
    # Link the clear button to reset history and state
    clear_btn.click(
        clear_history,
        outputs=[chatbot_comp, gr.State(value=uuid4())]
    )

    # Use gr.ChatInterface with explicit chatbot component and state
    gr.ChatInterface(
        chat_handler,
        additional_inputs=[gr.State(value=uuid4())],  # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import gradio as gr
import time
from typing import Generator
from uuid import uuid4


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[list[dict] | None, None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.
    
    Approach:
    1. Clear the chatbot (yield None)
    2. Build full history from provided history
    3. Add new messages and stream with full history
    
    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first
    yield None

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Clear chat then stream full history

        This test uses a novel approach:
        1. Clear the chatbot (yield None)
        2. Stream full history (previous messages + new chunks)
        
        This ensures we always have the complete, authoritative history without duplication.

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create explicit chatbot component
    chatbot_comp = gr.Chatbot(type="messages", label="Test Chat", height=400)

    # Create clear button
    clear_btn = gr.Button("Clear History")
    # Link the clear button to reset history and state
    clear_btn.click(
        clear_history,
        outputs=[chatbot_comp, gr.State(value=uuid4())]
    )

    # Use gr.ChatInterface with explicit chatbot component and state
    gr.ChatInterface(
        chat_handler,
        additional_inputs=[gr.State(value=uuid4())],  # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Fixing linter errors:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[list[dict] | None, None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.
    
    Approach:
    1. Clear the chatbot (yield None)
    2. Build full history from provided history
    3. Add new messages and stream with full history
    
    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first
    yield None

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Clear chat then stream full history

        This test uses a novel approach:
        1. Clear the chatbot (yield None)
        2. Stream full history (previous messages + new chunks)
        
        This ensures we always have the complete, authoritative history without duplication.

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create explicit chatbot component
    chatbot_comp = gr.Chatbot(type="messages", label="Test Chat", height=400)

    # Create clear button
    clear_btn = gr.Button("Clear History")
    # Link the clear button to reset history and state
    clear_btn.click(
        clear_history,
        outputs=[chatbot_comp, gr.State(value=uuid4())]
    )

    # Use gr.ChatInterface with explicit chatbot component and state
    gr.ChatInterface(
        chat_handler,
        additional_inputs=[gr.State(value=uuid4())],  # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[list[dict] | None, None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield None)
    2. Build full history from provided history
    3. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first
    yield None
```


Fixing whitespace issues:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[list[dict] | None, None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield None)
    2. Build full history from provided history
    3. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first
    yield None

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Clear chat then stream full history

        This test uses a novel approach:
        1. Clear the chatbot (yield None)
        2. Stream full history (previous messages + new chunks)
        
        This ensures we always have the complete, authoritative history without duplication.

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create explicit chatbot component
    chatbot_comp = gr.Chatbot(type="messages", label="Test Chat", height=400)

    # Create clear button
    clear_btn = gr.Button("Clear History")
    # Link the clear button to reset history and state
    clear_btn.click(
        clear_history,
        outputs=[chatbot_comp, gr.State(value=uuid4())]
    )

    # Use gr.ChatInterface with explicit chatbot component and state
    gr.ChatInterface(
        chat_handler,
        additional_inputs=[gr.State(value=uuid4())],  # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
        This test uses a novel approach:
        1. Clear the chatbot (yield None)
        2. Stream full history (previous messages + new chunks)

        This ensures we always have the complete, authoritative history without duplication.
```


Fixing the last whitespace issue:



Fixing the whitespace issue:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[list[dict] | None, None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield None)
    2. Build full history from provided history
    3. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first
    yield None

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Clear chat then stream full history

        This test uses a novel approach:
        1. Clear the chatbot (yield None)
        2. Stream full history (previous messages + new chunks)

        This ensures we always have the complete, authoritative history without duplication.

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create explicit chatbot component
    chatbot_comp = gr.Chatbot(type="messages", label="Test Chat", height=400)

    # Create clear button
    clear_btn = gr.Button("Clear History")
    # Link the clear button to reset history and state
    clear_btn.click(
        clear_history,
        outputs=[chatbot_comp, gr.State(value=uuid4())]
    )

    # Use gr.ChatInterface with explicit chatbot component and state
    gr.ChatInterface(
        chat_handler,
        additional_inputs=[gr.State(value=uuid4())],  # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Linter errors fixed. The test script is ready.

## Summary of the Approach

The test now uses a clear-then-stream pattern:

1. Clear the chatbot first: `yield None` to reset the chatbot component
2. Build full history: reconstruct from the provided `history` parameter
3. Add new messages: append user message, thinking blocks, and answer
4. Stream with full history: yield the complete history list (previous + new) to preserve thinking blocks

## Why This Should Work

- Prevents duplication: clearing first ensures no leftover state
- Preserves thinking blocks: full history includes all previous thinking blocks
- Authoritative history: each yield is the complete, correct history

## What to Test

The test server should be running. Test:
1. Send first message: "Привет"
2. Watch: chatbot clears, then full conversation appears with thinking blocks
3. Send second message: "Как дела?"
4. Check: first conversation should NOT duplicate, thinking blocks should persist
5. Verify: streaming should work smoothly with full history

This approach should solve both issues: no duplication and preserved thinking blocks. Test and report results.

---

**User**

use two tabs with explicit chatbot: one with cleanding one with some approach that does not clean the history

|@powershell (718-732) 

---

**Cursor**


Fixing the error and creating two tabs: one with cleaning and one without:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler_with_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict] | None, None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield None)
    2. Build full history from provided history
    3. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[With Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first
    yield None

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


def chat_handler_without_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that streams full history without clearing first.

    Approach:
    1. Build full history from provided history
    2. Add new messages and stream with full history
    3. Do NOT clear - rely on ChatInterface to manage history

    This tests if ChatInterface can handle full history lists without clearing.
    """
    print(f"[Without Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Clear chat then stream full history

        This test uses a novel approach:
        1. Clear the chatbot (yield None)
        2. Stream full history (previous messages + new chunks)

        This ensures we always have the complete, authoritative history without duplication.

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create explicit chatbot component
    chatbot_comp = gr.Chatbot(type="messages", label="Test Chat", height=400)

    # Create clear button
    clear_btn = gr.Button("Clear History")
    # Link the clear button to reset history and state
    clear_btn.click(
        clear_history,
        outputs=[chatbot_comp, gr.State(value=uuid4())]
    )

    # Use gr.ChatInterface with explicit chatbot component and state
    gr.ChatInterface(
        chat_handler,
        additional_inputs=[gr.State(value=uuid4())],  # Initial state
        chatbot=chatbot_comp,
        type="messages"
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler_with_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict] | None, None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield None)
    2. Build full history from provided history
    3. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[With Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first
    yield None

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


def chat_handler_without_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that streams full history without clearing first.

    Approach:
    1. Build full history from provided history
    2. Add new messages and stream with full history
    3. Do NOT clear - rely on ChatInterface to manage history

    This tests if ChatInterface can handle full history lists without clearing.
    """
    print(f"[Without Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Two approaches with explicit chatbot

        **Tab 1: With Clearing**
        - Clears chatbot first (yield None)
        - Then streams full history (previous + new chunks)
        - Ensures authoritative history without duplication

        **Tab 2: Without Clearing**
        - Streams full history without clearing
        - Relies on ChatInterface to manage history
        - Tests if ChatInterface handles full history lists correctly

        **Instructions:**
        1. Test each tab separately
        2. Send first message: "Привет"
        3. Wait for response to complete
        4. Send second message: "Как дела?"
        5. Check if first conversation is duplicated
        6. Check if thinking blocks persist during streaming
        7. Click "Clear" to reset history and state
        """
    )

    with gr.Tabs():
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400)

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_1,
            )

        # Tab 2: Without Clearing
        with gr.Tab("Without Clearing"):
            # Create explicit chatbot component
            chatbot_comp_2 = gr.Chatbot(label="Test Chat (Without Clearing)", height=400)

            # Create clear button
            clear_btn_2 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_2.click(
                clear_history,
                outputs=[chatbot_comp_2, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_without_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_2,
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Created two tabs with explicit chatbot components:

## Changes Made

1. Fixed the error: removed `type="messages"` from `gr.Chatbot` (not needed in Gradio 6.x; ChatInterface handles it)
2. Created two tabs:
   - Tab 1: "With Clearing" — clears chatbot first (`yield None`), then streams full history
   - Tab 2: "Without Clearing" — streams full history without clearing

## What Each Tab Tests

**Tab 1: With Clearing**
- Clears the chatbot first (`yield None`)
- Then streams full history (previous messages + new chunks)
- Should prevent duplication by starting fresh each turn

**Tab 2: Without Clearing**
- Streams full history without clearing
- Relies on ChatInterface to manage history
- Tests if ChatInterface handles full history lists correctly

## What to Test

The test server should be running. Test both tabs:

1. Tab 1 (With Clearing):
   - Send "Привет" → watch it clear, then show full conversation
   - Send "Как дела?" → check if first conversation duplicates
   - Check if thinking blocks persist

2. Tab 2 (Without Clearing):
   - Send "Привет" → watch conversation appear
   - Send "Как дела?" → check if first conversation duplicates
   - Check if thinking blocks persist

Compare which approach works better for preventing duplication while preserving thinking blocks.

---

**User**

Ia @python (790-843) with cleaning focus on this tab

@python (859-866) 

---

**Cursor**


Fixing the "With Clearing" tab. Yielding `None` may not work with ChatInterface. Checking the error and updating the approach:



Yielding `None` isn't supported. Updating the "With Clearing" handler to yield an empty list `[]` to clear the chatbot:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler_with_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield empty list [])
    2. Build full history from provided history
    3. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[With Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first by yielding empty list
    yield []

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


def chat_handler_without_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that streams full history without clearing first.

    Approach:
    1. Build full history from provided history
    2. Add new messages and stream with full history
    3. Do NOT clear - rely on ChatInterface to manage history

    This tests if ChatInterface can handle full history lists without clearing.
    """
    print(f"[Without Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Two approaches with explicit chatbot

        **Tab 1: With Clearing**
        - Clears chatbot first (yield None)
        - Then streams full history (previous + new chunks)
        - Ensures authoritative history without duplication

        **Tab 2: Without Clearing**
        - Streams full history without clearing
        - Relies on ChatInterface to manage history
        - Tests if ChatInterface handles full history lists correctly

        **Instructions:**
        1. Test each tab separately
        2. Send first message: "Привет"
        3. Wait for response to complete
        4. Send second message: "Как дела?"
        5. Check if first conversation is duplicated
        6. Check if thinking blocks persist during streaming
        7. Click "Clear" to reset history and state
        """
    )

    with gr.Tabs():
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400)

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_1,
            )

        # Tab 2: Without Clearing
        with gr.Tab("Without Clearing"):
            # Create explicit chatbot component
            chatbot_comp_2 = gr.Chatbot(label="Test Chat (Without Clearing)", height=400)

            # Create clear button
            clear_btn_2 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_2.click(
                clear_history,
                outputs=[chatbot_comp_2, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_without_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_2,
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def chat_handler_with_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield empty list [])
    2. Build full history from provided history
    3. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[With Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first by yielding empty list
    yield []

    # Step 2: Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


def chat_handler_without_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that streams full history without clearing first.

    Approach:
    1. Build full history from provided history
    2. Add new messages and stream with full history
    3. Do NOT clear - rely on ChatInterface to manage history

    This tests if ChatInterface can handle full history lists without clearing.
    """
    print(f"[Without Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Two approaches with explicit chatbot

        **Tab 1: With Clearing**
        - Clears chatbot first (yield empty list [])
        - Then streams full history (previous + new chunks)
        - Ensures authoritative history without duplication

        **Tab 2: Without Clearing**
        - Streams full history without clearing
        - Relies on ChatInterface to manage history
        - Tests if ChatInterface handles full history lists correctly

        **Instructions:**
        1. Test each tab separately
        2. Send first message: "Привет"
        3. Wait for response to complete
        4. Send second message: "Как дела?"
        5. Check if first conversation is duplicated
        6. Check if thinking blocks persist during streaming
        7. Click "Clear" to reset history and state
        """
    )

    with gr.Tabs():
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400)

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_1,
            )

        # Tab 2: Without Clearing
        with gr.Tab("Without Clearing"):
            # Create explicit chatbot component
            chatbot_comp_2 = gr.Chatbot(label="Test Chat (Without Clearing)", height=400)

            # Create clear button
            clear_btn_2 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_2.click(
                clear_history,
                outputs=[chatbot_comp_2, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_without_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_2,
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Fixed the "With Clearing" tab. Changes:

## Fix Applied

Changed `yield None` to `yield []` (empty list) to clear the chatbot. ChatInterface expects a list, not `None`.

## Updated Approach

**Tab 1: With Clearing**
1. Clear the chatbot: `yield []` (empty list)
2. Build full history from provided history
3. Add new messages and stream with full history

This should:
- Clear the chatbot at the start of each turn
- Prevent duplication by starting fresh
- Preserve thinking blocks by including them in the full history

## What to Test

The test server should be running. Test the "With Clearing" tab:

1. Send first message: "Test 1"
   - Chatbot should clear, then show the full conversation with thinking blocks
2. Send second message: "Test 2"
   - First conversation should not duplicate
   - Thinking blocks should persist during streaming
3. Verify streaming works smoothly with full history

This should work now. Test and report results.

---

**User**

Does tab 1 activate uuid24 cleaning for subsequent turns?

@python (858-873) 

Тест 1


Тест 1


🧠 Поиск информации в базе знаний

Ищу: ТЕСТ 1


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Тест 1


туст 2


Тест 1

Тест 1


🧠 Поиск информации в базе знаний

Ищу: ТЕСТ 1


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Тест 1

туст 2


🧠 Поиск информации в базе знаний

Ищу: ТУСТ 2


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: туст 2


Вопрос 2


Тест 1

Тест 1


🧠 Поиск информации в базе знаний

Ищу: ТЕСТ 1


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Тест 1

туст 2

Тест 1

Тест 1


🧠 Поиск информации в базе знаний

Ищу: ТЕСТ 1


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Тест 1

туст 2


🧠 Поиск информации в базе знаний

Ищу: ТУСТ 2


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: туст 2

Вопрос 2


🧠 Поиск информации в базе знаний

Ищу: ВОПРОС 2


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Вопрос 2


Вопрос 3


Тест 1

Тест 1


🧠 Поиск информации в базе знаний

Ищу: ТЕСТ 1


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Тест 1

туст 2

Тест 1

Тест 1


🧠 Поиск информации в базе знаний

Ищу: ТЕСТ 1


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Тест 1

туст 2


🧠 Поиск информации в базе знаний

Ищу: ТУСТ 2


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: туст 2

Вопрос 2

Тест 1

Тест 1


🧠 Поиск информации в базе знаний

Ищу: ТЕСТ 1


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Тест 1

туст 2

Тест 1

Тест 1


🧠 Поиск информации в базе знаний

Ищу: ТЕСТ 1


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Тест 1

туст 2


🧠 Поиск информации в базе знаний

Ищу: ТУСТ 2


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: туст 2

Вопрос 2


🧠 Поиск информации в базе знаний

Ищу: ВОПРОС 2


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Вопрос 2

Вопрос 3


🧠 Поиск информации в базе знаний

Ищу: ВОПРОС 3


✅ Поиск завершен

Найдено статей: 5.

Это ответ на вопрос: Вопрос 3

---

**Cursor**


Duplication persists: `yield []` clears the display, but ChatInterface still accumulates history. The `history` parameter includes previous messages, so we rebuild from it. Fixing by deduplicating the received history and ensuring we only yield new messages:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def _deduplicate_history(history: list[dict]) -> list[dict]:
    """Deduplicate history by message content and role."""
    seen = set()
    deduplicated = []
    for msg in history:
        if isinstance(msg, dict):
            # Create a unique key from role and content
            content = msg.get("content", "")
            role = msg.get("role", "")
            # For messages with metadata, include metadata title in the key
            metadata = msg.get("metadata", {})
            metadata_title = metadata.get("title", "") if metadata else ""
            key = (role, content, metadata_title)
            if key not in seen:
                seen.add(key)
                deduplicated.append(msg)
        else:
            deduplicated.append(msg)
    return deduplicated


def chat_handler_with_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield empty list [])
    2. Deduplicate provided history (ChatInterface may have accumulated duplicates)
    3. Build full history from deduplicated history
    4. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[With Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first by yielding empty list
    yield []

    # Step 2: Deduplicate provided history (ChatInterface may have accumulated duplicates)
    # Then build full history from deduplicated history
    deduplicated_history = _deduplicate_history(history) if history else []
    gradio_history = list(deduplicated_history)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


def chat_handler_without_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that streams full history without clearing first.

    Approach:
    1. Build full history from provided history
    2. Add new messages and stream with full history
    3. Do NOT clear - rely on ChatInterface to manage history

    This tests if ChatInterface can handle full history lists without clearing.
    """
    print(f"[Without Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Two approaches with explicit chatbot

        **Tab 1: With Clearing**
        - Clears chatbot first (yield empty list [])
        - Then streams full history (previous + new chunks)
        - Ensures authoritative history without duplication

        **Tab 2: Without Clearing**
        - Streams full history without clearing
        - Relies on ChatInterface to manage history
        - Tests if ChatInterface handles full history lists correctly

        **Instructions:**
        1. Test each tab separately
        2. Send first message: "Привет"
        3. Wait for response to complete
        4. Send second message: "Как дела?"
        5. Check if first conversation is duplicated
        6. Check if thinking blocks persist during streaming
        7. Click "Clear" to reset history and state
        """
    )

    with gr.Tabs():
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400)

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_1,
            )

        # Tab 2: Without Clearing
        with gr.Tab("Without Clearing"):
            # Create explicit chatbot component
            chatbot_comp_2 = gr.Chatbot(label="Test Chat (Without Clearing)", height=400)

            # Create clear button
            clear_btn_2 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_2.click(
                clear_history,
                outputs=[chatbot_comp_2, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_without_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_2,
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Added deduplication. The issue: `yield []` clears the display, but ChatInterface still accumulates history internally, so the `history` parameter includes duplicates.

## Fix Applied

Added a `_deduplicate_history()` function that:
- Removes duplicate messages based on role, content, and metadata title
- Ensures each unique message appears only once
- Preserves message order

## Updated Approach

**Tab 1: With Clearing**
1. Clear the chatbot: `yield []`
2. Deduplicate provided history (ChatInterface may have accumulated duplicates)
3. Build full history from deduplicated history
4. Add new messages and stream with full history

## What This Should Fix

- Prevents duplication: deduplicates history before rebuilding
- Preserves thinking blocks: includes them in the full history
- Handles ChatInterface accumulation: removes duplicates that ChatInterface may have added

## Test Again

The test server should be running. Test the "With Clearing" tab:

1. Send "Тест 1" → should appear once
2. Send "туст 2" → "Тест 1" should not duplicate
3. Send "Вопрос 2" → previous conversations should not duplicate
4. Verify thinking blocks persist during streaming

The deduplication should prevent the exponential growth you saw (0 → 5 → 15 → 35). Test and report results.

---

**User**

@python (874-968) 

https://www.google.com/search?q=gradio+chatbot+how+to+clean 

https://discuss.huggingface.co/t/gradio-how-to-clear-everything-w-blocks/38238 

https://discuss.huggingface.co/t/clear-chat-interface/49866 

---

**Cursor**


Fixing the deduplication function (metadata can be a list) and using `chatbot_state` to clear ChatInterface, per the Gradio forum discussions:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def _deduplicate_history(history: list[dict]) -> list[dict]:
    """Deduplicate history by message content and role."""
    seen = set()
    deduplicated = []
    for msg in history:
        if isinstance(msg, dict):
            # Create a unique key from role and content
            content = msg.get("content", "")
            role = msg.get("role", "")
            # For messages with metadata, include metadata title in the key
            # Handle both dict and list metadata formats
            metadata = msg.get("metadata")
            if isinstance(metadata, dict):
                metadata_title = metadata.get("title", "")
            elif isinstance(metadata, list):
                # If metadata is a list, convert to string for hashing
                metadata_title = str(metadata)
            else:
                metadata_title = ""
            # Use string representation for hashable key
            key = (role, str(content), str(metadata_title))
            if key not in seen:
                seen.add(key)
                deduplicated.append(msg)
        else:
            deduplicated.append(msg)
    return deduplicated


def chat_handler_with_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield empty list [])
    2. Deduplicate provided history (ChatInterface may have accumulated duplicates)
    3. Build full history from deduplicated history
    4. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[With Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first by yielding empty list
    yield []

    # Step 2: Deduplicate provided history (ChatInterface may have accumulated duplicates)
    # Then build full history from deduplicated history
    deduplicated_history = _deduplicate_history(history) if history else []
    gradio_history = list(deduplicated_history)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


def chat_handler_without_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that streams full history without clearing first.

    Approach:
    1. Build full history from provided history
    2. Add new messages and stream with full history
    3. Do NOT clear - rely on ChatInterface to manage history

    This tests if ChatInterface can handle full history lists without clearing.
    """
    print(f"[Without Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Two approaches with explicit chatbot

        **Tab 1: With Clearing**
        - Clears chatbot first (yield empty list [])
        - Then streams full history (previous + new chunks)
        - Ensures authoritative history without duplication

        **Tab 2: Without Clearing**
        - Streams full history without clearing
        - Relies on ChatInterface to manage history
        - Tests if ChatInterface handles full history lists correctly

        **Instructions:**
        1. Test each tab separately
        2. Send first message: "Привет"
        3. Wait for response to complete
        4. Send second message: "Как дела?"
        5. Check if first conversation is duplicated
        6. Check if thinking blocks persist during streaming
        7. Click "Clear" to reset history and state
        """
    )

    with gr.Tabs():
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400)

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_1,
            )

        # Tab 2: Without Clearing
        with gr.Tab("Without Clearing"):
            # Create explicit chatbot component
            chatbot_comp_2 = gr.Chatbot(label="Test Chat (Without Clearing)", height=400)

            # Create clear button
            clear_btn_2 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_2.click(
                clear_history,
                outputs=[chatbot_comp_2, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_without_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_2,
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, and a new UUID for state
    # According to Gradio forums: return [] or None to clear
    return [], uuid4()
```

```python
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component with render=False to avoid duplicate block error
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400, render=False)

            # Create state for session tracking
            session_state_1 = gr.State(value=uuid4())

            # Use gr.ChatInterface with explicit chatbot component and state
            chat_interface_1 = gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[session_state_1],  # Initial state
                chatbot=chatbot_comp_1,
            )

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # According to Gradio forums: need to clear both chatbot and chatbot_state
            # https://discuss.huggingface.co/t/clear-chat-interface/49866
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, session_state_1]
            )
            # Also clear the ChatInterface's internal state
            clear_btn_1.click(
                lambda: [],
                outputs=[chat_interface_1.chatbot_state]
            )
```

```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def _deduplicate_history(history: list[dict]) -> list[dict]:
    """Deduplicate history by message content and role."""
    seen = set()
    deduplicated = []
    for msg in history:
        if isinstance(msg, dict):
            # Create a unique key from role and content
            content = msg.get("content", "")
            role = msg.get("role", "")
            # For messages with metadata, include metadata title in the key
            # Handle both dict and list metadata formats
            metadata = msg.get("metadata")
            if isinstance(metadata, dict):
                metadata_title = metadata.get("title", "")
            elif isinstance(metadata, list):
                # If metadata is a list, convert to string for hashing
                metadata_title = str(metadata)
            else:
                metadata_title = ""
            # Use string representation for hashable key
            key = (role, str(content), str(metadata_title))
            if key not in seen:
                seen.add(key)
                deduplicated.append(msg)
        else:
            deduplicated.append(msg)
    return deduplicated


def chat_handler_with_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield empty list [])
    2. Deduplicate provided history (ChatInterface may have accumulated duplicates)
    3. Build full history from deduplicated history
    4. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[With Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first by yielding empty list
    yield []

    # Step 2: Deduplicate provided history (ChatInterface may have accumulated duplicates)
    # Then build full history from deduplicated history
    deduplicated_history = _deduplicate_history(history) if history else []
    gradio_history = list(deduplicated_history)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


def chat_handler_without_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that streams full history without clearing first.

    Approach:
    1. Build full history from provided history
    2. Add new messages and stream with full history
    3. Do NOT clear - rely on ChatInterface to manage history

    This tests if ChatInterface can handle full history lists without clearing.
    """
    print(f"[Without Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Two approaches with explicit chatbot

        **Tab 1: With Clearing**
        - Clears chatbot first (yield empty list [])
        - Then streams full history (previous + new chunks)
        - Ensures authoritative history without duplication

        **Tab 2: Without Clearing**
        - Streams full history without clearing
        - Relies on ChatInterface to manage history
        - Tests if ChatInterface handles full history lists correctly

        **Instructions:**
        1. Test each tab separately
        2. Send first message: "Привет"
        3. Wait for response to complete
        4. Send second message: "Как дела?"
        5. Check if first conversation is duplicated
        6. Check if thinking blocks persist during streaming
        7. Click "Clear" to reset history and state
        """
    )

    with gr.Tabs():
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400)

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # Link the clear button to reset history and state
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, gr.State(value=uuid4())]
            )

            # Use gr.ChatInterface with explicit chatbot component and state
            gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[gr.State(value=uuid4())],  # Initial state
                chatbot=chatbot_comp_1,
            )

        # Tab 2: Without Clearing
        with gr.Tab("Without Clearing"):
            # Create explicit chatbot component with render=False
            chatbot_comp_2 = gr.Chatbot(label="Test Chat (Without Clearing)", height=400, render=False)

            # Create state for session tracking
            session_state_2 = gr.State(value=uuid4())

            # Use gr.ChatInterface with explicit chatbot component and state
            chat_interface_2 = gr.ChatInterface(
                chat_handler_without_clearing,
                additional_inputs=[session_state_2],  # Initial state
                chatbot=chatbot_comp_2,
            )

            # Create clear button
            clear_btn_2 = gr.Button("Clear History")
            # Clear both chatbot and state
            clear_btn_2.click(
                clear_history,
                outputs=[chatbot_comp_2, session_state_2]
            )
            # Also clear the ChatInterface's internal state
            clear_btn_2.click(
                lambda: [],
                outputs=[chat_interface_2.chatbot_state]
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Updating the clear button to use `chatbot_state` per the Gradio forum discussion:


```python
"""Test script to isolate Gradio ChatInterface behavior with multi-turn conversations.

This script tests the approach using explicit gr.Chatbot component with gr.State
to prevent duplication while preserving thinking blocks.

Based on: https://www.gradio.app/docs/gradio/state

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Concepts:
- Use explicit gr.Chatbot component
- Use gr.State for session tracking
- Yield full history lists to preserve thinking blocks
- ChatInterface manages the chatbot component explicitly
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return None for chatbot, and a new UUID for state
    return None, uuid4()


def _deduplicate_history(history: list[dict]) -> list[dict]:
    """Deduplicate history by message content and role."""
    seen = set()
    deduplicated = []
    for msg in history:
        if isinstance(msg, dict):
            # Create a unique key from role and content
            content = msg.get("content", "")
            role = msg.get("role", "")
            # For messages with metadata, include metadata title in the key
            # Handle both dict and list metadata formats
            metadata = msg.get("metadata")
            if isinstance(metadata, dict):
                metadata_title = metadata.get("title", "")
            elif isinstance(metadata, list):
                # If metadata is a list, convert to string for hashing
                metadata_title = str(metadata)
            else:
                metadata_title = ""
            # Use string representation for hashable key
            key = (role, str(content), str(metadata_title))
            if key not in seen:
                seen.add(key)
                deduplicated.append(msg)
        else:
            deduplicated.append(msg)
    return deduplicated


def chat_handler_with_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that clears chat first, then streams full history + new chunks.

    Approach:
    1. Clear the chatbot (yield empty list [])
    2. Deduplicate provided history (ChatInterface may have accumulated duplicates)
    3. Build full history from deduplicated history
    4. Add new messages and stream with full history

    This ensures we always have the complete, authoritative history without duplication.
    """
    print(f"[With Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Step 1: Clear the chatbot first by yielding empty list
    yield []

    # Step 2: Deduplicate provided history (ChatInterface may have accumulated duplicates)
    # Then build full history from deduplicated history
    deduplicated_history = _deduplicate_history(history) if history else []
    gradio_history = list(deduplicated_history)

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


def chat_handler_without_clearing(message: str, history: list[dict], session_uuid) -> Generator[list[dict], None, None]:
    """Chat handler that streams full history without clearing first.

    Approach:
    1. Build full history from provided history
    2. Add new messages and stream with full history
    3. Do NOT clear - rely on ChatInterface to manage history

    This tests if ChatInterface can handle full history lists without clearing.
    """
    print(f"[Without Clearing] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Build full history from provided history
    gradio_history = list(history) if history else []

    # Add user message
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    gradio_history.append(search_started)
    # Yield full history (previous + new)
    yield list(gradio_history)

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield list(gradio_history)

    # Add search completed
    search_completed = simulate_search_completed(5)
    gradio_history.append(search_completed)
    # Yield full history with search completed
    yield list(gradio_history)

    # Stream answer - re-yield full history with thinking blocks preserved
    answer = ""
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message (answer) in history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": answer}
        else:
            gradio_history.append({"role": "assistant", "content": answer})
        # Yield full history - thinking blocks are preserved because they're in the list
        yield list(gradio_history)
        time.sleep(0.2)


with gr.Blocks(title="Gradio ChatInterface History Test") as demo:
    gr.Markdown("# Gradio ChatInterface History Behavior Test")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Two approaches with explicit chatbot

        **Tab 1: With Clearing**
        - Clears chatbot first (yield empty list [])
        - Then streams full history (previous + new chunks)
        - Ensures authoritative history without duplication

        **Tab 2: Without Clearing**
        - Streams full history without clearing
        - Relies on ChatInterface to manage history
        - Tests if ChatInterface handles full history lists correctly

        **Instructions:**
        1. Test each tab separately
        2. Send first message: "Привет"
        3. Wait for response to complete
        4. Send second message: "Как дела?"
        5. Check if first conversation is duplicated
        6. Check if thinking blocks persist during streaming
        7. Click "Clear" to reset history and state
        """
    )

    with gr.Tabs():
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component with render=False to avoid duplicate block error
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400, render=False)

            # Create state for session tracking
            session_state_1 = gr.State(value=uuid4())

            # Use gr.ChatInterface with explicit chatbot component and state
            chat_interface_1 = gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[session_state_1],  # Initial state
                chatbot=chatbot_comp_1,
            )

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # According to Gradio forums: need to clear both chatbot and chatbot_state
            # https://discuss.huggingface.co/t/clear-chat-interface/49866
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, session_state_1]
            )
            # Also clear the ChatInterface's internal state
            clear_btn_1.click(
                lambda: [],
                outputs=[chat_interface_1.chatbot_state]
            )

        # Tab 2: Without Clearing
        with gr.Tab("Without Clearing"):
            # Create explicit chatbot component with render=False
            chatbot_comp_2 = gr.Chatbot(label="Test Chat (Without Clearing)", height=400, render=False)

            # Create state for session tracking
            session_state_2 = gr.State(value=uuid4())

            # Use gr.ChatInterface with explicit chatbot component and state
            chat_interface_2 = gr.ChatInterface(
                chat_handler_without_clearing,
                additional_inputs=[session_state_2],  # Initial state
                chatbot=chatbot_comp_2,
            )

            # Create clear button
            clear_btn_2 = gr.Button("Clear History")
            # Clear both chatbot and state
            clear_btn_2.click(
                clear_history,
                outputs=[chatbot_comp_2, session_state_2]
            )
            # Also clear the ChatInterface's internal state
            clear_btn_2.click(
                lambda: [],
                outputs=[chat_interface_2.chatbot_state]
            )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
        # Tab 2: Without Clearing
        with gr.Tab("Without Clearing"):
            # Create explicit chatbot component with render=False
            chatbot_comp_2 = gr.Chatbot(label="Test Chat (Without Clearing)", height=400, render=False)

            # Create state for session tracking
            session_state_2 = gr.State(value=uuid4())

            # Use gr.ChatInterface with explicit chatbot component and state
            chat_interface_2 = gr.ChatInterface(
                chat_handler_without_clearing,
                additional_inputs=[session_state_2],  # Initial state
                chatbot=chatbot_comp_2,
            )

            # Create clear button
            clear_btn_2 = gr.Button("Clear History")
            # Clear both chatbot and state
            clear_btn_2.click(
                clear_history,
                outputs=[chatbot_comp_2, session_state_2]
            )
            # Also clear the ChatInterface's internal state
            clear_btn_2.click(
                lambda: [],
                outputs=[chat_interface_2.chatbot_state]
            )
```


Fixing Tab 1 to use `chatbot_state`:


```python
        # Tab 1: With Clearing
        with gr.Tab("With Clearing"):
            # Create explicit chatbot component with render=False to avoid duplicate block error
            chatbot_comp_1 = gr.Chatbot(label="Test Chat (With Clearing)", height=400, render=False)

            # Create state for session tracking
            session_state_1 = gr.State(value=uuid4())

            # Use gr.ChatInterface with explicit chatbot component and state
            chat_interface_1 = gr.ChatInterface(
                chat_handler_with_clearing,
                additional_inputs=[session_state_1],  # Initial state
                chatbot=chatbot_comp_1,
            )

            # Create clear button
            clear_btn_1 = gr.Button("Clear History")
            # According to Gradio forums: need to clear both chatbot and chatbot_state
            # https://discuss.huggingface.co/t/clear-chat-interface/49866
            clear_btn_1.click(
                clear_history,
                outputs=[chatbot_comp_1, session_state_1]
            )
            # Also clear the ChatInterface's internal state
            clear_btn_1.click(
                lambda: [],
                outputs=[chat_interface_1.chatbot_state]
            )
```


## Fixes Applied

1. Fixed deduplication function: handles metadata that can be a list (unhashable) by converting to string for hashing
2. Updated clear button: uses `chatbot_state` per the [Gradio forum discussion](https://discuss.huggingface.co/t/clear-chat-interface/49866)
3. Added `render=False` to chatbot components: avoids duplicate block errors

## Key Changes

**Deduplication Function:**
- Handles metadata as both dict and list
- Converts all values to strings for hashing
- Prevents "unhashable type: 'list'" errors

**Clear Button (per Gradio forums):**
- Clears the chatbot component: `outputs=[chatbot_comp, session_state]`
- Clears ChatInterface's internal state: `outputs=[chat_interface.chatbot_state]`
- This is required because ChatInterface maintains its own internal state

## What This Should Fix

- Prevents TypeError: handles list metadata correctly
- Proper clearing: clears both chatbot and ChatInterface's internal state
- Prevents duplication: deduplication + proper state clearing

The test server should be running. Test the "With Clearing" tab again; it should work without errors and prevent duplication.

---

**User**

The clear button just purches it all and both tabs duplicate history

How does our working agent handle the chat history ?

@.reference-repos/.cmw-platform-agent  

It is not perfect but it shows tool calls and does not duplicate things

---

**Cursor**


Checking how the working agent handles chat history in the reference repo:



Reading the chat tab implementation:



Checking how the stream handler manages history:



Checking the actual stream_message implementation:



Checking how they handle clearing:



Checking the clear_conversation return value:



The reference agent uses a regular `gr.Chatbot` (not `ChatInterface`) and builds `working_history = history + [new messages]`, then yields the full list. This avoids ChatInterface's internal history management.

Updating our approach to match this pattern:



The reference agent uses a regular `gr.Chatbot` (not `ChatInterface`) and builds `working_history = history + [new messages]`, then yields the full list. This avoids ChatInterface's internal history management.

Updating our code to match this pattern:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Build working history from provided history (like reference agent pattern)
    # Reference agent pattern: working_history = history + [new messages]
    # This ensures we start from ChatInterface's current state and build incrementally
    # Then we always yield the full working_history list
    gradio_history = list(history) if history else []

    # Add user message to history (reference agent pattern: always add, don't check)
    # The reference agent doesn't check for duplicates - it trusts ChatInterface's history
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # According to Gradio docs, yielding a list should replace, not append
            # Create a new list to avoid mutating ChatInterface's internal state
            yield list(gradio_history)

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message for this turn
    # Note: LLM can call tools multiple times, so we may need multiple search_started messages
    # The duplicate check only prevents adding the exact same search_started message (same query)
    # multiple times in quick succession, not legitimate multiple search_started for different tool calls
    # Check only the last few messages to avoid adding exact duplicate within same turn
    recent_messages = gradio_history[-2:] if len(gradio_history) >= 2 else gradio_history
    if not _message_exists_in_history(search_started_msg, recent_messages):
        gradio_history.append(search_started_msg)
        # Yield full history - ChatInterface will replace its internal history
        # This is necessary for streaming robustness, especially with vLLM
        # Full history ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
        # When we yield the full history, ChatInterface replaces its internal state,
        # ensuring all previous messages and thinking blocks remain visible
        # According to Gradio docs, yielding a list should replace, not append
        yield list(gradio_history)

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield list(gradio_history)

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield list(gradio_history)

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield list(gradio_history)

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield list(gradio_history)
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield list(gradio_history)
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield list(gradio_history)
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield list(gradio_history)
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield list(gradio_history)
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield list(gradio_history)

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield list(gradio_history)
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield list(gradio_history)

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield list(gradio_history)

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield list(gradio_history)

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield list(gradio_history)
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield list(gradio_history)
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield list(gradio_history)




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```

```python
    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        gradio_history.append(disclaimer_msg)
        yield list(gradio_history)
```


The reference agent uses a regular `gr.Chatbot` (not `ChatInterface`) and builds `working_history = history + [new messages]`, then yields the full list. Simplifying our code to match this pattern:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Build working history from provided history (like reference agent pattern)
    # Reference agent pattern: working_history = history + [new messages]
    # This ensures we start from ChatInterface's current state and build incrementally
    # Then we always yield the full working_history list
    gradio_history = list(history) if history else []

    # Add user message to history (reference agent pattern: always add, don't check)
    # The reference agent doesn't check for duplicates - it trusts ChatInterface's history
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # According to Gradio docs, yielding a list should replace, not append
            # Create a new list to avoid mutating ChatInterface's internal state
            yield list(gradio_history)

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    # Reference agent pattern: always append, don't check for duplicates
    # Trust that ChatInterface's history parameter is correct
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message (reference agent pattern: always append)
    gradio_history.append(search_started_msg)
    # Yield full history - always yield full working_history like reference agent
    yield list(gradio_history)

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield list(gradio_history)

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield list(gradio_history)

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield list(gradio_history)

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield list(gradio_history)
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield list(gradio_history)
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield list(gradio_history)
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        # Check if we already added a thinking block for this tool in recent messages
                                        recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                        if not _message_exists_in_history(thinking_msg, recent_messages):
                                            gradio_history.append(thinking_msg)
                                            yield list(gradio_history)
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield list(gradio_history)
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield list(gradio_history)

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield list(gradio_history)
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield list(gradio_history)

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield list(gradio_history)

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield list(gradio_history)

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield list(gradio_history)
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield list(gradio_history)
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield list(gradio_history)




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Simplifying to match the reference agent pattern: build incrementally and always yield the full list, without duplicate checks:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Build working history from provided history (like reference agent pattern)
    # Reference agent pattern: working_history = history + [new messages]
    # This ensures we start from ChatInterface's current state and build incrementally
    # Then we always yield the full working_history list
    gradio_history = list(history) if history else []

    # Add user message to history (reference agent pattern: always add, don't check)
    # The reference agent doesn't check for duplicates - it trusts ChatInterface's history
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # According to Gradio docs, yielding a list should replace, not append
            # Create a new list to avoid mutating ChatInterface's internal state
            yield list(gradio_history)

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    # Reference agent pattern: always append, don't check for duplicates
    # Trust that ChatInterface's history parameter is correct
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message (reference agent pattern: always append)
    gradio_history.append(search_started_msg)
    # Yield full history - always yield full working_history like reference agent
    yield list(gradio_history)

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield list(gradio_history)

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield list(gradio_history)

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield list(gradio_history)

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield list(gradio_history)
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                # Check if we already added a thinking block for this tool in recent messages
                                recent_messages = gradio_history[-3:] if len(gradio_history) >= 3 else gradio_history
                                if not _message_exists_in_history(thinking_msg, recent_messages):
                                    gradio_history.append(thinking_msg)
                                    yield list(gradio_history)
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield list(gradio_history)
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        # Reference agent pattern: always append, don't check duplicates
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        gradio_history.append(thinking_msg)
                                        yield list(gradio_history)
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield list(gradio_history)
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield list(gradio_history)

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield list(gradio_history)
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield list(gradio_history)

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield list(gradio_history)

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield list(gradio_history)

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield list(gradio_history)
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield list(gradio_history)
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield list(gradio_history)




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```


Removing the remaining duplicate check to match the reference agent pattern:


```python
"""Gradio UI with ChatInterface and REST API endpoint."""
from __future__ import annotations

import sys
from collections.abc import Generator
from pathlib import Path

# Add project root to path if not already installed
_project_root = Path(__file__).parent.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

import logging

import gradio as gr
from openai import APIError as OpenAIAPIError

from rag_engine.config.settings import get_allowed_fallback_models, settings  # noqa: F401
from rag_engine.llm.fallback import check_context_fallback
from rag_engine.llm.llm_manager import LLMManager
from rag_engine.retrieval.embedder import FRIDAEmbedder
from rag_engine.retrieval.retriever import RAGRetriever
from rag_engine.storage.vector_store import ChromaStore
from rag_engine.tools import retrieve_context
from rag_engine.utils.context_tracker import (
    AgentContext,
    compute_context_tokens,
    estimate_accumulated_context,
    estimate_accumulated_tokens,
)
from rag_engine.utils.conversation_store import salt_session_id
from rag_engine.utils.formatters import format_with_citations
from rag_engine.utils.logging_manager import setup_logging
from rag_engine.utils.vllm_fallback import (
    check_stream_completion,
    execute_fallback_invoke,
    is_vllm_provider,
    should_use_fallback,
)

setup_logging()

logger = logging.getLogger(__name__)


# Initialize singletons (order matters: llm_manager before retriever)
embedder = FRIDAEmbedder(
    model_name=settings.embedding_model,
    device=settings.embedding_device,
)
vector_store = ChromaStore(
    persist_dir=settings.chromadb_persist_dir,
    collection_name=settings.chromadb_collection,
)
llm_manager = LLMManager(
    provider=settings.default_llm_provider,
    model=settings.default_model,
    temperature=settings.llm_temperature,
)
retriever = RAGRetriever(
    embedder=embedder,
    vector_store=vector_store,
    llm_manager=llm_manager,  # NEW: Pass for dynamic context budgeting
    top_k_retrieve=settings.top_k_retrieve,
    top_k_rerank=settings.top_k_rerank,
    rerank_enabled=settings.rerank_enabled,
)


def _find_model_for_tokens(required_tokens: int) -> str | None:
    """Find a model that can handle the required token count.

    Args:
        required_tokens: Minimum token capacity needed

    Returns:
        Model name if found, None otherwise
    """
    from rag_engine.llm.fallback import find_fallback_model

    return find_fallback_model(required_tokens)


def _check_context_fallback(messages: list[dict]) -> str | None:
    """Check if context fallback is needed and return fallback model.

    Thin wrapper around ``rag_engine.llm.fallback.check_context_fallback`` so
    that all pre-agent fallback logic is centralized in the LLM fallback
    module. Kept for backward compatibility and tests.
    """
    # Early return if fallback is disabled
    if not getattr(settings, "llm_fallback_enabled", False):
        return None

    return check_context_fallback(messages)


def compress_tool_results(state: dict, runtime) -> dict | None:
    """Compress tool results before LLM call if approaching context limit.

    This middleware runs right before each LLM invocation, AFTER all tool calls complete.
    It extracts ALL articles from ALL tool messages, deduplicates, and compresses
    proportionally based on normalized_rank (0.0 = best, 1.0 = worst).

    Args:
        state: Agent state containing messages
        runtime: Runtime object with access to model config

    Returns:
        Updated state dict with compressed messages, or None if no changes needed
    """
    from rag_engine.llm.compression import compress_tool_messages

    messages = state.get("messages", [])
    if not messages:
        return None

    updated_messages = compress_tool_messages(
        messages=messages,
        runtime=runtime,
        llm_manager=llm_manager,
        threshold_pct=float(getattr(settings, "llm_compression_threshold_pct", 0.85)),
        target_pct=float(getattr(settings, "llm_compression_target_pct", 0.80)),
    )

    if updated_messages:
        return {"messages": updated_messages}

    return None


def _compute_context_tokens_from_state(messages: list[dict]) -> tuple[int, int]:
    """Compute (conversation_tokens, accumulated_tool_tokens) from agent state messages.

    This is a wrapper for the unified compute_context_tokens function.
    Uses configurable JSON overhead percentage from settings.

    - Conversation tokens: count non-tool message contents
    - Accumulated tool tokens: parse tool JSONs, dedupe by kb_id, sum content tokens,
      add configurable JSON overhead percentage (default 30%)
    """
    return compute_context_tokens(messages, tool_results=None, add_json_overhead=True)


def update_context_budget(state: dict, runtime) -> dict | None:
    """Middleware to populate runtime.context token figures before each model call.

    Ensures tools see accurate conversation and accumulated tool tokens via runtime.context.
    """
    messages = state.get("messages", [])
    if not messages:
        return None

    conv_toks, tool_toks = _compute_context_tokens_from_state(messages)

    # Mutate runtime.context (AgentContext) so tools can read accurate figures
    try:
        if hasattr(runtime, "context") and runtime.context:
            runtime.context.conversation_tokens = conv_toks
            runtime.context.accumulated_tool_tokens = tool_toks
            logger.debug(
                "Updated runtime.context: conversation_tokens=%d, accumulated_tool_tokens=%d",
                conv_toks,
                tool_toks,
            )
    except Exception:
        # Do not fail the run due to budgeting issues
        logger.debug("Unable to update runtime.context tokens")

    return None


from collections.abc import Callable  # noqa: E402

from langchain.agents.middleware import AgentMiddleware  # noqa: E402
from langchain.agents.middleware import wrap_tool_call as middleware_wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest  # noqa: E402
from langchain_core.messages import ToolMessage  # noqa: E402
from langgraph.types import Command  # noqa: E402


class ToolBudgetMiddleware(AgentMiddleware):
    """Populate runtime.context tokens right before each tool execution.

    Ensures tools see up-to-date conversation and accumulated tool tokens
    even when multiple tool calls happen within a single agent step.
    """

    @middleware_wrap_tool_call()
    def tool_budget_wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        try:
            state = getattr(request, "state", {}) or {}
            runtime = getattr(request, "runtime", None)
            if state and runtime is not None and hasattr(runtime, "context") and runtime.context:
                conv_toks, tool_toks = _compute_context_tokens_from_state(state.get("messages", []))
                runtime.context.conversation_tokens = conv_toks
                runtime.context.accumulated_tool_tokens = tool_toks
                logger.debug(
                    "[ToolBudget] runtime.context updated before tool: conv=%d, tools=%d",
                    conv_toks,
                    tool_toks,
                )
        except Exception as exc:  # noqa: BLE001
            logger.warning("[ToolBudget] Failed to update context before tool: %s", exc)

        return handler(request)

def _create_rag_agent(override_model: str | None = None):
    """Create LangChain agent with forced retrieval tool execution and memory compression.

    Uses centralized factory with app-specific middleware. The factory enforces
    tool execution via tool_choice="retrieve_context" to ensure the agent always
    searches the knowledge base before answering.

    This wrapper preserves test patch points while delegating to the centralized
    agent_factory for consistent agent creation.

    Args:
        override_model: Optional model name to use instead of default
                       (for context window fallback)

    Returns:
        Configured LangChain agent with retrieve_context tool and middleware
    """
    from rag_engine.llm.agent_factory import create_rag_agent

    return create_rag_agent(
        override_model=override_model,
        tool_budget_middleware=ToolBudgetMiddleware(),
        update_context_budget_middleware=update_context_budget,
        compress_tool_results_middleware=compress_tool_results,
    )


def _is_ui_only_message(msg: dict) -> bool:
    """Check if a message is UI-only (should not be sent to agent).

    UI-only messages include:
    - Disclaimer messages
    - Search started/completed metadata messages
    - Model switch notices
    - Any message with metadata field (UI metadata)

    Args:
        msg: Message dict to check

    Returns:
        True if message is UI-only, False otherwise
    """
    if not isinstance(msg, dict):
        return False

    # Check for metadata field (UI-only messages have this)
    if "metadata" in msg:
        return True

    # Check for disclaimer content
    from rag_engine.llm.prompts import AI_DISCLAIMER
    content = msg.get("content", "")
    if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
        return True

    return False


def _messages_are_equivalent(msg1: dict, msg2: dict) -> bool:
    """Check if two messages are equivalent (same role and content).

    Used to detect duplicate messages in history.

    Args:
        msg1: First message dict
        msg2: Second message dict

    Returns:
        True if messages are equivalent, False otherwise
    """
    if not isinstance(msg1, dict) or not isinstance(msg2, dict):
        return False

    role1 = msg1.get("role")
    role2 = msg2.get("role")
    if role1 != role2:
        return False

    content1 = msg1.get("content", "")
    content2 = msg2.get("content", "")
    
    # Compare string content
    if isinstance(content1, str) and isinstance(content2, str):
        return content1.strip() == content2.strip()
    
    # For structured content, compare as strings
    return str(content1) == str(content2)


def _message_exists_in_history(message: dict, history: list[dict]) -> bool:
    """Check if a message already exists in history.

    Args:
        message: Message dict to check
        history: List of message dicts

    Returns:
        True if message exists in history, False otherwise
    """
    for existing_msg in history:
        if _messages_are_equivalent(message, existing_msg):
            return True
    return False


def _build_agent_messages_from_gradio_history(
    gradio_history: list[dict],
    current_message: str,
    wrapped_message: str,
) -> list[dict]:
    """Build messages for agent from Gradio history, filtering out UI-only messages.

    Filters gradio_history to exclude UI metadata messages (disclaimer, search_started, etc.)
    and builds a clean message list for the agent. This ensures the agent only sees
    actual conversation content, not UI elements.

    Args:
        gradio_history: Full Gradio history including UI messages
        current_message: Current user message (unwrapped, for filtering)
        wrapped_message: Current user message wrapped with template (for agent)

    Returns:
        List of message dicts in LangChain format for agent
    """
    messages = []

    # Filter gradio_history to exclude UI-only messages
    # We need to include previous conversation messages but exclude:
    # - Disclaimer messages
    # - Search started/completed metadata
    # - Model switch notices
    # - The current user message (we'll add it wrapped below)
    from rag_engine.utils.message_utils import normalize_gradio_history_message

    for msg in gradio_history:
        # Skip UI-only messages
        if _is_ui_only_message(msg):
            continue

        # Skip the current user message (we'll add wrapped version below)
        msg_role = msg.get("role")
        msg_content = msg.get("content", "")
        if msg_role == "user" and isinstance(msg_content, str) and msg_content.strip() == current_message.strip():
            continue

        # Normalize message for LangChain (convert structured content to string)
        normalized_msg = normalize_gradio_history_message(msg)
        # Include actual conversation messages
        messages.append(normalized_msg)

    # Add wrapped current message for agent
    messages.append({"role": "user", "content": wrapped_message})

    return messages


def _process_text_chunk_for_streaming(
    text_chunk: str,
    answer: str,
    disclaimer_prepended: bool,
    has_seen_tool_results: bool,
) -> tuple[str, bool]:
    """Process a text chunk for streaming with disclaimer and formatting.

    Handles optional newline after tool results and accumulates the answer.

    Args:
        text_chunk: Raw text chunk from agent
        answer: Accumulated answer so far
        disclaimer_prepended: Whether disclaimer has already been added
        has_seen_tool_results: Whether tool results have been seen

    Returns:
        Tuple of (updated_answer, updated_disclaimer_prepended)
    """
    # Mark that we started streaming answer text
    if not disclaimer_prepended:
        disclaimer_prepended = True
    # Prepend newline before first text chunk after tool results
    elif has_seen_tool_results and not answer:
        text_chunk = "\n" + text_chunk

    answer = answer + text_chunk
    return answer, disclaimer_prepended


def agent_chat_handler(
    message: str,
    history: list[dict],
    request: gr.Request | None = None,
) -> Generator[str | dict | list[dict], None, None]:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question
        history: Chat history from Gradio
        request: Gradio request object for session management

    Yields:
        Complete message history lists (for ChatInterface) to preserve all messages
        including disclaimer, thinking blocks, and streaming answer.
    """
    if not message or not message.strip():
        yield "Пожалуйста, введите вопрос / Please enter a question."
        return

    # Build full message history starting from provided history
    # This ensures all messages (disclaimer, thinking blocks, answer) persist during streaming
    # Note: We keep original format for ChatInterface, normalize only when needed for agent

    # Initialize variables that might be needed in error handling
    current_model = settings.default_model
    messages = []
    tool_results = []

    # Build working history from provided history (like reference agent pattern)
    # Reference agent pattern: working_history = history + [new messages]
    # This ensures we start from ChatInterface's current state and build incrementally
    # Then we always yield the full working_history list
    gradio_history = list(history) if history else []

    # Add user message to history (reference agent pattern: always add, don't check)
    # The reference agent doesn't check for duplicates - it trusts ChatInterface's history
    user_msg = {"role": "user", "content": message}
    gradio_history.append(user_msg)

    # Stream AI-generated content disclaimer as a persistent assistant message
    # so it stays above tool-call progress/thinking chunks in the Chatbot UI.
    # Only add it once per conversation (check if it already exists in history)
    from rag_engine.llm.prompts import AI_DISCLAIMER

    # Check if disclaimer already exists in history
    disclaimer_exists = False
    for msg in gradio_history:
        if isinstance(msg, dict):
            content = msg.get("content", "")
            if isinstance(content, str) and AI_DISCLAIMER.strip() in content:
                disclaimer_exists = True
                break

    # Add disclaimer to history only if it doesn't exist yet
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        # Double-check it doesn't exist (robust duplicate prevention)
        if not _message_exists_in_history(disclaimer_msg, gradio_history):
            gradio_history.append(disclaimer_msg)
            # Yield full history - ChatInterface will replace its internal history
            # According to Gradio docs, yielding a list should replace, not append
            # Create a new list to avoid mutating ChatInterface's internal state
            yield list(gradio_history)

    # Show "search started" immediately with user's message (before LLM tool call)
    # This provides instant feedback. We'll update it with LLM-generated query when available.
    # Reference agent pattern: always append, don't check for duplicates
    # Trust that ChatInterface's history parameter is correct
    from rag_engine.api.stream_helpers import yield_search_started

    # Use user's message as initial query (will be updated when tool call is detected)
    initial_query = message.strip() if message else ""
    search_started_msg = yield_search_started(initial_query)
    # Add search_started message (reference agent pattern: always append)
    gradio_history.append(search_started_msg)
    # Yield full history - always yield full working_history like reference agent
    yield list(gradio_history)

    # Session management (reuse existing pattern)
    base_session_id = getattr(request, "session_hash", None) if request is not None else None
    session_id = salt_session_id(base_session_id, history, message)

    # Wrap user message in template only for the first question in the conversation
    from rag_engine.llm.prompts import (
        USER_QUESTION_TEMPLATE_FIRST,
        USER_QUESTION_TEMPLATE_SUBSEQUENT,
    )

    # Apply template only if this is the first message (empty history)
    is_first_message = not history
    wrapped_message = (
        USER_QUESTION_TEMPLATE_FIRST.format(question=message)
        if is_first_message
        else USER_QUESTION_TEMPLATE_SUBSEQUENT.format(question=message)
    )

    # Save user message to conversation store (BEFORE agent execution)
    # This ensures conversation history is tracked for memory compression
    if session_id:
        llm_manager._conversations.append(session_id, "user", message)

    # Build messages from gradio_history for agent (LangChain format)
    # This filters out UI-only messages (disclaimer, search_started, etc.)
    # and ensures the agent only sees actual conversation content
    messages = _build_agent_messages_from_gradio_history(
        gradio_history, message, wrapped_message
    )

    # Note: pre-agent trimming removed by request; rely on existing middleware

    # Check if we need model fallback BEFORE creating agent
    # This matches old handler's upfront fallback check
    selected_model = None
    if settings.llm_fallback_enabled:
        selected_model = _check_context_fallback(messages)

    # Create agent (with fallback model if needed) and stream execution
    agent = _create_rag_agent(override_model=selected_model)
    tool_results = []
    answer = ""
    current_model = selected_model or settings.default_model
    has_seen_tool_results = False
    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

    # Track accumulated context for progressive budgeting
    # Agent is responsible for counting context, not the tool
    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])

    # Initialize tool call accumulator for streaming chunks
    from rag_engine.api.stream_helpers import ToolCallAccumulator

    tool_call_accumulator = ToolCallAccumulator()

    try:
        # Track tool execution state
        # Only stream text content when NOT executing tools
        tool_executing = False

        # Pass accumulated context to agent via typed context parameter
        # Tools can access this via runtime.context (typed, clean!)
        agent_context = AgentContext(
            conversation_tokens=conversation_tokens,
            accumulated_tool_tokens=0,  # Updated as we go
        )

        # vLLM streaming limitation: tool_choice doesn't work in streaming mode
        # vLLM ignores tool_choice="retrieve_context" in streaming and returns finish_reason=stop
        # instead of finish_reason=tool_calls. This means forced tool execution requires invoke() mode.
        # The fallback ensures tool calls are executed even when streaming detection fails.
        # Can be disabled via vllm_streaming_fallback_enabled=False for testing (will fail to execute tools)
        is_vllm = is_vllm_provider()
        fallback_to_invoke = False

        # Use multiple stream modes for complete streaming experience
        # Per https://docs.langchain.com/oss/python/langchain/streaming#stream-multiple-modes
        # Always try streaming first - improved detection should catch tool calls via content_blocks/finish_reason
        logger.info("Starting agent.stream() with %d messages", len(messages))
        stream_chunk_count = 0
        tool_calls_detected_in_stream = False

        try:
            for stream_mode, chunk in agent.stream(
                {"messages": messages},
                context=agent_context,
                stream_mode=["updates", "messages"]
            ):
                stream_chunk_count += 1
                logger.debug("Stream chunk #%d: mode=%s", stream_chunk_count, stream_mode)

                # Handle "messages" mode for token streaming
                if stream_mode == "messages":
                    token, metadata = chunk
                    token_type = getattr(token, "type", "unknown")
                    # Disabled verbose token logging for production
                    # logger.info("Messages token #%d: type=%s", stream_chunk_count, token_type)

                    # Debug logging for vLLM tool calling issues (only when expecting tool calls)
                    # Check for both "ai" type and AIMessageChunk class (LangChain uses different representations)
                    token_class = type(token).__name__
                    is_ai_message = token_type == "ai" or "AIMessage" in token_class or "AIMessage" in str(token_type)

                    # Process all AI tokens through accumulator to accumulate tool_call chunks
                    # This ensures we capture query even if chunks arrive before tool_call is detected
                    if is_ai_message:
                        tool_query_from_accumulator = tool_call_accumulator.process_token(token)
                        # If accumulator found a complete query, update search_started message
                        if tool_query_from_accumulator:
                            from rag_engine.api.stream_helpers import update_search_started_in_history

                            if update_search_started_in_history(gradio_history, tool_query_from_accumulator):
                                yield list(gradio_history)

                    if is_ai_message:
                        has_tool_calls = bool(getattr(token, "tool_calls", None))
                        content = str(getattr(token, "content", ""))
                        response_metadata = getattr(token, "response_metadata", {})
                        finish_reason = response_metadata.get("finish_reason", "N/A") if isinstance(response_metadata, dict) else "N/A"

                        # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                        content_blocks = getattr(token, "content_blocks", None)
                        has_content_blocks = bool(content_blocks)
                        tool_call_chunks_in_blocks = 0
                        if content_blocks:
                            tool_call_chunks_in_blocks = sum(1 for block in content_blocks if block.get("type") == "tool_call_chunk")
                            # Log first few content_blocks in detail for debugging (only when tool calls detected)
                            if tool_call_chunks_in_blocks > 0 or finish_reason == "tool_calls":
                                logger.debug("Content blocks detail: %s", content_blocks[:3] if len(content_blocks) > 3 else content_blocks)

                        # Enhanced logging only when we expect tool calls but haven't detected them yet
                        expected_tool_calls = is_vllm and not has_seen_tool_results
                        if expected_tool_calls and not tool_calls_detected_in_stream:
                            # Only log when we're actively debugging missing tool calls
                            logger.debug(
                                "AI token #%d: has_tool_calls=%s, content_length=%d, finish_reason=%s, "
                                "has_content_blocks=%s, tool_call_chunks=%d",
                                stream_chunk_count,
                                has_tool_calls,
                                len(content),
                                finish_reason,
                                has_content_blocks,
                                tool_call_chunks_in_blocks,
                            )
                        # Disabled verbose AI token logging for production
                        # else:
                        #     logger.debug(
                        #         "AI token: has_tool_calls=%s, content_length=%d, finish_reason=%s",
                        #         has_tool_calls,
                        #         len(content),
                        #         finish_reason,
                        #     )

                    # Filter out tool-related messages (DO NOT display in chat)
                    # 1. Tool results (type="tool") - processed internally for citations
                    if hasattr(token, "type") and token.type == "tool":
                        tool_results.append(token.content)
                        logger.debug("Tool result received, %d total results", len(tool_results))
                        tool_executing = False
                        has_seen_tool_results = True

                        # Update accumulated context for next tool call
                        # Agent tracks context, not the tool!
                        _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                        agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                        logger.debug(
                            "Updated accumulated context: conversation=%d, tools=%d (total: %d)",
                            conversation_tokens,
                            accumulated_tool_tokens,
                            conversation_tokens + accumulated_tool_tokens,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.api.stream_helpers import (
                            extract_article_count_from_tool_result,
                            yield_search_completed,
                        )

                        # Parse result to get articles and emit completion metadata with sources
                        from rag_engine.tools.utils import parse_tool_result_to_articles

                        articles_list = parse_tool_result_to_articles(token.content)
                        articles_count = len(articles_list) if articles_list else extract_article_count_from_tool_result(token.content)

                        # Format articles for display (title and URL)
                        articles_for_display = []
                        if articles_list:
                            for article in articles_list:
                                article_meta = article.metadata if hasattr(article, "metadata") else {}
                                title = article_meta.get("title", "Untitled")
                                url = article_meta.get("url", "")
                                articles_for_display.append({"title": title, "url": url})

                        search_completed_msg = yield_search_completed(
                            count=articles_count,
                            articles=articles_for_display if articles_for_display else None,
                        )
                        # Add search completed to history and yield full history
                        gradio_history.append(search_completed_msg)
                        yield list(gradio_history)

                        # CRITICAL: Check if accumulated tool results exceed safe threshold
                        # This prevents overflow when agent makes multiple tool calls
                        if settings.llm_fallback_enabled and not selected_model:
                            from rag_engine.config.settings import get_allowed_fallback_models
                            from rag_engine.llm.fallback import select_mid_turn_fallback_model

                            fallback_model = select_mid_turn_fallback_model(
                                current_model,
                                messages,
                                tool_results,
                                get_allowed_fallback_models(),
                            )

                            if fallback_model:
                                from rag_engine.api.stream_helpers import yield_model_switch_notice

                                model_switch_msg = yield_model_switch_notice(fallback_model)
                                # Add model switch notice to history and yield full history
                                gradio_history.append(model_switch_msg)
                                yield list(gradio_history)

                                # Recreate agent with larger model
                                # Note: This is expensive but prevents catastrophic overflow
                                agent = _create_rag_agent(override_model=fallback_model)
                                current_model = fallback_model

                                # Note: Can't restart stream here - agent will continue with new model
                                # for subsequent calls

                        # Skip further processing of tool messages
                        continue

                    # 2. AI messages with tool_calls (when agent decides to call tools)
                    # Check multiple indicators: token.tool_calls, content_blocks, and finish_reason
                    # These should NEVER be displayed - only show metadata
                    has_tool_calls_attr = hasattr(token, "tool_calls") and bool(token.tool_calls)

                    # Check content_blocks for tool_call_chunk (critical for vLLM streaming)
                    token_content_blocks = getattr(token, "content_blocks", None)
                    has_tool_call_chunks = bool(token_content_blocks) and any(
                        block.get("type") == "tool_call_chunk" for block in token_content_blocks
                    )

                    # Check finish_reason (indicates tool calls completed)
                    token_response_metadata = getattr(token, "response_metadata", {})
                    token_finish_reason = token_response_metadata.get("finish_reason") if isinstance(token_response_metadata, dict) else None
                    finish_reason_is_tool_calls = token_finish_reason == "tool_calls"

                    # Tool call detected if any of these conditions are true
                    tool_call_detected = has_tool_calls_attr or has_tool_call_chunks or finish_reason_is_tool_calls

                    if tool_call_detected:
                        tool_calls_detected_in_stream = True
                        if not tool_executing:
                            tool_executing = True
                            # Log which method detected the tool call
                            if has_tool_calls_attr:
                                call_count = len(token.tool_calls) if isinstance(token.tool_calls, list) else "?"
                                logger.info("Agent calling tool(s) via token.tool_calls: %s call(s)", call_count)
                            elif has_tool_call_chunks:
                                logger.info("Agent calling tool(s) via content_blocks tool_call_chunk")
                            elif finish_reason_is_tool_calls:
                                logger.info("Agent calling tool(s) detected via finish_reason=tool_calls")
                                # Check if tool_calls are now available in the token
                                final_tool_calls = getattr(token, "tool_calls", None)
                                if final_tool_calls:
                                    logger.info("Final tool_calls after finish_reason: %s call(s)",
                                               len(final_tool_calls) if isinstance(final_tool_calls, list) else "?")

                            # Get tool name to determine which thinking block to show
                            tool_name = tool_call_accumulator.get_tool_name(token)
                            
                            if tool_name == "retrieve_context":
                                # Update existing "search started" message with LLM-generated query
                                from rag_engine.api.stream_helpers import update_search_started_in_history
                                tool_query = tool_call_accumulator.process_token(token)
                                # If we got a query from tool call, update the existing search_started message
                                if tool_query:
                                    if update_search_started_in_history(gradio_history, tool_query):
                                        yield list(gradio_history)
                                # If no query extracted yet, keep the existing message with user's query
                            elif tool_name:
                                # Show generic thinking block for non-search tools
                                # Reference agent pattern: always append, don't check duplicates
                                from rag_engine.api.stream_helpers import yield_thinking_block
                                thinking_msg = yield_thinking_block(tool_name)
                                gradio_history.append(thinking_msg)
                                yield list(gradio_history)
                        # Skip displaying the tool call itself and any content
                        continue

                    # 3. Only stream text content from messages WITHOUT tool_calls
                    # This ensures we only show the final answer, not tool reasoning
                    if hasattr(token, "tool_calls") and token.tool_calls:
                        # Skip any message that has tool_calls (redundant check for safety)
                        continue

                    # Process content blocks for final answer text streaming
                    text_chunk_found = False
                    if hasattr(token, "content_blocks") and token.content_blocks:
                        for block in token.content_blocks:
                            if block.get("type") == "tool_call_chunk":
                                # Tool call chunk detected - emit metadata if not already done
                                if not tool_executing:
                                    tool_executing = True
                                    logger.debug("Agent calling tool via chunk")

                                    # Get tool name to determine which thinking block to show
                                    tool_name = tool_call_accumulator.get_tool_name(token)
                                    
                                    if tool_name == "retrieve_context":
                                        # Update existing "search started" message with LLM-generated query
                                        from rag_engine.api.stream_helpers import update_search_started_in_history
                                        tool_query = tool_call_accumulator.process_token(token)
                                        if tool_query:
                                            if update_search_started_in_history(gradio_history, tool_query):
                                                yield list(gradio_history)
                                        # If no query extracted yet, keep the existing message with user's query
                                    elif tool_name:
                                        # Show generic thinking block for non-search tools
                                        # Reference agent pattern: always append, don't check duplicates
                                        from rag_engine.api.stream_helpers import yield_thinking_block
                                        thinking_msg = yield_thinking_block(tool_name)
                                        gradio_history.append(thinking_msg)
                                        yield list(gradio_history)
                                # Never stream tool call chunks as text
                                continue

                            elif block.get("type") == "text" and block.get("text"):
                                # Only stream text if we're not currently executing tools
                                # This prevents streaming the agent's "reasoning" about tool calls
                                if not tool_executing:
                                    text_chunk = block["text"]
                                    answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                        text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                    )
                                    # Update last message (answer) in history and yield full history
                                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                        # Update existing answer message
                                        gradio_history[-1] = {"role": "assistant", "content": answer}
                                    else:
                                        # Create new answer message
                                        gradio_history.append({"role": "assistant", "content": answer})
                                    yield list(gradio_history)
                                    text_chunk_found = True

                    # Fallback: If no text found in content_blocks, check token.content directly
                    # This handles vLLM and other providers that provide text directly in token.content
                    # LangChain streaming provides incremental chunks (tested and confirmed)
                    if not text_chunk_found and is_ai_message and not tool_executing:
                        token_content = str(getattr(token, "content", ""))
                        if token_content:
                            # LangChain streaming provides incremental chunks, but handle both cases for robustness
                            new_chunk = None
                            if answer and token_content.startswith(answer):
                                # Cumulative content - extract only the new part
                                new_chunk = token_content[len(answer):]
                            elif token_content != answer:
                                # Incremental chunk - use as-is (typical case)
                                new_chunk = token_content
                            
                            if new_chunk:
                                answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                    new_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                )
                                # Update last message (answer) in history and yield full history
                                if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                    # Update existing answer message
                                    gradio_history[-1] = {"role": "assistant", "content": answer}
                                else:
                                    # Create new answer message
                                    gradio_history.append({"role": "assistant", "content": answer})
                                yield list(gradio_history)

                # Handle "updates" mode for agent state updates
                elif stream_mode == "updates":
                    # We can log updates but don't need to yield them
                    logger.debug("Agent update: %s", list(chunk.keys()) if isinstance(chunk, dict) else chunk)

            # After stream completes, check if we expected tool calls but didn't get results
            # Only check for vLLM on first message (when we expect tool calls)
            should_fallback, fallback_enabled = check_stream_completion(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=tool_calls_detected_in_stream,
                tool_results_count=len(tool_results),
                stream_chunk_count=stream_chunk_count,
            )

        except OpenAIAPIError as api_error:
            # Handle OpenAI API errors (e.g., malformed streaming response)
            error_msg = str(api_error).lower()
            is_streaming_error = "list index out of range" in error_msg or "streaming" in error_msg

            if is_streaming_error:
                # If streaming fails, try falling back to invoke() mode
                # This can happen with any OpenAI-compatible provider, not just vLLM
                # Only fallback if we haven't seen tool results yet (first turn)
                if not has_seen_tool_results:
                    logger.warning(
                        "OpenAI API streaming error, falling back to invoke() mode: %s", api_error
                    )
                    fallback_to_invoke = True
                else:
                    # If we've already seen tool results, streaming error might be in the answer phase
                    # Re-raise to be handled by outer exception handler
                    logger.error("OpenAI API error during streaming (after tool execution): %s", api_error, exc_info=True)
                    raise
            else:
                # For other API errors, re-raise to be handled by outer exception handler
                logger.error("OpenAI API error: %s", api_error, exc_info=True)
                raise
        except Exception as stream_error:
            # If streaming fails and we expected tool calls, fall back to invoke()
            # Only if fallback is enabled
            if should_use_fallback(
                is_vllm=is_vllm,
                has_seen_tool_results=has_seen_tool_results,
                tool_calls_detected=False,
                tool_results_count=len(tool_results),
            ):
                logger.warning(
                    "Streaming failed for vLLM, falling back to invoke() mode: %s", stream_error
                )
                fallback_to_invoke = True
            else:
                raise

        # Fallback to invoke() only if we expected tool calls but didn't get them
        # This ensures tool execution happens even if streaming detection fails
        # Can be disabled via vllm_streaming_fallback_enabled setting for testing
        if fallback_to_invoke or should_fallback:
            # Execute fallback and process results
            fallback_results = {}
            for chunk in execute_fallback_invoke(
                agent=agent,
                messages=messages,
                agent_context=agent_context,
                has_seen_tool_results=has_seen_tool_results,
                result_container=fallback_results,
            ):
                # Handle metadata (dict) or text chunks (str) and add to history
                if isinstance(chunk, dict):
                    # Metadata message (search_started, search_completed, etc.)
                    gradio_history.append(chunk)
                    yield list(gradio_history)
                elif isinstance(chunk, str):
                    # Text chunk - update last message or create new one
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        # Update existing answer message
                        gradio_history[-1] = {"role": "assistant", "content": chunk}
                    else:
                        # Create new answer message
                        gradio_history.append({"role": "assistant", "content": chunk})
                    yield list(gradio_history)

            # Extract results from container
            if fallback_results:
                fallback_tool_results = fallback_results.get("tool_results", [])
                fallback_answer = fallback_results.get("final_answer", "")
                fallback_disclaimer = fallback_results.get("disclaimer_prepended", False)

                # Update tool results
                if fallback_tool_results:
                    tool_results.extend(fallback_tool_results)
                    has_seen_tool_results = True
                    # Update accumulated context
                    _, accumulated_tool_tokens = estimate_accumulated_tokens([], tool_results)
                    agent_context.accumulated_tool_tokens = accumulated_tool_tokens

                # Update answer if we got one from fallback
                if fallback_answer:
                    answer = fallback_answer
                    disclaimer_prepended = fallback_disclaimer or disclaimer_prepended

        # Accumulate articles from tool results and add citations
        logger.info("Stream completed: %d chunks processed, %d tool results", stream_chunk_count, len(tool_results))
        from rag_engine.tools import accumulate_articles_from_tool_results
        articles = accumulate_articles_from_tool_results(tool_results)

        # Handle no results case
        if not articles:
            final_text = answer
            logger.info("Agent completed with no retrieved articles")
        else:
            final_text = format_with_citations(answer, articles)
            logger.info("Agent completed with %d articles", len(articles))

        # Update last message (answer) with final formatted text and yield full history
        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            # Update existing answer message
            gradio_history[-1] = {"role": "assistant", "content": final_text}
        else:
            # Create new answer message
            gradio_history.append({"role": "assistant", "content": final_text})

        # Save conversation turn (reuse existing pattern)
        if session_id:
            llm_manager.save_assistant_turn(session_id, final_text)

        yield list(gradio_history)

    except Exception as e:
        logger.error("Error in agent_chat_handler: %s", e, exc_info=True)
        # Variables are initialized at function start, so they should always exist
        # Handle context-length overflow gracefully by switching to a larger model (once)
        err_text = str(e).lower()
        is_context_overflow = (
            "maximum context length" in err_text
            or "context length" in err_text
            or "token limit" in err_text
            or "too many tokens" in err_text
        )

        if settings.llm_fallback_enabled and is_context_overflow and messages:
            try:  # Single-shot fallback retry
                # Estimate required tokens and pick a capable fallback model
                # Get current model context window for adaptive overhead calculation
                from rag_engine.llm.model_configs import MODEL_CONFIGS

                current_model_config = MODEL_CONFIGS.get(current_model)
                if not current_model_config:
                    for key in MODEL_CONFIGS:
                        if key != "default" and key in current_model:
                            current_model_config = MODEL_CONFIGS[key]
                            break
                if not current_model_config:
                    current_model_config = MODEL_CONFIGS["default"]
                current_window = int(current_model_config.get("token_limit", 0))

                required_tokens = estimate_accumulated_context(
                    messages,
                    tool_results,
                    context_window=current_window,
                )
                fallback_model = _find_model_for_tokens(required_tokens) or None

                if fallback_model and fallback_model != current_model:
                    logger.warning(
                        "Retrying with fallback model due to context overflow: %s -> %s (required≈%d)",
                        current_model,
                        fallback_model,
                        required_tokens,
                    )

                    # Inform UI about the switch
                    from rag_engine.api.stream_helpers import yield_model_switch_notice

                    model_switch_msg = yield_model_switch_notice(fallback_model)
                    # Add model switch notice to history and yield full history
                    gradio_history.append(model_switch_msg)
                    yield list(gradio_history)

                    # Recreate agent and re-run the stream once
                    agent = _create_rag_agent(override_model=fallback_model)
                    current_model = fallback_model

                    conversation_tokens, _ = estimate_accumulated_tokens(messages, [])
                    agent_context = AgentContext(
                        conversation_tokens=conversation_tokens,
                        accumulated_tool_tokens=0,
                    )

                    answer = ""
                    tool_results = []
                    has_seen_tool_results = False
                    disclaimer_prepended = False  # Track if disclaimer has been prepended to stream

                    for stream_mode, chunk in agent.stream(
                        {"messages": messages},
                        context=agent_context,
                        stream_mode=["updates", "messages"],
                    ):
                        if stream_mode == "messages":
                            token, metadata = chunk
                            # Collect tool results silently; only stream final text
                            if hasattr(token, "type") and token.type == "tool":
                                tool_results.append(token.content)
                                has_seen_tool_results = True
                                # keep agent_context.accumulated_tool_tokens updated
                                _, acc_tool_toks = estimate_accumulated_tokens([], tool_results)
                                agent_context.accumulated_tool_tokens = acc_tool_toks
                                continue

                            if hasattr(token, "tool_calls") and token.tool_calls:
                                # Do not stream tool call reasoning
                                continue

                            if hasattr(token, "content_blocks") and token.content_blocks:
                                for block in token.content_blocks:
                                    if block.get("type") == "text" and block.get("text"):
                                        text_chunk = block["text"]
                                        answer, disclaimer_prepended = _process_text_chunk_for_streaming(
                                            text_chunk, answer, disclaimer_prepended, has_seen_tool_results
                                        )
                                        # Update last message (answer) in history and yield full history
                                        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                                            gradio_history[-1] = {"role": "assistant", "content": answer}
                                        else:
                                            gradio_history.append({"role": "assistant", "content": answer})
                                        yield list(gradio_history)
                        elif stream_mode == "updates":
                            # No-op for UI
                            pass

                    from rag_engine.tools import accumulate_articles_from_tool_results

                    articles = accumulate_articles_from_tool_results(tool_results)
                    if not articles:
                        final_text = answer
                    else:
                        final_text = format_with_citations(answer, articles)

                    # Update last message with final formatted text
                    if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
                        gradio_history[-1] = {"role": "assistant", "content": final_text}
                    else:
                        gradio_history.append({"role": "assistant", "content": final_text})

                    if session_id:
                        llm_manager.save_assistant_turn(session_id, final_text)
                    yield list(gradio_history)
                    return
            except Exception as retry_exc:  # If fallback retry fails, emit original-style error
                logger.error("Fallback retry failed: %s", retry_exc, exc_info=True)

        # Default error path - add error message to history
        error_msg = f"Извините, произошла ошибка / Sorry, an error occurred: {str(e)}"
        # gradio_history should always exist (initialized early), but ensure it's not empty
        if not gradio_history:
            # Add user message if we have it
            if message:
                gradio_history.append({"role": "user", "content": message})

        if gradio_history and gradio_history[-1].get("role") == "assistant" and not gradio_history[-1].get("metadata"):
            gradio_history[-1] = {"role": "assistant", "content": error_msg}
        else:
            gradio_history.append({"role": "assistant", "content": error_msg})
        yield list(gradio_history)




def query_rag(question: str, provider: str = "gemini", top_k: int = 5) -> str:
    if not question or not question.strip():
        return "Error: Empty question"
    docs = retriever.retrieve(question, top_k=top_k)
    # If no documents found, inject a message into the context
    has_no_results_doc = False
    if not docs:
        from rag_engine.retrieval.retriever import Article
        no_results_msg = "К сожалению, не найдено релевантных материалов / No relevant results found."
        no_results_doc = Article(
            kb_id="",
            content=no_results_msg,
            metadata={"title": "No Results", "kbId": "", "_is_no_results": True}
        )
        docs = [no_results_doc]
        has_no_results_doc = True

    answer = llm_manager.generate(question, docs, provider=provider)
    # If we injected the "no results" message, don't add citations
    if has_no_results_doc:
        return answer  # Don't add citations for "no results" message
    return format_with_citations(answer, docs)


# Configure chatbot height and UI elements based on embedded widget setting
if settings.gradio_embedded_widget:
    # For embedded widget
    chatbot_height = "400px"
    chatbot_max_height = "65vh"
    chat_title = None
    chat_description = None
else:
    # For standalone app
    chatbot_height = "85vh"
    chatbot_max_height = "80vh"
    chat_title = "Ассистент базы знаний Comindware Platform"
    chat_description = None  # "RAG-агент базы знаний Comindware Platform"

chatbot_config = gr.Chatbot(
    min_height="30vh",
    height=chatbot_height,
    max_height=chatbot_max_height,
    resizable=True,
    elem_classes=["gradio-chatbot"],
    label="Диалог с агентом",
    buttons=["copy", "copy_all"],
)

# Force agent-based handler; legacy direct handler removed
handler_fn = agent_chat_handler
logger.info("Using agent-based (LangChain) handler for chat interface")

# Wrapper function to expose retrieve_context tool as API endpoint
# The tool is a StructuredTool, so we need to extract the underlying function
def get_knowledge_base_articles(query: str, top_k: int | str | None = None) -> str:
    """Search and retrieve documentation articles from the Comindware Platform knowledge base.

    Use this tool when you need raw search results with article metadata. For intelligent
    answers with automatic context retrieval, use agent_chat_handler instead.

    Args:
        query: Search query or question to find relevant documentation articles.
               Examples: "authentication", "API integration", "user management"
        top_k: Optional limit on number of articles to return. If not specified,
               returns the default number of most relevant articles (typically 10-20).
               Can be provided as int or string (will be converted).

    Returns:
        JSON string containing an array of articles, each with:
        - kb_id: Article identifier
        - title: Article title
        - url: Link to the article
        - content: Full article content (markdown format)
        - metadata: Additional metadata including rerank scores and source information
    """
    # Convert top_k from string to int if needed (Gradio/MCP may pass strings)
    converted_top_k: int | None = None
    if top_k is not None:
        if isinstance(top_k, str):
            try:
                converted_top_k = int(top_k)
            except (ValueError, TypeError) as e:
                raise ValueError(f"top_k must be a valid integer, got: {top_k!r}") from e
        elif isinstance(top_k, int):
            converted_top_k = top_k
        else:
            raise ValueError(f"top_k must be an integer or None, got: {type(top_k).__name__}")

        # Validate positive integer
        if converted_top_k <= 0:
            raise ValueError(f"top_k must be a positive integer, got: {converted_top_k}")

    # Access the underlying function from the StructuredTool
    return retrieve_context.func(query=query, top_k=converted_top_k)


# MCP-compatible wrapper for agent_chat_handler
# Collects streaming response into a single string for MCP tools
def ask_comindware(message: str) -> str:
    """Ask questions about Comindware Platform documentation and get intelligent answers with citations.

    The assistant automatically searches the knowledge base to find relevant articles
    and provides comprehensive answers based on official documentation. Use this for
    technical questions, configuration help, API usage, troubleshooting, and general
    platform guidance.

    Args:
        message: User's current message or question

    Returns:
        Complete response text with citations
    """
    # Collect all chunks from the generator into a single response
    response_parts = []
    last_text_response = None
    generator = None
    try:
        # Call the handler with empty history and None request (MCP context)
        # Note: agent_chat_handler is the generator function used by ChatInterface
        generator = agent_chat_handler(message=message, history=[], request=None)

        # Consume the entire generator to collect all responses
        # The generator now yields: full message history lists (for ChatInterface)
        # Extract the final answer from the last assistant message in the history
        for chunk in generator:
            if chunk is None:
                continue

            # Handle full history lists (new format - preserves all messages)
            if isinstance(chunk, list):
                # Extract final answer from last assistant message
                for msg in reversed(chunk):
                    if isinstance(msg, dict) and msg.get("role") == "assistant":
                        content = msg.get("content", "")
                        # Skip metadata-only messages (thinking blocks)
                        if content and not msg.get("metadata"):
                            if content.strip():
                                last_text_response = content
                                break
            # Handle string responses (backward compatibility)
            elif isinstance(chunk, str):
                if chunk.strip():  # Only add non-empty strings
                    response_parts.append(chunk)
                    last_text_response = chunk
            # Handle dict responses (backward compatibility)
            elif isinstance(chunk, dict):
                # Only extract text content from dicts, skip pure metadata
                content = chunk.get("content", "")
                if content and isinstance(content, str) and content.strip():
                    response_parts.append(content)
                    last_text_response = content

        # Ensure generator is fully consumed
        if generator:
            try:
                # Try to close the generator if it's still open
                generator.close()
            except Exception:
                pass

        # Return the final accumulated response (last chunk contains the full formatted text)
        if last_text_response:
            return last_text_response
        elif response_parts:
            # Fallback: join all parts if no single final response
            return "\n".join(response_parts)
        else:
            return "No response generated. Please try rephrasing your question."

    except StopIteration:
        # Generator exhausted normally
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return "No response generated."
    except IndexError as e:
        # Handle specific "pop index out of range" error
        logger.error("IndexError in ask_comindware (pop index): %s", e, exc_info=True)
        import traceback
        logger.error("Traceback: %s", traceback.format_exc())
        # Try to return whatever we collected
        if last_text_response:
            return last_text_response
        elif response_parts:
            return "\n".join(response_parts)
        return f"Error processing response: {str(e)}. Please try again."
    except Exception as e:
        logger.error("Error in ask_comindware: %s", e, exc_info=True)
        import traceback
        error_details = traceback.format_exc()
        logger.error("Full traceback: %s", error_details)
        # Try to return whatever we collected before the error
        if last_text_response:
            return last_text_response + f"\n\n[Note: An error occurred during processing: {type(e).__name__}]"
        elif response_parts:
            return "\n".join(response_parts) + f"\n\n[Note: An error occurred: {str(e)}]"
        return f"Error: {str(e)}. Please try rephrasing your question or contact support."

with gr.Blocks() as demo:
    # ChatInterface for UI only
    # Note: ChatInterface automatically exposes its function (agent_chat_handler generator),
    # but we register ask_comindware separately for MCP access via filtered endpoint
    gr.ChatInterface(
        fn=handler_fn,
        title=chat_title,
        description=chat_description,
        save_history=True,
        #fill_width=True,
        chatbot=chatbot_config,
        # Attempt to hide auto-generated API endpoint from API docs and MCP
        # Note: According to docs, this may not be effective for MCP, but worth trying
        api_visibility="private",  # Completely disable the API endpoint
    )
    gr.api(
        fn=get_knowledge_base_articles,
        api_name="get_knowledge_base_articles",
        api_description="Search the Comindware Platform documentation knowledge base and retrieve relevant articles with full content. Returns structured JSON with article titles, URLs, content, and metadata. Use this for programmatic access to documentation content. For conversational answers, use ask_comindware instead.",
    )
    # Register the working wrapper function with a business-oriented name for MCP consumers
    # This provides a clean API name that's meaningful to external tools like Cursor
    gr.api(
        fn=ask_comindware,
        api_name="ask_comindware",
        api_description="Ask questions about Comindware Platform documentation and get intelligent answers with citations. The assistant automatically searches the knowledge base to find relevant articles and provides comprehensive answers based on official documentation. Use this for technical questions, configuration help, API usage, troubleshooting, and general platform guidance.",
    )

    # Explicitly set a plain attribute for tests and downstream code to read
    demo.title = "Comindware Platform Documentation Assistant"

if __name__ == "__main__":
    logger = logging.getLogger(__name__)

    logger.info(
        "Starting Gradio server at %s:%s (share=%s)",
        settings.gradio_server_name,
        settings.gradio_server_port,
        settings.gradio_share,
    )

    if settings.gradio_share:
        logger.info(
            "Share link enabled. If share link creation fails, the app will still run locally."
        )

    demo.queue().launch(
        server_name=settings.gradio_server_name,
        server_port=settings.gradio_server_port,
        share=settings.gradio_share,
        mcp_server=True,
        footer_links=["api"],
    )



```

```python
    # Add disclaimer to history only if it doesn't exist yet
    # Keep this check since disclaimer should only appear once per conversation
    if not disclaimer_exists:
        disclaimer_msg = {
            "role": "assistant",
            "content": AI_DISCLAIMER,
        }
        gradio_history.append(disclaimer_msg)
        # Yield full history - always yield full working_history like reference agent
        yield list(gradio_history)
```


## Summary of Changes

Simplified our code to match the reference agent pattern:

1. User message: always append (no duplicate check)
2. Search started: always append (no duplicate check)
3. Thinking blocks: always append (no duplicate check)

## Reference Agent Pattern

The reference agent:
- Uses regular `gr.Chatbot` (not `ChatInterface`)
- Builds `working_history = history + [new messages]`
- Always yields the full `working_history` list
- Updates messages in-place by index
- Doesn't check for duplicates — trusts the `history` parameter

## The Real Issue

The reference agent uses a regular `gr.Chatbot` with manual button clicks, not `ChatInterface`. This avoids ChatInterface's internal history management that causes duplication.

## Next Steps

1. Test the simplified code — it may work better without duplicate checks
2. Consider switching to regular `gr.Chatbot` like the reference agent (bigger change)
3. Investigate why ChatInterface appends instead of replacing

The simplified code should be closer to the reference agent's pattern. Test it and see if duplication is reduced. If it persists, we may need to switch from `ChatInterface` to regular `gr.Chatbot` like the reference agent.

---

**User**

all in vane

Switching from ChatInterface to a regular gr.Chatbot like the reference agent to avoid ChatInterface's internal history management:

first do with the @rag_engine/scripts/test_gradio_chatinterface_history.py 

---

**Cursor**


Fixing all yield statements to return tuples (history, message) instead of just lists:


```python
    if not message or not message.strip():
        yield history, ""
        return
```


Switching the test script to use a regular `gr.Chatbot` like the reference agent, instead of `ChatInterface`:


```python
"""Test script to test regular gr.Chatbot approach (like reference agent).

This script uses regular gr.Chatbot with manual button clicks instead of ChatInterface.
This matches the reference agent pattern which doesn't duplicate history.

Based on reference agent: .reference-repos/.cmw-platform-agent/agent_ng/tabs/chat_tab.py

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Pattern (from reference agent):
- Use regular gr.Chatbot (NOT ChatInterface)
- Build working_history = history + [new messages]
- Always yield full working_history list
- Update messages in-place by index
- Don't check for duplicates - trust history parameter
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, and a new UUID for state
    return [], uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[tuple[list[dict], str], None, None]:
    """Chat handler using reference agent pattern.

    Reference agent pattern:
    1. Build working_history = history + [new messages]
    2. Always yield full working_history list
    3. Update messages in-place by index
    4. Don't check for duplicates - trust history parameter
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Reference agent pattern: working_history = history + [new messages]
    # Start with provided history and build incrementally
    working_history = list(history) if history else []

    # Add user message (reference agent: always add, don't check)
    user_msg = {"role": "user", "content": message}
    working_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    working_history.append(search_started)
    # Yield full history (reference agent pattern: always yield full working_history)
    yield working_history, ""

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query (reference agent: update in-place by index)
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield working_history, ""

    # Add search completed
    search_completed = simulate_search_completed(5)
    working_history.append(search_completed)
    # Yield full history with search completed
    yield working_history, ""

    # Stream answer - update last message in-place (reference agent pattern)
    answer = ""
    assistant_message_index = len(working_history) - 1  # Track assistant message index
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message in-place (reference agent pattern)
        if assistant_message_index >= 0 and assistant_message_index < len(working_history):
            working_history[assistant_message_index] = {"role": "assistant", "content": answer}
        else:
            working_history.append({"role": "assistant", "content": answer})
            assistant_message_index = len(working_history) - 1
        # Yield full history - always yield full working_history
        yield working_history, ""
        time.sleep(0.2)


with gr.Blocks(title="Gradio Chatbot Test (Reference Agent Pattern)") as demo:
    gr.Markdown("# Gradio Chatbot Test - Reference Agent Pattern")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using regular gr.Chatbot (NOT ChatInterface)

        This matches the reference agent pattern:
        - Regular gr.Chatbot component (not ChatInterface)
        - Manual button clicks (send button, clear button)
        - Build working_history = history + [new messages]
        - Always yield full working_history list
        - Update messages in-place by index

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create chatbot component (like reference agent)
    chatbot = gr.Chatbot(
        label="Test Chat",
        height=400,
        type="messages",
        show_label=True,
        container=True,
        show_copy_button=True,
    )

    # Create message input (like reference agent)
    msg = gr.MultimodalTextbox(
        label="Message",
        placeholder="Type your message...",
        lines=2,
        max_lines=4,
    )

    # Create buttons (like reference agent)
    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")

    # Create state for session tracking
    session_state = gr.State(value=uuid4())

    # Connect events (like reference agent)
    # Send button click
    send_btn.click(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Message submit (Enter key)
    msg.submit(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Clear button
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot, session_state]
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Converted the test script to use a regular `gr.Chatbot` instead of `ChatInterface`, following the reference agent pattern.

## Changes

1. Removed `ChatInterface` — using regular `gr.Chatbot` component
2. Manual UI setup:
   - `gr.Chatbot` component
   - `gr.MultimodalTextbox` for input
   - `gr.Button` for Send and Clear
3. Manual event connections:
   - `send_btn.click()` for send button
   - `msg.submit()` for Enter key
   - `clear_btn.click()` for clear
4. Reference agent pattern in handler:
   - Build `working_history = history + [new messages]`
   - Always yield full `working_history` list
   - Update messages in-place by index
   - No duplicate checks — trust the `history` parameter

The test server should be running on `http://127.0.0.1:7861`. Test it:

1. Send "Привет" → should appear once
2. Send "Как дела?" → previous conversation should not duplicate
3. Verify thinking blocks persist during streaming
4. Click "Clear" to reset

This matches the reference agent pattern. If it works without duplication, we can apply the same pattern to the main app (`rag_engine/api/app.py`).

---

**User**

https://www.gradio.app/docs/gradio/chatbot 

https://www.gradio.app/guides/conversational-chatbot 

do not forget that we use gradio 6 @powershell (998-1013) 

---

**Cursor**


Fixing the test script: in Gradio 6, `gr.Chatbot` doesn't accept `type="messages"`. Removing that parameter:


```python
"""Test script to test regular gr.Chatbot approach (like reference agent).

This script uses regular gr.Chatbot with manual button clicks instead of ChatInterface.
This matches the reference agent pattern which doesn't duplicate history.

Based on reference agent: .reference-repos/.cmw-platform-agent/agent_ng/tabs/chat_tab.py

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Pattern (from reference agent):
- Use regular gr.Chatbot (NOT ChatInterface)
- Build working_history = history + [new messages]
- Always yield full working_history list
- Update messages in-place by index
- Don't check for duplicates - trust history parameter
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, and a new UUID for state
    return [], uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[tuple[list[dict], str], None, None]:
    """Chat handler using reference agent pattern.

    Reference agent pattern:
    1. Build working_history = history + [new messages]
    2. Always yield full working_history list
    3. Update messages in-place by index
    4. Don't check for duplicates - trust history parameter
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Reference agent pattern: working_history = history + [new messages]
    # Start with provided history and build incrementally
    working_history = list(history) if history else []

    # Add user message (reference agent: always add, don't check)
    user_msg = {"role": "user", "content": message}
    working_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    working_history.append(search_started)
    # Yield full history (reference agent pattern: always yield full working_history)
    yield working_history, ""

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query (reference agent: update in-place by index)
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield working_history, ""

    # Add search completed
    search_completed = simulate_search_completed(5)
    working_history.append(search_completed)
    # Yield full history with search completed
    yield working_history, ""

    # Stream answer - update last message in-place (reference agent pattern)
    answer = ""
    assistant_message_index = len(working_history) - 1  # Track assistant message index
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message in-place (reference agent pattern)
        if assistant_message_index >= 0 and assistant_message_index < len(working_history):
            working_history[assistant_message_index] = {"role": "assistant", "content": answer}
        else:
            working_history.append({"role": "assistant", "content": answer})
            assistant_message_index = len(working_history) - 1
        # Yield full history - always yield full working_history
        yield working_history, ""
        time.sleep(0.2)


with gr.Blocks(title="Gradio Chatbot Test (Reference Agent Pattern)") as demo:
    gr.Markdown("# Gradio Chatbot Test - Reference Agent Pattern")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using regular gr.Chatbot (NOT ChatInterface)

        This matches the reference agent pattern:
        - Regular gr.Chatbot component (not ChatInterface)
        - Manual button clicks (send button, clear button)
        - Build working_history = history + [new messages]
        - Always yield full working_history list
        - Update messages in-place by index

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create chatbot component (like reference agent)
    # In Gradio 6, Chatbot uses messages format by default (no type parameter)
    chatbot = gr.Chatbot(
        label="Test Chat",
        height=400,
        show_label=True,
        container=True,
        show_copy_button=True,
    )

    # Create message input (like reference agent)
    msg = gr.MultimodalTextbox(
        label="Message",
        placeholder="Type your message...",
        lines=2,
        max_lines=4,
    )

    # Create buttons (like reference agent)
    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")

    # Create state for session tracking
    session_state = gr.State(value=uuid4())

    # Connect events (like reference agent)
    # Send button click
    send_btn.click(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Message submit (Enter key)
    msg.submit(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Clear button
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot, session_state]
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Removed the `type="messages"` parameter. In Gradio 6, `gr.Chatbot` uses the messages format by default, so that parameter isn't needed. Verifying the script is correct:



Removed the `type="messages"` parameter. In Gradio 6, `gr.Chatbot` uses the messages format by default, so that parameter isn't needed.

The test script should now work. It follows the reference agent pattern:

1. Regular `gr.Chatbot` (not `ChatInterface`)
2. Manual UI components (`MultimodalTextbox`, buttons)
3. Reference agent pattern: `working_history = history + [new messages]`
4. Always yields full `working_history` list
5. Updates messages in-place by index

The test server should be running on `http://127.0.0.1:7861`. Test it:

1. Send "Привет" → should appear once
2. Send "Как дела?" → previous conversation should not duplicate
3. Verify thinking blocks persist during streaming
4. Click "Clear" to reset

If this works without duplication, we can apply the same pattern to the main app (`rag_engine/api/app.py`).

---

**User**

@powershell (996-1013) 

https://www.gradio.app/main/guides/gradio-6-migration-guide#other-removed-component-parameters 

---

**Cursor**


Replacing `show_copy_button` with `buttons=["copy"]` per the Gradio 6 migration guide:


```python
"""Test script to test regular gr.Chatbot approach (like reference agent).

This script uses regular gr.Chatbot with manual button clicks instead of ChatInterface.
This matches the reference agent pattern which doesn't duplicate history.

Based on reference agent: .reference-repos/.cmw-platform-agent/agent_ng/tabs/chat_tab.py

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Pattern (from reference agent):
- Use regular gr.Chatbot (NOT ChatInterface)
- Build working_history = history + [new messages]
- Always yield full working_history list
- Update messages in-place by index
- Don't check for duplicates - trust history parameter
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, and a new UUID for state
    return [], uuid4()


def chat_handler(message: str, history: list[dict], session_uuid) -> Generator[tuple[list[dict], str], None, None]:
    """Chat handler using reference agent pattern.

    Reference agent pattern:
    1. Build working_history = history + [new messages]
    2. Always yield full working_history list
    3. Update messages in-place by index
    4. Don't check for duplicates - trust history parameter
    """
    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    # Reference agent pattern: working_history = history + [new messages]
    # Start with provided history and build incrementally
    working_history = list(history) if history else []

    # Add user message (reference agent: always add, don't check)
    user_msg = {"role": "user", "content": message}
    working_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    working_history.append(search_started)
    # Yield full history (reference agent pattern: always yield full working_history)
    yield working_history, ""

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query (reference agent: update in-place by index)
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield working_history, ""

    # Add search completed
    search_completed = simulate_search_completed(5)
    working_history.append(search_completed)
    # Yield full history with search completed
    yield working_history, ""

    # Stream answer - update last message in-place (reference agent pattern)
    answer = ""
    assistant_message_index = len(working_history) - 1  # Track assistant message index
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message in-place (reference agent pattern)
        if assistant_message_index >= 0 and assistant_message_index < len(working_history):
            working_history[assistant_message_index] = {"role": "assistant", "content": answer}
        else:
            working_history.append({"role": "assistant", "content": answer})
            assistant_message_index = len(working_history) - 1
        # Yield full history - always yield full working_history
        yield working_history, ""
        time.sleep(0.2)


with gr.Blocks(title="Gradio Chatbot Test (Reference Agent Pattern)") as demo:
    gr.Markdown("# Gradio Chatbot Test - Reference Agent Pattern")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using regular gr.Chatbot (NOT ChatInterface)

        This matches the reference agent pattern:
        - Regular gr.Chatbot component (not ChatInterface)
        - Manual button clicks (send button, clear button)
        - Build working_history = history + [new messages]
        - Always yield full working_history list
        - Update messages in-place by index

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create chatbot component (like reference agent)
    # In Gradio 6, Chatbot uses messages format by default (no type parameter)
    # show_copy_button removed - use buttons parameter instead
    chatbot = gr.Chatbot(
        label="Test Chat",
        height=400,
        show_label=True,
        container=True,
        buttons=["copy"],
    )

    # Create message input (like reference agent)
    msg = gr.MultimodalTextbox(
        label="Message",
        placeholder="Type your message...",
        lines=2,
        max_lines=4,
    )

    # Create buttons (like reference agent)
    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")

    # Create state for session tracking
    session_state = gr.State(value=uuid4())

    # Connect events (like reference agent)
    # Send button click
    send_btn.click(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Message submit (Enter key)
    msg.submit(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Clear button
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot, session_state]
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Fixed the `show_copy_button` parameter. Per the [Gradio 6 Migration Guide](https://www.gradio.app/main/guides/gradio-6-migration-guide#other-removed-component-parameters), `show_copy_button` was removed and replaced with `buttons=["copy"]`.

The test script should now work with Gradio 6. Changes:
1. Removed `type="messages"` (Gradio 6 uses messages format by default)
2. Replaced `show_copy_button=True` with `buttons=["copy"]`

The test server should be running on `http://127.0.0.1:7861`. Test it:
1. Send "Привет" → should appear once
2. Send "Как дела?" → previous conversation should not duplicate
3. Verify thinking blocks persist during streaming
4. Click "Clear" to reset

If this works without duplication, we can apply the same pattern to the main app (`rag_engine/api/app.py`).

---

**User**

@python (973-1013) 

---

**Cursor**


The `MultimodalTextbox` returns a dict `{'text': '...', 'files': []}`, but the handler expects a string. Updating the handler to extract the text, as in the reference agent:


```python
"""Test script to test regular gr.Chatbot approach (like reference agent).

This script uses regular gr.Chatbot with manual button clicks instead of ChatInterface.
This matches the reference agent pattern which doesn't duplicate history.

Based on reference agent: .reference-repos/.cmw-platform-agent/agent_ng/tabs/chat_tab.py

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Pattern (from reference agent):
- Use regular gr.Chatbot (NOT ChatInterface)
- Build working_history = history + [new messages]
- Always yield full working_history list
- Update messages in-place by index
- Don't check for duplicates - trust history parameter
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, and a new UUID for state
    return [], uuid4()


def chat_handler(multimodal_value, history: list[dict], session_uuid) -> Generator[tuple[list[dict], dict], None, None]:
    """Chat handler using reference agent pattern.

    Reference agent pattern:
    1. Extract text from MultimodalTextbox format (dict with 'text' and 'files')
    2. Build working_history = history + [new messages]
    3. Always yield full working_history list
    4. Update messages in-place by index
    5. Don't check for duplicates - trust history parameter
    """
    # Extract text from MultimodalTextbox format (like reference agent)
    if isinstance(multimodal_value, dict):
        message = multimodal_value.get("text", "")
        files = multimodal_value.get("files", [])
    else:
        # Fallback for non-dict values
        message = str(multimodal_value) if multimodal_value else ""
        files = []

    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    if not message or not message.strip():
        yield history, {"text": "", "files": []}
        return

    # Reference agent pattern: working_history = history + [new messages]
    # Start with provided history and build incrementally
    working_history = list(history) if history else []

    # Add user message (reference agent: always add, don't check)
    user_msg = {"role": "user", "content": message}
    working_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    working_history.append(search_started)
    # Yield full history (reference agent pattern: always yield full working_history)
    yield working_history, ""

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query (reference agent: update in-place by index)
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield working_history, ""

    # Add search completed
    search_completed = simulate_search_completed(5)
    working_history.append(search_completed)
    # Yield full history with search completed
    yield working_history, ""

    # Stream answer - update last message in-place (reference agent pattern)
    answer = ""
    assistant_message_index = len(working_history) - 1  # Track assistant message index
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message in-place (reference agent pattern)
        if assistant_message_index >= 0 and assistant_message_index < len(working_history):
            working_history[assistant_message_index] = {"role": "assistant", "content": answer}
        else:
            working_history.append({"role": "assistant", "content": answer})
            assistant_message_index = len(working_history) - 1
        # Yield full history - always yield full working_history
        yield working_history, ""
        time.sleep(0.2)


with gr.Blocks(title="Gradio Chatbot Test (Reference Agent Pattern)") as demo:
    gr.Markdown("# Gradio Chatbot Test - Reference Agent Pattern")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using regular gr.Chatbot (NOT ChatInterface)

        This matches the reference agent pattern:
        - Regular gr.Chatbot component (not ChatInterface)
        - Manual button clicks (send button, clear button)
        - Build working_history = history + [new messages]
        - Always yield full working_history list
        - Update messages in-place by index

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create chatbot component (like reference agent)
    # In Gradio 6, Chatbot uses messages format by default (no type parameter)
    # show_copy_button removed - use buttons parameter instead
    chatbot = gr.Chatbot(
        label="Test Chat",
        height=400,
        show_label=True,
        container=True,
        buttons=["copy"],
    )

    # Create message input (like reference agent)
    msg = gr.MultimodalTextbox(
        label="Message",
        placeholder="Type your message...",
        lines=2,
        max_lines=4,
    )

    # Create buttons (like reference agent)
    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")

    # Create state for session tracking
    session_state = gr.State(value=uuid4())

    # Connect events (like reference agent)
    # Send button click
    send_btn.click(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Message submit (Enter key)
    msg.submit(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Clear button
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot, session_state]
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
"""Test script to test regular gr.Chatbot approach (like reference agent).

This script uses regular gr.Chatbot with manual button clicks instead of ChatInterface.
This matches the reference agent pattern which doesn't duplicate history.

Based on reference agent: .reference-repos/.cmw-platform-agent/agent_ng/tabs/chat_tab.py

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Pattern (from reference agent):
- Use regular gr.Chatbot (NOT ChatInterface)
- Build working_history = history + [new messages]
- Always yield full working_history list
- Update messages in-place by index
- Don't check for duplicates - trust history parameter
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, and a new UUID for state
    return [], uuid4()


def chat_handler(multimodal_value, history: list[dict], session_uuid) -> Generator[tuple[list[dict], dict], None, None]:
    """Chat handler using reference agent pattern.

    Reference agent pattern:
    1. Extract text from MultimodalTextbox format (dict with 'text' and 'files')
    2. Build working_history = history + [new messages]
    3. Always yield full working_history list
    4. Update messages in-place by index
    5. Don't check for duplicates - trust history parameter
    """
    # Extract text from MultimodalTextbox format (like reference agent)
    if isinstance(multimodal_value, dict):
        message = multimodal_value.get("text", "")
        files = multimodal_value.get("files", [])
    else:
        # Fallback for non-dict values
        message = str(multimodal_value) if multimodal_value else ""
        files = []

    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    if not message or not message.strip():
        yield history, {"text": "", "files": []}
        return

    # Reference agent pattern: working_history = history + [new messages]
    # Start with provided history and build incrementally
    working_history = list(history) if history else []

    # Add user message (reference agent: always add, don't check)
    user_msg = {"role": "user", "content": message}
    working_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    working_history.append(search_started)
    # Yield full history (reference agent pattern: always yield full working_history)
    # Return empty MultimodalTextbox value to clear input
    yield working_history, {"text": "", "files": []}

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query (reference agent: update in-place by index)
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield working_history, {"text": "", "files": []}

    # Add search completed
    search_completed = simulate_search_completed(5)
    working_history.append(search_completed)
    # Yield full history with search completed
    yield working_history, {"text": "", "files": []}

    # Stream answer - update last message in-place (reference agent pattern)
    answer = ""
    assistant_message_index = len(working_history) - 1  # Track assistant message index
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message in-place (reference agent pattern)
        if assistant_message_index >= 0 and assistant_message_index < len(working_history):
            working_history[assistant_message_index] = {"role": "assistant", "content": answer}
        else:
            working_history.append({"role": "assistant", "content": answer})
            assistant_message_index = len(working_history) - 1
        # Yield full history - always yield full working_history
        # Return empty MultimodalTextbox value to clear input
        yield working_history, {"text": "", "files": []}
        time.sleep(0.2)


with gr.Blocks(title="Gradio Chatbot Test (Reference Agent Pattern)") as demo:
    gr.Markdown("# Gradio Chatbot Test - Reference Agent Pattern")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using regular gr.Chatbot (NOT ChatInterface)

        This matches the reference agent pattern:
        - Regular gr.Chatbot component (not ChatInterface)
        - Manual button clicks (send button, clear button)
        - Build working_history = history + [new messages]
        - Always yield full working_history list
        - Update messages in-place by index

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create chatbot component (like reference agent)
    # In Gradio 6, Chatbot uses messages format by default (no type parameter)
    # show_copy_button removed - use buttons parameter instead
    chatbot = gr.Chatbot(
        label="Test Chat",
        height=400,
        show_label=True,
        container=True,
        buttons=["copy"],
    )

    # Create message input (like reference agent)
    msg = gr.MultimodalTextbox(
        label="Message",
        placeholder="Type your message...",
        lines=2,
        max_lines=4,
    )

    # Create buttons (like reference agent)
    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")

    # Create state for session tracking
    session_state = gr.State(value=uuid4())

    # Connect events (like reference agent)
    # Send button click
    send_btn.click(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Message submit (Enter key)
    msg.submit(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Clear button
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot, session_state]
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
"""Test script to test regular gr.Chatbot approach (like reference agent).

This script uses regular gr.Chatbot with manual button clicks instead of ChatInterface.
This matches the reference agent pattern which doesn't duplicate history.

Based on reference agent: .reference-repos/.cmw-platform-agent/agent_ng/tabs/chat_tab.py

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Pattern (from reference agent):
- Use regular gr.Chatbot (NOT ChatInterface)
- Build working_history = history + [new messages]
- Always yield full working_history list
- Update messages in-place by index
- Don't check for duplicates - trust history parameter
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, empty MultimodalTextbox value, and a new UUID for state
    return [], {"text": "", "files": []}, uuid4()


def chat_handler(multimodal_value, history: list[dict], session_uuid) -> Generator[tuple[list[dict], dict], None, None]:
    """Chat handler using reference agent pattern.

    Reference agent pattern:
    1. Extract text from MultimodalTextbox format (dict with 'text' and 'files')
    2. Build working_history = history + [new messages]
    3. Always yield full working_history list
    4. Update messages in-place by index
    5. Don't check for duplicates - trust history parameter
    """
    # Extract text from MultimodalTextbox format (like reference agent)
    if isinstance(multimodal_value, dict):
        message = multimodal_value.get("text", "")
        files = multimodal_value.get("files", [])
    else:
        # Fallback for non-dict values
        message = str(multimodal_value) if multimodal_value else ""
        files = []

    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    if not message or not message.strip():
        yield history, {"text": "", "files": []}
        return

    # Reference agent pattern: working_history = history + [new messages]
    # Start with provided history and build incrementally
    working_history = list(history) if history else []

    # Add user message (reference agent: always add, don't check)
    user_msg = {"role": "user", "content": message}
    working_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    working_history.append(search_started)
    # Yield full history (reference agent pattern: always yield full working_history)
    # Return empty MultimodalTextbox value to clear input
    yield working_history, {"text": "", "files": []}

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query (reference agent: update in-place by index)
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield working_history, {"text": "", "files": []}

    # Add search completed
    search_completed = simulate_search_completed(5)
    working_history.append(search_completed)
    # Yield full history with search completed
    yield working_history, {"text": "", "files": []}

    # Stream answer - update last message in-place (reference agent pattern)
    answer = ""
    assistant_message_index = len(working_history) - 1  # Track assistant message index
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message in-place (reference agent pattern)
        if assistant_message_index >= 0 and assistant_message_index < len(working_history):
            working_history[assistant_message_index] = {"role": "assistant", "content": answer}
        else:
            working_history.append({"role": "assistant", "content": answer})
            assistant_message_index = len(working_history) - 1
        # Yield full history - always yield full working_history
        # Return empty MultimodalTextbox value to clear input
        yield working_history, {"text": "", "files": []}
        time.sleep(0.2)


with gr.Blocks(title="Gradio Chatbot Test (Reference Agent Pattern)") as demo:
    gr.Markdown("# Gradio Chatbot Test - Reference Agent Pattern")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using regular gr.Chatbot (NOT ChatInterface)

        This matches the reference agent pattern:
        - Regular gr.Chatbot component (not ChatInterface)
        - Manual button clicks (send button, clear button)
        - Build working_history = history + [new messages]
        - Always yield full working_history list
        - Update messages in-place by index

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create chatbot component (like reference agent)
    # In Gradio 6, Chatbot uses messages format by default (no type parameter)
    # show_copy_button removed - use buttons parameter instead
    chatbot = gr.Chatbot(
        label="Test Chat",
        height=400,
        show_label=True,
        container=True,
        buttons=["copy"],
    )

    # Create message input (like reference agent)
    msg = gr.MultimodalTextbox(
        label="Message",
        placeholder="Type your message...",
        lines=2,
        max_lines=4,
    )

    # Create buttons (like reference agent)
    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")

    # Create state for session tracking
    session_state = gr.State(value=uuid4())

    # Connect events (like reference agent)
    # Send button click
    send_btn.click(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Message submit (Enter key)
    msg.submit(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Clear button
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot, session_state]
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```

```python
"""Test script to test regular gr.Chatbot approach (like reference agent).

This script uses regular gr.Chatbot with manual button clicks instead of ChatInterface.
This matches the reference agent pattern which doesn't duplicate history.

Based on reference agent: .reference-repos/.cmw-platform-agent/agent_ng/tabs/chat_tab.py

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Pattern (from reference agent):
- Use regular gr.Chatbot (NOT ChatInterface)
- Build working_history = history + [new messages]
- Always yield full working_history list
- Update messages in-place by index
- Don't check for duplicates - trust history parameter
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, empty MultimodalTextbox value, and a new UUID for state
    return [], {"text": "", "files": []}, uuid4()


def chat_handler(multimodal_value, history: list[dict], session_uuid) -> Generator[tuple[list[dict], dict], None, None]:
    """Chat handler using reference agent pattern.

    Reference agent pattern:
    1. Extract text from MultimodalTextbox format (dict with 'text' and 'files')
    2. Build working_history = history + [new messages]
    3. Always yield full working_history list
    4. Update messages in-place by index
    5. Don't check for duplicates - trust history parameter
    """
    # Extract text from MultimodalTextbox format (like reference agent)
    if isinstance(multimodal_value, dict):
        message = multimodal_value.get("text", "")
        files = multimodal_value.get("files", [])
    else:
        # Fallback for non-dict values
        message = str(multimodal_value) if multimodal_value else ""
        files = []

    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    if not message or not message.strip():
        yield history, {"text": "", "files": []}
        return

    # Reference agent pattern: working_history = history + [new messages]
    # Start with provided history and build incrementally
    working_history = list(history) if history else []

    # Add user message (reference agent: always add, don't check)
    user_msg = {"role": "user", "content": message}
    working_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    working_history.append(search_started)
    # Yield full history (reference agent pattern: always yield full working_history)
    # Return empty MultimodalTextbox value to clear input
    yield working_history, {"text": "", "files": []}

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query (reference agent: update in-place by index)
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield working_history, {"text": "", "files": []}

    # Add search completed
    search_completed = simulate_search_completed(5)
    working_history.append(search_completed)
    # Yield full history with search completed
    yield working_history, {"text": "", "files": []}

    # Stream answer - update last message in-place (reference agent pattern)
    answer = ""
    assistant_message_index = len(working_history) - 1  # Track assistant message index
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message in-place (reference agent pattern)
        if assistant_message_index >= 0 and assistant_message_index < len(working_history):
            working_history[assistant_message_index] = {"role": "assistant", "content": answer}
        else:
            working_history.append({"role": "assistant", "content": answer})
            assistant_message_index = len(working_history) - 1
        # Yield full history - always yield full working_history
        # Return empty MultimodalTextbox value to clear input
        yield working_history, {"text": "", "files": []}
        time.sleep(0.2)


with gr.Blocks(title="Gradio Chatbot Test (Reference Agent Pattern)") as demo:
    gr.Markdown("# Gradio Chatbot Test - Reference Agent Pattern")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using regular gr.Chatbot (NOT ChatInterface)

        This matches the reference agent pattern:
        - Regular gr.Chatbot component (not ChatInterface)
        - Manual button clicks (send button, clear button)
        - Build working_history = history + [new messages]
        - Always yield full working_history list
        - Update messages in-place by index

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create chatbot component (like reference agent)
    # In Gradio 6, Chatbot uses messages format by default (no type parameter)
    # show_copy_button removed - use buttons parameter instead
    chatbot = gr.Chatbot(
        label="Test Chat",
        height=400,
        show_label=True,
        container=True,
        buttons=["copy"],
    )

    # Create message input (like reference agent)
    msg = gr.MultimodalTextbox(
        label="Message",
        placeholder="Type your message...",
        lines=2,
        max_lines=4,
    )

    # Create buttons (like reference agent)
    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")

    # Create state for session tracking
    session_state = gr.State(value=uuid4())

    # Connect events (like reference agent)
    # Send button click
    send_btn.click(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Message submit (Enter key)
    msg.submit(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Clear button
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot, msg, session_state]
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Fixing the linter warning:


```python
"""Test script to test regular gr.Chatbot approach (like reference agent).

This script uses regular gr.Chatbot with manual button clicks instead of ChatInterface.
This matches the reference agent pattern which doesn't duplicate history.

Based on reference agent: .reference-repos/.cmw-platform-agent/agent_ng/tabs/chat_tab.py

Usage:
    python -m rag_engine.scripts.test_gradio_chatinterface_history

    Or activate venv first:
    .venv\\Scripts\\Activate.ps1  # Windows PowerShell
    .venv-wsl/bin/activate         # WSL/Linux
    python -m rag_engine.scripts.test_gradio_chatinterface_history

The app will launch on http://127.0.0.1:7861

Key Pattern (from reference agent):
- Use regular gr.Chatbot (NOT ChatInterface)
- Build working_history = history + [new messages]
- Always yield full working_history list
- Update messages in-place by index
- Don't check for duplicates - trust history parameter
"""

import time
from collections.abc import Generator
from uuid import uuid4

import gradio as gr


def simulate_search_started(query: str) -> dict:
    """Simulate search started thinking block."""
    return {
        "role": "assistant",
        "content": f"Ищу: {query}",
        "metadata": {"title": "🧠 Поиск информации в базе знаний"},
    }


def simulate_search_completed(count: int) -> dict:
    """Simulate search completed thinking block."""
    return {
        "role": "assistant",
        "content": f"Найдено статей: {count}.",
        "metadata": {"title": "✅ Поиск завершен"},
    }


def clear_history():
    """Clear chatbot history and reset state."""
    print("History cleared!")
    # Return empty list for chatbot, empty MultimodalTextbox value, and a new UUID for state
    return [], {"text": "", "files": []}, uuid4()


def chat_handler(multimodal_value, history: list[dict], session_uuid) -> Generator[tuple[list[dict], dict], None, None]:
    """Chat handler using reference agent pattern.

    Reference agent pattern:
    1. Extract text from MultimodalTextbox format (dict with 'text' and 'files')
    2. Build working_history = history + [new messages]
    3. Always yield full working_history list
    4. Update messages in-place by index
    5. Don't check for duplicates - trust history parameter
    """
    # Extract text from MultimodalTextbox format (like reference agent)
    if isinstance(multimodal_value, dict):
        message = multimodal_value.get("text", "")
        # files = multimodal_value.get("files", [])  # Not used in test, but available for file handling
    else:
        # Fallback for non-dict values
        message = str(multimodal_value) if multimodal_value else ""

    print(f"[Chat] Message: {message}, History length: {len(history)}, Session: {session_uuid}")

    if not message or not message.strip():
        yield history, {"text": "", "files": []}
        return

    # Reference agent pattern: working_history = history + [new messages]
    # Start with provided history and build incrementally
    working_history = list(history) if history else []

    # Add user message (reference agent: always add, don't check)
    user_msg = {"role": "user", "content": message}
    working_history.append(user_msg)

    # Add search started
    search_started = simulate_search_started(message)
    working_history.append(search_started)
    # Yield full history (reference agent pattern: always yield full working_history)
    # Return empty MultimodalTextbox value to clear input
    yield working_history, {"text": "", "files": []}

    # Simulate tool execution
    time.sleep(0.5)

    # Update search started with LLM query (reference agent: update in-place by index)
    search_started["content"] = f"Ищу: {message.upper()}"
    # Yield full history with updated search
    yield working_history, {"text": "", "files": []}

    # Add search completed
    search_completed = simulate_search_completed(5)
    working_history.append(search_completed)
    # Yield full history with search completed
    yield working_history, {"text": "", "files": []}

    # Stream answer - update last message in-place (reference agent pattern)
    answer = ""
    assistant_message_index = len(working_history) - 1  # Track assistant message index
    for chunk in ["Это ", "ответ ", "на ", "вопрос: ", message]:
        answer += chunk
        # Update last message in-place (reference agent pattern)
        if assistant_message_index >= 0 and assistant_message_index < len(working_history):
            working_history[assistant_message_index] = {"role": "assistant", "content": answer}
        else:
            working_history.append({"role": "assistant", "content": answer})
            assistant_message_index = len(working_history) - 1
        # Yield full history - always yield full working_history
        # Return empty MultimodalTextbox value to clear input
        yield working_history, {"text": "", "files": []}
        time.sleep(0.2)


with gr.Blocks(title="Gradio Chatbot Test (Reference Agent Pattern)") as demo:
    gr.Markdown("# Gradio Chatbot Test - Reference Agent Pattern")
    gr.Markdown(
        """
        **Testing with Gradio 6.1.0** - Using regular gr.Chatbot (NOT ChatInterface)

        This matches the reference agent pattern:
        - Regular gr.Chatbot component (not ChatInterface)
        - Manual button clicks (send button, clear button)
        - Build working_history = history + [new messages]
        - Always yield full working_history list
        - Update messages in-place by index

        **Instructions:**
        1. Send first message: "Привет"
        2. Wait for response to complete
        3. Send second message: "Как дела?"
        4. Check if first conversation is duplicated (should NOT duplicate)
        5. Check if thinking blocks persist during streaming (should persist)
        6. Click "Clear" to reset history and state
        """
    )

    # Create chatbot component (like reference agent)
    # In Gradio 6, Chatbot uses messages format by default (no type parameter)
    # show_copy_button removed - use buttons parameter instead
    chatbot = gr.Chatbot(
        label="Test Chat",
        height=400,
        show_label=True,
        container=True,
        buttons=["copy"],
    )

    # Create message input (like reference agent)
    msg = gr.MultimodalTextbox(
        label="Message",
        placeholder="Type your message...",
        lines=2,
        max_lines=4,
    )

    # Create buttons (like reference agent)
    with gr.Row():
        send_btn = gr.Button("Send", variant="primary")
        clear_btn = gr.Button("Clear", variant="secondary")

    # Create state for session tracking
    session_state = gr.State(value=uuid4())

    # Connect events (like reference agent)
    # Send button click
    send_btn.click(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Message submit (Enter key)
    msg.submit(
        fn=chat_handler,
        inputs=[msg, chatbot, session_state],
        outputs=[chatbot, msg],
    )

    # Clear button
    clear_btn.click(
        fn=clear_history,
        outputs=[chatbot, msg, session_state]
    )


if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)

```


Fixed the issue. `MultimodalTextbox` returns a dict `{'text': '...', 'files': []}`, not a plain string. Updated the handler to:

1. Extract text from the MultimodalValue format (like the reference agent)
2. Return empty MultimodalTextbox value `{"text": "", "files": []}` to clear the input after sending
3. Updated `clear_history` to also return the MultimodalTextbox value

The test script should now work. The test server should be running on `http://127.0.0.1:7861`. Test it:

1. Send "Привет" → should appear once
2. Send "Как дела?" → previous conversation should not duplicate
3. Verify thinking blocks persist during streaming
4. Click "Clear" to reset

This matches the reference agent pattern. If it works without duplication, we can apply the same pattern to the main app (`rag_engine/api/app.py`).

---

**User**

works perectly now in the test 

but we do not need multimodal textbox, also enable retry button for chatbot

and we have some nicer styling 

and we do not use ChatMessage?

https://www.gradio.app/docs/gradio/chatbot#message-format 



