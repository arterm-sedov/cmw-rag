# =============================================================================
# CMW-RAG Configuration (model-slug based)
# =============================================================================
# Model slugs are case-insensitive (e.g. qwen/qwen3-embedding-8b = Qwen/Qwen3-Embedding-8B).
#
# Embedding models: ai-forever/FRIDA (direct/infinity), Qwen/Qwen3-Embedding-0.6B|4B|8B (openrouter/infinity).
# Reranker models: DiTy/cross-encoder-russian-msmarco (direct/infinity), BAAI/bge-reranker-v2-m3,
#   Qwen/Qwen3-Reranker-0.6B|4B|8B (infinity). See rag_engine/config/models.yaml for details.
# =============================================================================

# =============================================================================
# LLM Providers (at least one required)
# =============================================================================
GOOGLE_API_KEY=your_google_api_key_here
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# =============================================================================
# Embedding
# =============================================================================
# Provider: direct (sentence-transformers) | infinity (HTTP server) | openrouter (cloud API)
EMBEDDING_PROVIDER_TYPE=direct
EMBEDDING_MODEL=ai-forever/FRIDA
# Device for direct/legacy path (build_index, legacy retriever). Factory uses models.yaml.
EMBEDDING_DEVICE=auto

# Request Configuration
EMBEDDING_TIMEOUT=60.0
EMBEDDING_MAX_RETRIES=3

# Optional overrides (defaults in Settings / models.yaml)
# INFINITY_EMBEDDING_ENDPOINT=http://localhost:7997
# OPENROUTER_ENDPOINT=https://openrouter.ai/api/v1

# =============================================================================
# ChromaDB
# =============================================================================
# Data directory (used by server process, not app directly)
CHROMADB_PERSIST_DIR=./data/chromadb_data
CHROMADB_COLLECTION=mkdocs_kb

# ChromaDB HTTP client (HTTP-only; no CHROMADB_USE_HTTP toggle - see .opencode/plans/chromadb-http-migration.md)
# Start server: python rag_engine/scripts/start_chroma_server.py or chroma run --host ... --port ... --path ...
CHROMADB_HOST=localhost
CHROMADB_PORT=8000
CHROMADB_SSL=false
CHROMADB_CONNECTION_TIMEOUT=30.0
CHROMADB_MAX_CONNECTIONS=100

# =============================================================================
# Retrieval
# =============================================================================
TOP_K_RETRIEVE=20
TOP_K_RERANK=10
CHUNK_SIZE=500
CHUNK_OVERLAP=150

# Retrieval – Multi-vector queries and Query Decomposition
# Safe defaults; override to tune latency/recall trade-offs
RETRIEVAL_MULTIQUERY_ENABLED=true
RETRIEVAL_MULTIQUERY_MAX_SEGMENTS=4
RETRIEVAL_MULTIQUERY_SEGMENT_TOKENS=448
RETRIEVAL_MULTIQUERY_SEGMENT_OVERLAP=64
RETRIEVAL_MULTIQUERY_PRE_RERANK_LIMIT=60

# Optional: LLM-based query decomposition (off by default in code; set true here to enable)
RETRIEVAL_QUERY_DECOMP_ENABLED=true
RETRIEVAL_QUERY_DECOMP_MAX_SUBQUERIES=4

# =============================================================================
# Reranker
# =============================================================================
RERANK_ENABLED=true
# Provider: direct (CrossEncoder) | infinity (HTTP server)
RERANKER_PROVIDER_TYPE=direct
RERANKER_MODEL=DiTy/cross-encoder-russian-msmarco

# Request Configuration
RERANKER_TIMEOUT=60.0
RERANKER_MAX_RETRIES=3

# Optional overrides
# INFINITY_RERANKER_ENDPOINT=http://localhost:7998

# =============================================================================
# LLM
# =============================================================================
# vLLM Configuration (default – optimized for local inference; requires server at http://localhost:8000)
# Start server with: cd ~/cmw-vllm && python3 -m cmw_vllm.cli start
DEFAULT_LLM_PROVIDER=vllm
DEFAULT_MODEL=Qwen/Qwen3-30B-A3B-Instruct-2507
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=EMPTY
LLM_TEMPERATURE=0.1
# vLLM streaming fallback: if True, falls back to invoke() mode when tool calls aren't detected in stream.
# Required because vLLM doesn't respect tool_choice parameter in streaming mode.
# Set to False to disable fallback and test pure streaming (tools won't execute with vLLM).
VLLM_STREAMING_FALLBACK_ENABLED=true
# Optional overrides (if set, override values from model_configs.py)
# LLM_MAX_TOKENS=4096   # Overrides max_tokens from model config (hard cutoff)
# LLM_TOKEN_LIMIT=32768 # Overrides token_limit (context window) from model config
LLM_MILD_LIMIT=1024  # Soft guidance limit in words for response length (injected into system prompt)

# Timezone (IANA name, e.g. Europe/Moscow, UTC)
DEFAULT_TIMEZONE=Europe/Moscow

# Alternative LLM providers (uncomment to use)
# DEFAULT_LLM_PROVIDER=gemini
# DEFAULT_MODEL=gemini-2.5-flash
# LLM_TEMPERATURE=0.1
# LLM_MAX_TOKENS=4096
#
# DEFAULT_LLM_PROVIDER=openrouter
# DEFAULT_MODEL=qwen/qwen3-30b-a3b-instruct-2507
# LLM_TEMPERATURE=0.1
# LLM_MAX_TOKENS=65536
#
# Alternative model options:
# DEFAULT_MODEL=z-ai/glm-4.7-flash

# =============================================================================
# Gradio
# =============================================================================
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
# Share link: if True, attempts to create a public shareable link (app still runs locally if creation fails)
GRADIO_SHARE=false
# Queue configuration: concurrency limit for all event listeners
# Per Gradio queuing docs: https://www.gradio.app/guides/queuing
GRADIO_DEFAULT_CONCURRENCY_LIMIT=3
# Gradio embedded widget mode (optional): if true, uses smaller heights suitable for embedded widget
GRADIO_EMBEDDED_WIDGET=false
# UI Locale (optional): language code for UI translations – "en" (English) or "ru" (Russian). Defaults to "ru" if not set
GRADIO_LOCALE=ru

# =============================================================================
# Agent Mode (LangChain agent with tool calling)
# =============================================================================
# If true, uses agent-based handler; if false, uses direct retrieval
USE_AGENT_MODE=false

# =============================================================================
# Memory compression (conversation history)
# =============================================================================
# Percentage of context window at which we trigger compression
MEMORY_COMPRESSION_THRESHOLD_PCT=80
# Target tokens for the compressed history turn
MEMORY_COMPRESSION_TARGET_TOKENS=1000
# Number of recent messages to keep uncompressed (for agent mode)
MEMORY_COMPRESSION_MESSAGES_TO_KEEP=2

# =============================================================================
# Context budgeting
# =============================================================================
# Context overhead safety margin for formatting and message structure.
# Additional tokens reserved beyond actual system prompt and tool schema counts.
# Accounts for: message formatting, JSON structure overhead, output buffer. Default 2000 tokens.
LLM_CONTEXT_OVERHEAD_SAFETY_MARGIN=3000
# Tool results JSON overhead (scales with accumulated tool results).
# JSON overhead percentage for tool results (JSON format adds overhead vs raw content).
# Applied to accumulated tool result tokens. Default 0.30 = 30% overhead.
LLM_TOOL_RESULTS_JSON_OVERHEAD_PCT=0.30
# Context thresholds (as fractions of model context window).
# Pre-agent threshold: trigger fallback/compression when approaching this percentage
LLM_PRE_CONTEXT_THRESHOLD_PCT=0.80
# Tool-results compression controls
# When total tokens exceed this fraction, trigger compression (also used for post-tool checks)
LLM_COMPRESSION_THRESHOLD_PCT=0.80
# After compression, target total tokens to be at/below this fraction of the context window
LLM_COMPRESSION_TARGET_PCT=0.80
# Minimum tokens to preserve per article during proportional-by-rank compression
LLM_COMPRESSION_MIN_TOKENS=300

# =============================================================================
# Fallback and summarization
# =============================================================================
# Fallback model selection (optional): enable automatic fallback to larger models when context limit is approached
LLM_FALLBACK_ENABLED=false
LLM_FALLBACK_PROVIDER=openrouter
# Comma-separated list of allowed fallback models
LLM_ALLOWED_FALLBACK_MODELS=openai/gpt-5-mini,anthropic/claude-sonnet-4.5
# Summarization (optional): enable article summarization for context compression
LLM_SUMMARIZATION_ENABLED=true
# Target tokens per article after summarization
LLM_SUMMARIZATION_TARGET_TOKENS_PER_ARTICLE=1200
