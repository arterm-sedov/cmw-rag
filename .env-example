# LLM Providers (at least one required)
GOOGLE_API_KEY=your_google_api_key_here
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Embedding
EMBEDDING_MODEL=ai-forever/FRIDA
EMBEDDING_DEVICE=auto

# ChromaDB
CHROMADB_PERSIST_DIR=./data/chromadb_data
CHROMADB_COLLECTION=mkdocs_kb

# Retrieval
TOP_K_RETRIEVE=20
TOP_K_RERANK=10
CHUNK_SIZE=500
CHUNK_OVERLAP=150

# Retrieval â€“ Multi-vector queries and Query Decomposition
# Safe defaults; override to tune latency/recall trade-offs
RETRIEVAL_MULTIQUERY_ENABLED=true
RETRIEVAL_MULTIQUERY_MAX_SEGMENTS=4
RETRIEVAL_MULTIQUERY_SEGMENT_TOKENS=448
RETRIEVAL_MULTIQUERY_SEGMENT_OVERLAP=64
RETRIEVAL_MULTIQUERY_PRE_RERANK_LIMIT=60

# Optional: LLM-based query decomposition (off by default)
RETRIEVAL_QUERY_DECOMP_ENABLED=false
RETRIEVAL_QUERY_DECOMP_MAX_SUBQUERIES=4

# Reranker
RERANK_ENABLED=true
RERANKER_MODEL=DiTy/cross-encoder-russian-msmarco

# LLM
# vLLM Configuration (default - optimized for 48GB GPUs)
# Requires vLLM server running at http://localhost:8000
# Start server with: cd ~/cmw-vllm && python3 -m cmw_vllm.cli start
DEFAULT_LLM_PROVIDER=vllm
DEFAULT_MODEL=Qwen/Qwen3-30B-A3B-Instruct-2507
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=EMPTY
LLM_TEMPERATURE=0.1
# Optional overrides (if set, override values from model_configs.py)
# LLM_MAX_TOKENS=40000  # Overrides max_tokens from model config
# LLM_TOKEN_LIMIT=40000  # Overrides token_limit (context window) from model config

# Alternative Provider Options (commented out)
#DEFAULT_LLM_PROVIDER=gemini
#DEFAULT_MODEL=gemini-2.5-flash
#LLM_TEMPERATURE=0.1
#LLM_MAX_TOKENS=4096
#
#DEFAULT_LLM_PROVIDER=openrouter
#DEFAULT_MODEL=qwen/qwen3-30b-a3b-instruct-2507
#LLM_TEMPERATURE=0.1
#LLM_MAX_TOKENS=65536

# Gradio
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
GRADIO_SHARE=false

# Agent Mode (LangChain agent with tool calling)
# If true, uses agent-based handler; if false, uses direct retrieval
USE_AGENT_MODE=false

# Memory compression (conversation history)
MEMORY_COMPRESSION_THRESHOLD_PCT=80
MEMORY_COMPRESSION_TARGET_TOKENS=1000
MEMORY_COMPRESSION_MESSAGES_TO_KEEP=2

# Context budgeting overhead
# Context overhead safety margin for formatting and message structure
# Additional tokens reserved beyond actual system prompt and tool schema counts
# Accounts for: message formatting, JSON structure overhead, output buffer
# Default 2000 tokens
LLM_CONTEXT_OVERHEAD_SAFETY_MARGIN=2000

# Tool results JSON overhead (scales with accumulated tool results)
# JSON overhead percentage for tool results (JSON format adds overhead vs raw content)
# Applied to accumulated tool result tokens to account for JSON serialization overhead
# Default 0.30 = 30% overhead (adjust if JSON overhead is different in your use case)
LLM_TOOL_RESULTS_JSON_OVERHEAD_PCT=0.30

# Context thresholds (as fractions of model context window)
# Pre-agent threshold: trigger fallback/compression when approaching this percentage
LLM_PRE_CONTEXT_THRESHOLD_PCT=0.90

# Tool-results compression controls
# When total tokens exceed this fraction, trigger compression (also used for post-tool checks)
LLM_COMPRESSION_THRESHOLD_PCT=0.85
# After compression, target total tokens to be at/below this fraction of the context window
LLM_COMPRESSION_TARGET_PCT=0.80
# Minimum tokens to preserve per article during proportional-by-rank compression
LLM_COMPRESSION_MIN_TOKENS=300

# Fallback model selection (optional)
# Enable automatic fallback to larger models when context limit is approached
LLM_FALLBACK_ENABLED=false
LLM_FALLBACK_PROVIDER=openrouter
# Comma-separated list of allowed fallback models
LLM_ALLOWED_FALLBACK_MODELS=openai/gpt-5-mini,anthropic/claude-sonnet-4.5

# Gradio embedded widget mode (optional)
# If true, uses smaller heights suitable for embedded widget
GRADIO_EMBEDDED_WIDGET=false
