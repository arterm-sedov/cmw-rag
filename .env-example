# =============================================================================
# CMW-RAG Environment Configuration
# =============================================================================
# Copy this file to .env and configure your values.
# NEVER commit .env to version control.
# =============================================================================
#
# FORMAT:
#   VARIABLE_NAME=value                    # Required: uncomment and set value
#   # VARIABLE_NAME=commented_value         # Optional: uncomment to override
#
# All variables are documented below. Search for [REQUIRED] or [OPTIONAL] tags.
# =============================================================================

# =============================================================================
# LLM PROVIDERS
# =============================================================================
# [REQUIRED] At least one API key must be configured.
# Get keys from: https://console.google.com (Gemini) or https://openrouter.ai (OpenRouter)
GOOGLE_API_KEY=
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# =============================================================================
# EMBEDDING MODEL
# =============================================================================
# Configures how text is converted to vectors for semantic search.
#
# PROVIDER OPTIONS:
#   direct    - Use sentence-transformers library locally (requires GPU/CPU)
#   infinity  - Connect to external Infinity server (HTTP)
#   openrouter - Use OpenRouter cloud API
#
# MODEL OPTIONS:
#   direct/infinity: ai-forever/FRIDA (Russian/English, ~4GB)
#   openrouter:     Qwen/Qwen3-Embedding-0.6B|4B|8B
#
EMBEDDING_PROVIDER_TYPE=direct
EMBEDDING_MODEL=ai-forever/FRIDA
EMBEDDING_DEVICE=auto          # auto=gpu if available, cpu, or cuda device index

# Embedding request settings
EMBEDDING_TIMEOUT=60.0        # Seconds before timeout (float)
EMBEDDING_MAX_RETRIES=3       # Number of retry attempts on failure

# Optional: Override default Infinity endpoint
# INFINITY_EMBEDDING_ENDPOINT=http://localhost:7997

# =============================================================================
# HUGGINGFACE CONFIGURATION
# =============================================================================
# Configuration for HuggingFace model downloads and caching.
# Relevant when using sentence-transformers locally (direct provider).
#
# Get token: https://huggingface.co/settings/tokens
HF_TOKEN=
# Set to true to trust locally cached models (skip remote validation)
# Reduces network calls and timeouts; model is loaded from cache only
# HF_HUB_DISABLE_REMOTE_VALIDATION=true

# =============================================================================
# CHROMADB VECTOR STORE
# =============================================================================
# ChromaDB stores document embeddings for semantic search.
# Start server: python rag_engine/scripts/start_chroma_server.py
#
CHROMADB_PERSIST_DIR=./data/chromadb_data    # Local data directory
CHROMADB_COLLECTION=mkdocs_kb                # Collection name for your documents

# HTTP client connection settings
CHROMADB_HOST=localhost                       # Server hostname
CHROMADB_PORT=8000                           # Server port
CHROMADB_SSL=false                           # Use HTTPS (true/false)
CHROMADB_HTTP_KEEPALIVE_SECS=60.0            # Keep connections alive (seconds)
CHROMADB_MAX_CONNECTIONS=100                  # Connection pool size

# =============================================================================
# RETRIEVAL SETTINGS
# =============================================================================
# Controls how documents are retrieved and ranked.

TOP_K_RETRIEVE=20            # Initial vector search results (before reranking)
TOP_K_RERANK=10              # Final results after cross-encoder reranking
CHUNK_SIZE=500               # Tokens per document chunk
CHUNK_OVERLAP=150            # Overlap tokens between chunks (maintains context)

# ----- Multi-Vector Query Retrieval -----
# Splits long queries into segments, retrieves per segment, unions + reranks.
# Improves recall for complex/long queries.

RETRIEVAL_MULTIQUERY_ENABLED=true            # Enable segment-based retrieval
RETRIEVAL_MULTIQUERY_MAX_SEGMENTS=4           # Maximum query segments
RETRIEVAL_MULTIQUERY_SEGMENT_TOKENS=448      # Target tokens per segment (max 512)
RETRIEVAL_MULTIQUERY_SEGMENT_OVERLAP=64       # Overlap tokens between segments
RETRIEVAL_MULTIQUERY_PRE_RERANK_LIMIT=60     # Max candidates before rerank

# ----- LLM-Based Query Decomposition -----
# Uses LLM to generate multiple sub-queries from complex questions.

RETRIEVAL_QUERY_DECOMP_ENABLED=false         # Enable LLM query decomposition
RETRIEVAL_QUERY_DECOMP_MAX_SUBQUERIES=4      # Maximum sub-queries to generate

# =============================================================================
# CROSS-ENCODER RERANKER
# =============================================================================
# Re-ranks initial retrieval results for better relevance.
#
# PROVIDER OPTIONS:
#   direct   - Use CrossEncoder library locally
#   infinity - Connect to external Infinity server (HTTP)
#
# MODEL OPTIONS:
#   direct/infinity: DiTy/cross-encoder-russian-msmarco, BAAI/bge-reranker-v2-m3
#                    Qwen/Qwen3-Reranker-0.6B|4B|8B (infinity)
#
RERANK_ENABLED=true
RERANK_SCORE_THRESHOLD=0.5
RERANKER_PROVIDER_TYPE=direct
RERANKER_MODEL=DiTy/cross-encoder-russian-msmarco

# Reranker request settings
RERANKER_TIMEOUT=60.0          # Seconds before timeout (float)
RERANKER_MAX_RETRIES=3         # Number of retry attempts on failure

# Optional: Override default Infinity endpoint
# INFINITY_RERANKER_ENDPOINT=http://localhost:7998

# =============================================================================
# LANGUAGE MODEL (LLM)
# =============================================================================
# Configures the language model for response generation.
#
# PROVIDER OPTIONS:
#   vllm      - Local vLLM server (fastest, requires setup)
#   gemini    - Google Gemini API
#   openrouter - OpenRouter cloud API (access to many models)
#
# MODEL SLUGS (case-insensitive):
#   vLLM:        Qwen/Qwen3-30B-A3B-Instruct-2507, deepseek/deepseek-v3.1-terminus
#   Gemini:      gemini-2.5-flash, gemini-2.5-pro
#   OpenRouter:  qwen/qwen3-30b-a3b-instruct-2507, moonshotai/kimi-k2.5,
#                minimax/minimax-m2.1, openai/gpt-5-mini
#
# See rag_engine/config/models.yaml for full model catalog with context windows.
#
DEFAULT_LLM_PROVIDER=vllm
DEFAULT_MODEL=Qwen/Qwen3-30B-A3B-Instruct-2507

# vLLM server settings (if DEFAULT_LLM_PROVIDER=vllm)
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=EMPTY
VLLM_STREAMING_FALLBACK_ENABLED=true   # Fallback to non-streaming if tools needed

# LLM behavior settings
LLM_TEMPERATURE=0.1                    # Response randomness (0.0-2.0, lower=more deterministic)
LLM_MILD_LIMIT=1024                   # Soft token limit for response length

# Optional: Override model config limits
# LLM_MAX_TOKENS=4096                  # Hard cutoff for response tokens
# LLM_TOKEN_LIMIT=32768                # Context window size override

# Timezone for datetime operations (IANA format: Europe/Moscow, UTC, America/New_York)
DEFAULT_TIMEZONE=Europe/Moscow

# =============================================================================
# GRADIO WEB INTERFACE
# =============================================================================
# Configuration for the web-based chat UI.

GRADIO_SERVER_NAME=0.0.0.0            # Bind address (0.0.0.0=all interfaces)
GRADIO_SERVER_PORT=7860               # HTTP port
GRADIO_SHARE=false                   # Create public share link (true/false)
GRADIO_DEFAULT_CONCURRENCY_LIMIT=3   # Max concurrent requests per listener
GRADIO_EMBEDDED_WIDGET=false         # Use compact widget layout for embedding
GRADIO_LOCALE=ru                     # UI language: ru (Russian) or en (English)

# =============================================================================
# MEMORY & CONTEXT MANAGEMENT
# =============================================================================
# Conversation history and context compression settings.

# ----- Conversation Memory -----
MEMORY_COMPRESSION_THRESHOLD_PCT=80   # Compress history at 80% of context window
MEMORY_COMPRESSION_TARGET_TOKENS=1000 # Target tokens after compression
MEMORY_COMPRESSION_MESSAGES_TO_KEEP=2 # Recent messages to keep uncompressed

# ----- Tool Results Compression -----
# Compresses accumulated retrieval results when context is nearly full.
LLM_COMPRESSION_THRESHOLD_PCT=0.80   # Trigger at 80% of context
LLM_COMPRESSION_TARGET_PCT=0.80      # Target 80% after compression
LLM_COMPRESSION_MIN_TOKENS=300       # Minimum tokens per article

# ----- Context Safety Margins -----
LLM_CONTEXT_OVERHEAD_SAFETY_MARGIN=3000  # Reserve tokens for formatting/JSON
LLM_TOOL_RESULTS_JSON_OVERHEAD_PCT=0.30 # 30% overhead for JSON serialization
LLM_PRE_CONTEXT_THRESHOLD_PCT=0.80       # Pre-agent fallback threshold

# =============================================================================
# FALLBACK & SUMMARIZATION
# =============================================================================
# Automatic model selection and context compression.

# ----- Model Fallback -----
# Automatically switch to larger models when context limit is approached.
LLM_FALLBACK_ENABLED=false
LLM_FALLBACK_PROVIDER=openrouter
LLM_ALLOWED_FALLBACK_MODELS=openai/gpt-5-mini,anthropic/claude-sonnet-4.5

# ----- Article Summarization -----
# Compresses retrieved articles to fit more content in context.
LLM_SUMMARIZATION_ENABLED=true
LLM_SUMMARIZATION_TARGET_TOKENS_PER_ARTICLE=1200
